%
%  Hi!, before adding a new Bibtex entry, make sure that it has the following
%  fields and complies with this format:
%
%    title={Title of your article},
%    author={Doe, J. and Smith, K.L.},
%    journal={Name of the journal or conference},
%    year={2020},
%    doi={it can be empty},
%    url={url to the journal/conference page of your article},
%

@misc{zhu2025hashconsing,
      title={Efficient Symbolic Computation via Hash Consing},
      author={Zhu, Bowen and Sabharwal, Aayush and Tan, Songchen and Ma, Yingbo and Edelman, Alan and Rackauckas, Christopher},
      year={2025},
      eprint={2509.20534},
      archivePrefix={arXiv},
      primaryClass={cs.PL},
      url={https://arxiv.org/abs/2509.20534},
      abstract={Symbolic computation systems suffer from memory inefficiencies due to redundant storage of structurally identical subexpressions. This work presents the first integration of hash consing into JuliaSymbolics, employing a global weak-reference hash table that canonicalizes expressions and eliminates duplication, reducing memory consumption and accelerating operations such as differentiation, simplification, and code generation.}
}

@misc{ferguson2025aimps,
      title={The Future of Artificial Intelligence and the Mathematical and Physical Sciences (AI+MPS)},
      author={Ferguson, Andrew and LaFleur, Marisa and Ruthotto, Lars and Thaler, Jesse and Ting, Yuan-Sen and Tiwary, Pratyush and Villar, Soledad and others},
      year={2025},
      eprint={2509.02661},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2509.02661},
      note={Community Paper from NSF Future of AI+MPS Workshop, Cambridge, Massachusetts, March 24-26, 2025. Christopher Rackauckas (JuliaHub, Pumas-AI, MIT) listed as contributing author.},
      abstract={Community paper from NSF Workshop examining how mathematical and physical science domains can leverage AI while contributing to its development. Proposes strategic priorities for bidirectional AI+MPS research, interdisciplinary community building, and workforce development.}
}

@misc{utkarsh2025pcfm,
      title={Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints},
      author={Utkarsh, Utkarsh and Cai, Pengfei and Edelman, Alan and Gomez-Bombarelli, Rafael and Rackauckas, Christopher Vincent},
      year={2025},
      eprint={2506.04171},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2506.04171},
      abstract={Deep generative models have recently been applied to physical systems governed by partial differential equations (PDEs), offering scalable simulation and uncertainty-aware inference. However, enforcing physical constraints, such as conservation laws and physical consistencies, remains challenging. This work proposes Physics-Constrained Flow Matching (PCFM), a zero-shot inference framework that enforces arbitrary nonlinear constraints in pretrained flow-based generative models while ensuring exact constraint satisfaction.}
}

@misc{pal2025neuralDAEs,
      title={Semi-Explicit Neural DAEs: Learning Long-Horizon Dynamical Systems with Algebraic Constraints},
      author={Pal, Avik and Edelman, Alan and Rackauckas, Christopher},
      year={2025},
      eprint={2505.20515},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2505.20515},
      abstract={A method for enforcing algebraic constraints in neural differential equations by projecting ODE steps onto constraint manifolds, achieving constraint violation errors below 10^{-10} across benchmark problems.}
}

@misc{bassik2023robust,
      title={Robust Parameter Estimation for Rational Ordinary Differential Equations},
      author={Bassik, Oren and Berman, Yosef and Go, Soo and Hong, Hoon and Ilmer, Ilia and Ovchinnikov, Alexey and Rackauckas, Chris and Soto, Pedro and Yap, Chee},
      year={2023},
      eprint={2303.02159},
      archivePrefix={arXiv},
      primaryClass={cs.MS},
      url={https://arxiv.org/abs/2303.02159},
      doi={10.1016/j.amc.2025.129638},
      note={Published in Applied Mathematics and Computation, Vol. 509, 15 January 2026},
      abstract={A novel approach for estimating parameters in rational ODE models from time series data using differential algebra, rational function interpolation, and multivariate polynomial system solving, avoiding dependence on initial guesses and search intervals.}
}

@misc{ringoot2025gpuresidentmemoryawarealgorithmaccelerating,
      title={A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization of Banded Matrices}, 
      author={Evelyne Ringoot and Rabab Alomairy and Alan Edelman},
      year={2025},
      eprint={2510.12705},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2510.12705}, 
}
@misc{ringoot2025performantUnified,
      title={Performant Unified GPU Kernels for Portable Singular Value Computation Across Hardware and Precision}, 
      author={Evelyne Ringoot and Rabab Alomairy and Valentin Churavy and Alan Edelman},
      year={2025},
      eprint={2508.06339},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2508.06339}, 
}
@InProceedings{10.1007/978-3-031-97196-9_13,
author="Carric, Vicki
and Onyango, Maxwell
and Alomairy, Rabab
and Ringoot, Evelyne
and Schloss, James
and Edelman, Alan",
editor="Diehl, Patrick
and Cao, Qinglei
and Herault, Thomas
and Bosilca, George",
title="Toward Portable GPU Performance: Julia Recursive Implementation of TRMM and TRSM",
booktitle="Asynchronous Many-Task Systems and Applications",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="154--164",
abstract="This paper presents a performant and portable recursive implementation of triangular matrix-matrix multiplication (TRMM) and triangular solve (TRSM) operations in Julia for GPUs, which form the backbone of many other linear algebra algorithms. This work is based on an existing recursive implementation for TRMM and TRSM, which restructures the operations to include general matrix-matrix multiplication (GEMM) calls, facilitating better utilization of the GPU memory hierarchy, and reducing latency overhead. The unified implementation in Julia harnesses the language's multiple-dispatch and metaprogramming capabilities through the existing GPUArrays and KernelAbstractions frameworks, enabling performant hardware-agnostic execution across different GPU architectures. By supporting a consistent API, this implementation allows users to seamlessly switch between different GPU backends. The recursive hardware-agnostic implementation we present achieves performance comparable to vendor-optimized (cuBLAS/rocBLAS) libraries for larger matrix sizes and provides such methods for the first time to Apple Silicion hardware with only a few hundred lines of code, demonstrating the power of unified implementations.",
isbn="978-3-031-97196-9"
}


@incollection{abdelrehim_active_nodate,
	title = {Active {Learning} {Enhanced} {Surrogate} {Modeling} of {Jet} {Engines} in {JuliaSim}},
	url = {https://arc.aiaa.org/doi/abs/10.2514/6.2025-2323},
	abstract = {Surrogate models are effective tools for accelerated design of complex systems. The result of a design optimization procedure using surrogate models can be used to initialize an optimization routine using the full order system. High accuracy of the surrogate model can be advantageous for fast convergence. In this work, we present an active learning approach to produce a very high accuracy surrogate model of a turbofan jet engine, that demonstrates 0.1\% relative error for all quantities of interest. We contrast this with a surrogate model produced using a more traditional brute-force data generation approach. .},
	urldate = {2025-02-18},
	booktitle = {{AIAA} {SCITECH} 2025 {Forum}},
	publisher = {American Institute of Aeronautics and Astronautics},
	author = {Abdelrehim, Anas and Gandhi, Dhairya and Yalburgi, Sharan and Bharambe, Ashutosh and Anantharaman, Ranjan and Rackauckas, Christopher V.},
	doi = {10.2514/6.2025-2323},
	note = {\_eprint: https://arc.aiaa.org/doi/pdf/10.2514/6.2025-2323},
}

@article{alomairy_scalable_2025,
	title = {Scalable {Hamming} {Distance} {Computation} {Using} {Accelerated} {Matrix} {Transformations}},
	url = {http://hdl.handle.net/10754/702459},
	language = {en},
	urldate = {2025-02-18},
	author = {Alomairy, Rabab M. and Cao, Q. and Ltaief, Hatem and Keyes, David E. and Edelman, A.},
	month = feb,
	year = {2025},
}

@misc{carlson_c_2025,
	title = {C codegen considered unnecessary: go directly to binary, do not pass {C}. {Compilation} of {Julia} code for deployment in model-based engineering},
	shorttitle = {C codegen considered unnecessary},
	url = {http://arxiv.org/abs/2502.01128},
	doi = {10.48550/arXiv.2502.01128},
	abstract = {Since time immemorial an old adage has always seemed to ring true: you cannot use a high-level productive programming language like Python or R for real-time control and embedded-systems programming, you must rewrite your program in C. We present a counterexample to this mantra by demonstrating how recent compiler developments in the Julia programming language allow users of Julia and the equation-based modeling language ModelingToolkit to compile and deploy binaries for real-time model-based estimation and control. Contrary to the approach taken by a majority of modeling and simulation tools, we do not generate C code, and instead demonstrate how we may use the native Julia code-generation pipeline through LLVM to compile architecture-specific binaries from high-level code. This approach avoids many of the restrictions typically placed on high-level languages to enable C-code generation. As case studies, we include a nonlinear state estimator derived from an equation-based model which is compiled into a program that performs state estimation for deployment onto a Raspberry Pi, as well as a PID controller library implemented in Julia and compiled into a shared library callable from a C program.},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Carlson, Fredrik Bagge and Tapscott, Cody and Baraldi, Gabriel and Rackauckas, Chris},
	month = feb,
	year = {2025},
	note = {arXiv:2502.01128 [eess]},
	keywords = {Computer Science - Systems and Control, Electrical Engineering and Systems Science - Systems and Control},
}

@misc{bisain_new_2025,
	title = {A {New} {Upper} {Bound} {For} the {Growth} {Factor} in {Gaussian} {Elimination} with {Complete} {Pivoting}},
	url = {http://arxiv.org/abs/2312.00994},
	doi = {10.48550/arXiv.2312.00994},
	abstract = {The growth factor in Gaussian elimination measures how large the entries of an LU factorization can be relative to the entries of the original matrix. It is a key parameter in error estimates, and one of the most fundamental topics in numerical analysis. We produce an upper bound of \$n{\textasciicircum}\{0.2079 {\textbackslash}ln n +0.91\}\$ for the growth factor in Gaussian elimination with complete pivoting -- the first improvement upon Wilkinson's original 1961 bound of \$2 {\textbackslash}, n {\textasciicircum}\{0.25{\textbackslash}ln n +0.5\}\$.},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Bisain, Ankit and Edelman, Alan and Urschel, John},
	month = feb,
	year = {2025},
	note = {arXiv:2312.00994 [math]},
	keywords = {Computer Science - Numerical Analysis, Mathematics - Numerical Analysis},
}

@misc{tan_scalable_2025,
	title = {Scalable higher-order nonlinear solvers via higher-order automatic differentiation},
	url = {http://arxiv.org/abs/2501.16895},
	doi = {10.48550/arXiv.2501.16895},
	abstract = {This paper demonstrates new methods and implementations of nonlinear solvers with higher-order of convergence, which is achieved by efficiently computing higher-order derivatives. Instead of computing full derivatives, which could be expensive, we compute directional derivatives with Taylor-mode automatic differentiation. We first implement Householder's method with arbitrary order for one variable, and investigate the trade-off between computational cost and convergence order. We find that the second-order variant, i.e., Halley's method, to be the most valuable, and further generalize Halley's method to systems of nonlinear equations and demonstrate that it can scale efficiently to large-scale problems. We further apply Halley's method on solving large-scale ill-conditioned nonlinear problems, as well as solving nonlinear equations inside stiff ODE solvers, and demonstrate that it could outperform Newton's method.},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Tan, Songchen and Miao, Keming and Edelman, Alan and Rackauckas, Christopher},
	month = jan,
	year = {2025},
	note = {arXiv:2501.16895 [math]},
	keywords = {Computer Science - Numerical Analysis, Mathematics - Numerical Analysis},
}

@misc{edelman_limit_2024,
	title = {On the {Limit} of the {Tridiagonal} {Model} for \$β\$-{Dyson} {Brownian} {Motion}},
	url = {http://arxiv.org/abs/2411.01633},
	doi = {10.48550/arXiv.2411.01633},
	abstract = {In previous work, a description of the result of applying the Householder tridiagonalization algorithm to a G\${\textbackslash}beta\$E random matrix is provided by Edelman and Dumitriu. The resulting tridiagonal ensemble makes sense for all \${\textbackslash}beta{\textgreater}0\$, and has spectrum given by the \${\textbackslash}beta\$-ensemble for all \${\textbackslash}beta{\textgreater}0\$. Moreover, the tridiagonal model has useful stochastic operator limits which was introduced and analyzed in subsequent studies. In this work, we analogously study the result of applying the Householder tridiagonalization algorithm to a G\${\textbackslash}beta\$E process which has eigenvalues governed by \${\textbackslash}beta\$-Dyson Brownian motion. We propose an explicit limit of the upper left \$k {\textbackslash}times k\$ minor of the \$n {\textbackslash}times n\$ tridiagonal process as \$n {\textbackslash}to {\textbackslash}infty\$ and \$k\$ remains fixed. We prove the result for \${\textbackslash}beta=1\$, and also provide numerical evidence for \${\textbackslash}beta=1,2,4\$. This leads us to conjecture the form of a dynamical \${\textbackslash}beta\$-stochastic Airy operator with smallest \$k\$ eigenvalues evolving according to the \$n {\textbackslash}to {\textbackslash}infty\$ limit of the largest, centered and re-scaled, \$k\$ eigenvalues of \${\textbackslash}beta\$-Dyson Brownian motion.},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Edelman, Alan and Jeong, Sungwoo and Nissim, Ron},
	month = nov,
	year = {2024},
	note = {arXiv:2411.01633 [math]},
	keywords = {Computer Science - Numerical Analysis, Mathematical Physics, Mathematics - Mathematical Physics, Mathematics - Numerical Analysis, Mathematics - Probability},
}

@misc{diamandis_solving_2024,
	title = {Solving the {Convex} {Flow} {Problem}},
	url = {http://arxiv.org/abs/2408.11040},
	doi = {10.48550/arXiv.2408.11040},
	abstract = {In this paper, we introduce the solver ConvexFlows for the convex flow problem first defined in the authors' previous work. In this problem, we aim to optimize a concave utility function depending on the flows over a graph. However, unlike the classic network flows literature, we also allow for a concave relationship between the input and output flows of edges. This nonlinear gain describes many physical phenomena, including losses in power network transmission lines. We outline an efficient algorithm for solving this problem which parallelizes over the graph edges. We provide an open source implementation of this algorithm in the Julia programming language package ConvexFlows.jl. This package includes an interface to easily specify these flow problems. We conclude by walking through an example of solving for an optimal power flow using ConvexFlows.},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Diamandis, Theo and Angeris, Guillermo},
	month = aug,
	year = {2024},
	note = {arXiv:2408.11040 [math]},
	keywords = {Computer Science - Mathematical Software, Mathematics - Optimization and Control},
}

@article{edelman_new_2024,
	title = {Some {New} {Results} on the {Maximum} {Growth} {Factor} in {Gaussian} {Elimination}},
	volume = {45},
	issn = {0895-4798},
	url = {https://epubs.siam.org/doi/abs/10.1137/23M1571903},
	doi = {10.1137/23M1571903},
	abstract = {.Gaussian elimination (GE) is the most used dense linear solver. Error analysis of GE with selected pivoting strategies on well-conditioned systems can focus on studying the behavior of growth factors. Although exponential growth is possible with GE with partial pivoting (GEPP), growth tends to stay much smaller in practice. Support for this behavior was provided recently by Huang and Tikhomirov’s average-case analysis of GEPP, which showed GEPP growth factors for Gaussian matrices stay at most polynomial with very high probability. GE with complete pivoting (GECP) has also seen a lot of recent interest, with improvements to both lower and upper bounds on worst-case GECP growth provided by Bisain, Edelman, and Urschel in 2023. We are interested in studying how GEPP and GECP behave on the same linear systems as well as studying large growth on particular subclasses of matrices, including orthogonal matrices. Moreover, as a means to better address the question of why large growth is rarely encountered, we further study matrices with a large difference in growth between using GEPP and GECP, and we explore how the smaller growth strategy dominates behavior in a small neighborhood of the initial matrix.},
	number = {2},
	urldate = {2025-02-18},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Edelman, Alan and Urschel, John},
	month = jun,
	year = {2024},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {967--991},
}

@misc{edelman_perturbation_2024,
	title = {On a perturbation analysis of {Higham} squared maximum {Gaussian} elimination growth matrices},
	url = {http://arxiv.org/abs/2406.00737},
	doi = {10.48550/arXiv.2406.00737},
	abstract = {Gaussian elimination is the most popular technique for solving a dense linear system. Large errors in this procedure can occur in floating point arithmetic when the matrix's growth factor is large. We study this potential issue and how perturbations can improve the robustness of the Gaussian elimination algorithm. In their 1989 paper, Higham and Higham characterized the complete set of real n by n matrices that achieves the maximum growth factor under partial pivoting. This set of matrices serves as the critical focus of this work. Through theoretical insights and empirical results, we illustrate the high sensitivity of the growth factor of these matrices to perturbations and show how subtle changes can be strategically applied to matrix entries to significantly reduce the growth, thus enhancing computational stability and accuracy.},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Edelman, Alan and Urschel, John and Zhu, Bowen},
	month = jun,
	year = {2024},
	note = {arXiv:2406.00737 [math]},
	keywords = {Computer Science - Numerical Analysis, Mathematics - Numerical Analysis},
}

@article{arnold_mapping_2024,
	title = {Mapping {Out} {Phase} {Diagrams} with {Generative} {Classifiers}},
	volume = {132},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.132.207301},
	doi = {10.1103/PhysRevLett.132.207301},
	abstract = {One of the central tasks in many-body physics is the determination of phase diagrams. However, mapping out a phase diagram generally requires a great deal of human intuition and understanding. To automate this process, one can frame it as a classification task. Typically, classification problems are tackled using discriminative classifiers that explicitly model the probability of the labels for a given sample. Here we show that phase-classification problems are naturally suitable to be solved using generative classifiers based on probabilistic models of the measurement statistics underlying the physical system. Such a generative approach benefits from modeling concepts native to the realm of statistical and quantum physics, as well as recent advances in machine learning. This leads to a powerful framework for the autonomous determination of phase diagrams with little to no human supervision that we showcase in applications to classical equilibrium systems and quantum ground states.},
	number = {20},
	urldate = {2025-02-18},
	journal = {Physical Review Letters},
	author = {Arnold, Julian and Schäfer, Frank and Edelman, Alan and Bruder, Christoph},
	month = may,
	year = {2024},
	note = {Publisher: American Physical Society},
	pages = {207301},
}

@misc{diamandis_convex_2024,
	title = {Convex {Network} {Flows}},
	url = {http://arxiv.org/abs/2404.00765},
	doi = {10.48550/arXiv.2404.00765},
	abstract = {We introduce a general framework for flow problems over hypergraphs. In our problem formulation, which we call the convex flow problem, we have a concave utility function for the net flow at every node and a concave utility function for each edge flow. The objective is to maximize the sum of these utilities, subject to constraints on the flows allowed at each edge, which we only assume to be a convex set. This framework not only includes many classic problems in network optimization, such as max flow, min-cost flow, and multi-commodity flows, but also generalizes these problems to allow, for example, concave edge gain functions. In addition, our framework includes applications spanning a number of fields: optimal power flow over lossy networks, routing and resource allocation in ad-hoc wireless networks, Arrow-Debreu Nash bargaining, and order routing through financial exchanges, among others. We show that the convex flow problem has a dual with a number of interesting interpretations, and that this dual decomposes over the edges of the hypergraph. Using this decomposition, we propose a fast solution algorithm that parallelizes over the edges and admits a clean problem interface. We provide an open source implementation of this algorithm in the Julia programming language, which we show is significantly faster than the state-of-the-art commercial convex solver Mosek.},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Diamandis, Theo and Angeris, Guillermo and Edelman, Alan},
	month = may,
	year = {2024},
	note = {arXiv:2404.00765 [math]},
	keywords = {Mathematics - Optimization and Control},
}

@article{edelman_backpropagation_2024,
	title = {Backpropagation through {Back} {Substitution} with a {Backslash}},
	volume = {45},
	issn = {0895-4798},
	url = {https://epubs.siam.org/doi/abs/10.1137/22M1532871},
	doi = {10.1137/22M1532871},
	abstract = {The back matter includes bibliography, index, and back cover.},
	number = {1},
	urldate = {2025-02-18},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Edelman, Alan and Akyürek, Ekin and Wang, Yuyang},
	month = mar,
	year = {2024},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {429--449},
}

@misc{pal_nonlinearsolvejl_2024,
	title = {{NonlinearSolve}.jl: {High}-{Performance} and {Robust} {Solvers} for {Systems} of {Nonlinear} {Equations} in {Julia}},
	shorttitle = {{NonlinearSolve}.jl},
	url = {http://arxiv.org/abs/2403.16341},
	doi = {10.48550/arXiv.2403.16341},
	abstract = {Efficiently solving nonlinear equations underpins numerous scientific and engineering disciplines, yet scaling these solutions for complex system models remains a challenge. This paper presents NonlinearSolve.jl - a suite of high-performance open-source nonlinear equation solvers implemented natively in the Julia programming language. NonlinearSolve.jl distinguishes itself by offering a unified API that accommodates a diverse range of solver specifications alongside features such as automatic algorithm selection based on runtime analysis, support for GPU-accelerated computation through static array kernels, and the utilization of sparse automatic differentiation and Jacobian-free Krylov methods for large-scale problem-solving. Through rigorous comparison with established tools such as Sundials and MINPACK, NonlinearSolve.jl demonstrates unparalleled robustness and efficiency, achieving significant advancements in solving benchmark problems and challenging real-world applications. The capabilities of NonlinearSolve.jl unlock new potentials in modeling and simulation across various domains, making it a valuable addition to the computational toolkit of researchers and practitioners alike.},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Pal, Avik and Holtorf, Flemming and Larsson, Axel and Loman, Torkel and Utkarsh and Schäefer, Frank and Qu, Qingyu and Edelman, Alan and Rackauckas, Chris},
	month = mar,
	year = {2024},
	note = {arXiv:2403.16341 [math]},
	keywords = {Computer Science - Numerical Analysis, Mathematics - Numerical Analysis},
}

@article{utkarsh_automated_2024,
	title = {Automated translation and accelerated solving of differential equations on multiple {GPU} platforms},
	volume = {419},
	issn = {0045-7825},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782523007156},
	doi = {10.1016/j.cma.2023.116591},
	abstract = {We demonstrate a high-performance vendor-agnostic method for massively parallel solving of ensembles of ordinary differential equations (ODEs) and stochastic differential equations (SDEs) on GPUs. The method is integrated with a widely used differential equation solver library in a high-level language (Julia’s DifferentialEquations.jl) and enables GPU acceleration without requiring code changes by the user. Our approach achieves state-of-the-art performance compared to hand-optimized CUDA-C++ kernels while performing 20–100× faster than the vectorizing map (vmap) approach implemented in JAX and PyTorch. Performance evaluation on NVIDIA, AMD, Intel, and Apple GPUs demonstrates performance portability and vendor-agnosticism. We show composability with MPI to enable distributed multi-GPU workflows. The implemented solvers are fully featured – supporting event handling, automatic differentiation, and incorporation of datasets via the GPU’s texture memory – allowing scientists to take advantage of GPU acceleration on all major current architectures without changing their model code and without loss of performance. We distribute the software as an open-source library, DiffEqGPU.jl.},
	urldate = {2025-02-18},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Utkarsh, Utkarsh and Churavy, Valentin and Ma, Yingbo and Besard, Tim and Srisuma, Prakitr and Gymnich, Tim and Gerlach, Adam R. and Edelman, Alan and Barbastathis, George and Braatz, Richard D. and Rackauckas, Christopher},
	month = feb,
	year = {2024},
	keywords = {Computer kernel, Data-parallelism, Differential equations, GPU, HPC, Numerical simulation},
	pages = {116591},
}

@phdthesis{utkarsh_automating_2024,
	type = {Thesis},
	title = {Automating {Heterogeneous} {Parallelism} in {Numerical} {Differential} {Equations}},
	copyright = {In Copyright - Educational Use Permitted},
	url = {https://dspace.mit.edu/handle/1721.1/155471},
	abstract = {Scientific computing is an amalgamation of numerical methods and computer science. Developments in numerical analysis have allowed stable and accurate numerical schemes, whereas computer algorithms have been successfully adopted to standard multicore systems of today, enabling parallelism. Combining efficient numerical algorithms with efficient parallelism presents a challenge mainly due to the independent development of these fields and is, therefore, typically solved on a domain-specific basis by domain experts. The development of general-purpose tools that integrate parallelism into algorithms, accessible through highlevel languages, signifies the future direction for addressing computational demands across various domains. This thesis work represents a culmination of efforts in general-purpose parallel numerical algorithms for solving differential equations. We make them accessible by choosing the Julia programming language to implement the high-level framework. Solving differential equations appears to be an intrinsically serial process due to progressive time-stepping that proves challenging to parallelize. Most of the approaches are linked to two broad categories; The first is the parallelism of the solver operations by making each solve faster, and the latter is the parallelism between the solves, i.e., solving multiple batches at a time. We automate the parallelization process in both these domains while keeping the algorithms general-purpose. Parallelization with different hardware accelerators, such as CPUs and GPUs, is also investigated. Parallelism for sufficiently large stiff ODEs is traditionally linked to the parallelization of the matrix factorization stage. However, these methods still need to overcome the threading overhead for ODEs having less than approximately 200 states. We propose implementing adaptive-order, adaptive time-stepping stiff ODE solvers such as extrapolation methods, which can parallelize a single instance of an ODE solve even for small ODEs. The other need for parallelization of ODE solvers arises from solving ODEs for batches of data, a typical workflow in inverse problems, global sensitivity analysis, and uncertainty quantification. Traditionally, GPU-accelerated ODE solvers were specially developed for high-dimensional PDE systems, which can be easily adapted for batched ODE solvers. The approach for parallelization is to convert an array-based ODE solver to work with GPU-based arrays. These approaches have shortcomings, such as implicit synchronization of time steps for all the ODEs and GPU overheads. We propose that these approaches can be improved significantly where GPU acceleration for ODE solvers is device-agnostic, general-purpose, and accessible from a high-level language.},
	language = {en},
	urldate = {2025-02-18},
	school = {Massachusetts Institute of Technology},
	author = {Utkarsh},
	month = may,
	year = {2024},
	note = {Accepted: 2024-07-08T18:53:28Z},
}

@article{holtorf_performance_2024,
	title = {Performance {Bounds} for {Quantum} {Feedback} {Control}},
	volume = {69},
	issn = {0018-9286, 1558-2523, 2334-3303},
	url = {http://arxiv.org/abs/2304.03366},
	doi = {10.1109/TAC.2024.3416008},
	abstract = {The limits of quantum feedback control have immediate consequences for quantum information science at large, yet remain largely unexplored. Here, we combine quantum filtering theory and moment-sum-of-squares techniques to construct a hierarchy of convex optimization problems that furnish monotonically improving, computable bounds on the best attainable performance for a broad class of quantum feedback control problems. These bounds may serve as witnesses of fundamental limitations, optimality certificates, or performance targets. We prove convergence of the bounds to the optimal control performance under technical conditions and demonstrate the practical utility of our approach by designing certifiably near-optimal controllers for a qubit in a cavity subjected to photon counting and homodyne detection measurements.},
	number = {11},
	urldate = {2025-02-18},
	journal = {IEEE Transactions on Automatic Control},
	author = {Holtorf, Flemming and Schäfer, Frank and Arnold, Julian and Rackauckas, Christopher and Edelman, Alan},
	month = nov,
	year = {2024},
	note = {arXiv:2304.03366 [quant-ph]},
	keywords = {Computer Science - Systems and Control, Convex optimization, Electrical Engineering and Systems Science - Systems and Control, Feedback control, Mathematics - Optimization and Control, Optimal control, Photonics, Polynomials, quantum filtering, quantum information and control, Quantum Physics, Quantum state, Quantum system, stochastic optimal control, Technological innovation},
	pages = {8057--8063},
}

@misc{silvestri_oceananigansjl_2024,
	title = {Oceananigans.jl: {A} {Julia} library that achieves breakthrough resolution, memory and energy efficiency in global ocean simulations},
	shorttitle = {Oceananigans.jl},
	url = {http://arxiv.org/abs/2309.06662},
	doi = {10.48550/arXiv.2309.06662},
	abstract = {Climate models must simulate hundreds of future scenarios for hundreds of years at coarse resolutions, and a handful of high-resolution decadal simulations to resolve localized extreme events. Using Oceananigans.jl, written from scratch in Julia, we report several achievements: First, a global ocean simulation with breakthrough horizontal resolution -- 488m -- reaching 15 simulated days per day (0.04 simulated years per day; SYPD). Second, Oceananigans simulates the global ocean at 488m with breakthrough memory efficiency on just 768 Nvidia A100 GPUs, a fraction of the resources available on current and upcoming exascale supercomputers. Third, and arguably most significant for climate modeling, Oceananigans achieves breakthrough energy efficiency reaching 0.95 SYPD at 1.7 km on 576 A100s and 9.9 SYPD at 10 km on 68 A100s -- the latter representing the highest horizontal resolutions employed by current IPCC-class ocean models. Routine climate simulations with 10 km ocean components are within reach.},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Silvestri, Simone and Wagner, Gregory L. and Hill, Christopher and Ardakani, Matin Raayai and Blaschke, Johannes and Campin, Jean-Michel and Churavy, Valentin and Constantinou, Navid C. and Edelman, Alan and Marshall, John and Ramadhan, Ali and Souza, Andre and Ferrari, Raffaele},
	month = oct,
	year = {2024},
	note = {arXiv:2309.06662 [physics]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Physics - Atmospheric and Oceanic Physics, Physics - Computational Physics, Physics - Fluid Dynamics},
}

@misc{ekanathan_fully_2024,
	title = {A {Fully} {Adaptive} {Radau} {Method} for the {Efficient} {Solution} of {Stiff} {Ordinary} {Differential} {Equations} at {Low} {Tolerances}},
	url = {http://arxiv.org/abs/2412.14362},
	doi = {10.48550/arXiv.2412.14362},
	abstract = {Radau IIA methods, specifically the adaptive order radau method in Fortran due to Hairer, are known to be state-of-the-art for the high-accuracy solution of highly stiff ordinary differential equations (ODEs). However, the traditional implementation was specialized to a specific range of tolerance, in particular only supporting 5th, 9th, and 13th order versions of the tableau and only derived in double precision floating point, thus limiting the ability to be truly general purpose for highly accurate scenarios. To alleviate these constraints, we implement an adaptive-time adaptive-order Radau method which can derive the coefficients for the Radau IIA embedded tableau to any order on the fly to any precision. Additionally, our Julia-based implementation includes many modernizations to improve performance, including improvements to the order adaptation scheme and improved linear algebra integrations. In a head-to-head benchmark against the classic Fortran implementation, we demonstrate our implementation is approximately 2x across a range of stiff ODEs. We benchmark our algorithm against several well-reputed numerical integrators for stiff ODEs and find state-of-the-art performance on several test problems, with a 1.5-times speed-up over common numerical integrators for stiff ODEs when low error tolerance is required. The newly implemented method is distributed in open source software for free usage on stiff ODEs.},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Ekanathan, Shreyas and Smith, Oscar and Rackauckas, Christopher},
	month = dec,
	year = {2024},
	note = {arXiv:2412.14362 [math]},
	keywords = {Computer Science - Mathematical Software, Computer Science - Numerical Analysis, Mathematics - Numerical Analysis},
}

@article{bhagavan_datainterpolationsjl_2024,
	title = {{DataInterpolations}.jl: {Fast} {Interpolations} of {1D} data},
	volume = {9},
	issn = {2475-9066},
	shorttitle = {{DataInterpolations}.jl},
	url = {https://joss.theoj.org/papers/10.21105/joss.06917},
	doi = {10.21105/joss.06917},
	abstract = {Bhagavan et al., (2024). DataInterpolations.jl: Fast Interpolations of 1D data. Journal of Open Source Software, 9(101), 6917, https://doi.org/10.21105/joss.06917},
	language = {en},
	number = {101},
	urldate = {2025-02-18},
	journal = {Journal of Open Source Software},
	author = {Bhagavan, Sathvik and Koning, Bart de and Maddhashiya, Shubham and Rackauckas, Christopher},
	month = sep,
	year = {2024},
	pages = {6917},
}

@article{hofmann_increasing_2024,
	title = {Increasing spectral {DCM} flexibility and speed by leveraging {Julia}’s {ModelingToolkit} and automated differentiation},
	issn = {2692-8205},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10634910/},
	doi = {10.1101/2023.10.27.564407},
	abstract = {Using neuroimaging and electrophysiological data to infer neural parameter estimations from theoretical circuits requires solving the inverse problem. Here, we provide a new Julia language package designed to i) compose complex dynamical models in a simple and modular way with ModelingToolkit.jl, ii) implement parameter fitting based on spectral dynamic causal modeling (sDCM) using the Laplace approximation, analogous to MATLAB implementation in SPM12, and iii) leverage Julia’s unique strengths to increase accuracy and speed by employing Automatic Differentiation during the fitting procedure. To illustrate the utility of our flexible modular approach, we provide a method to improve correction for fMRI scanner field strengths (1.5T, 3T, 7T) when fitting models to real data.},
	urldate = {2025-02-18},
	journal = {bioRxiv},
	author = {Hofmann, David and Chesebro, Anthony G. and Rackauckas, Chris and Mujica-Parodi, Lilianne R. and Friston, Karl J. and Edelman, Alan and Strey, Helmut H.},
	month = jul,
	year = {2024},
	pmid = {37961652},
	pmcid = {PMC10634910},
	pages = {2023.10.27.564407},
}

@inproceedings{iravanian_hybrid_2024,
	address = {New York, NY, USA},
	series = {{ISSAC} '24},
	title = {Hybrid {Symbolic}-{Numeric} and {Numerically}-{Assisted} {Symbolic} {Integration}},
	isbn = {979-8-4007-0696-7},
	url = {https://dl.acm.org/doi/10.1145/3666000.3669714},
	doi = {10.1145/3666000.3669714},
	abstract = {Most computer algebra systems (CAS) support symbolic integration using either algebraic or heuristic methods. This paper presents HYINT, a hybrid (symbolic-numeric) method to calculate the indefinite integrals of univariate expressions. Like the Risch-Norman algorithm, the symbolic part of HYINT generates an ansatz constituted of multiple candidate terms generated in parallel. The ansatz generator uses a combination of table lookup of integration rules and algebraic manipulations. The numeric part filters the candidate terms over the complex field and applies sparse regression, a component of the Sparse Identification of Nonlinear Dynamics (SINDy) technique, to find the coefficients of the terms in the ansatz. HYINT covers a larger range of potential integrals compared to the Risch-Norman algorithm. Moreover, the form of the final integral is similar to the integrand and consistent with what the users expect. The primary motivation for this work is to add symbolic integration functionality to a modern CAS (the symbolic manipulation packages of SciML, the Scientific Machine Learning ecosystem of the Julia programming language), which is designed for numerical and machine learning applications. We show that this system can solve many common integration problems using only a few dozen basic integration rules. We also discuss numerically-assisted symbolic integration, where HYINT acts as an ansatz generator for other symbolic integration packages.},
	urldate = {2025-02-18},
	booktitle = {Proceedings of the 2024 {International} {Symposium} on {Symbolic} and {Algebraic} {Computation}},
	publisher = {Association for Computing Machinery},
	author = {Iravanian, Shahriar and Gowda, Shashi and Rackauckas, Christopher},
	month = jul,
	year = {2024},
	pages = {410--418},
}

@misc{sapienza_differentiable_2024,
	title = {Differentiable {Programming} for {Differential} {Equations}: {A} {Review}},
	shorttitle = {Differentiable {Programming} for {Differential} {Equations}},
	url = {http://arxiv.org/abs/2406.09699},
	doi = {10.48550/arXiv.2406.09699},
	abstract = {The differentiable programming paradigm is a cornerstone of modern scientific computing. It refers to numerical methods for computing the gradient of a numerical model's output. Many scientific models are based on differential equations, where differentiable programming plays a crucial role in calculating model sensitivities, inverting model parameters, and training hybrid models that combine differential equations with data-driven approaches. Furthermore, recognizing the strong synergies between inverse methods and machine learning offers the opportunity to establish a coherent framework applicable to both fields. Differentiating functions based on the numerical solution of differential equations is non-trivial. Numerous methods based on a wide variety of paradigms have been proposed in the literature, each with pros and cons specific to the type of problem investigated. Here, we provide a comprehensive review of existing techniques to compute derivatives of numerical solutions of differential equations. We first discuss the importance of gradients of solutions of differential equations in a variety of scientific domains. Second, we lay out the mathematical foundations of the various approaches and compare them with each other. Third, we cover the computational considerations and explore the solutions available in modern scientific software. Last but not least, we provide best-practices and recommendations for practitioners. We hope that this work accelerates the fusion of scientific models and data, and fosters a modern approach to scientific modelling.},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Sapienza, Facundo and Bolibar, Jordi and Schäfer, Frank and Groenke, Brian and Pal, Avik and Boussange, Victor and Heimbach, Patrick and Hooker, Giles and Pérez, Fernando and Persson, Per-Olof and Rackauckas, Christopher},
	month = jun,
	year = {2024},
	note = {arXiv:2406.09699 [math]},
	keywords = {Computer Science - Numerical Analysis, Mathematics - Dynamical Systems, Mathematics - Numerical Analysis, Physics - Computational Physics, Statistics - Machine Learning},
}

@article{lang_sbmltoolkitjl_2024,
	title = {{SBMLToolkit}.jl: a {Julia} package for importing {SBML} into the {SciML} ecosystem},
	volume = {21},
	copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
	issn = {1613-4516},
	shorttitle = {{SBMLToolkit}.jl},
	url = {https://www.degruyter.com/document/doi/10.1515/jib-2024-0003/html},
	doi = {10.1515/jib-2024-0003},
	abstract = {Julia is a general purpose programming language that was designed for simplifying and accelerating numerical analysis and computational science. In particular the Scientific Machine Learning (SciML) ecosystem of Julia packages includes frameworks for high-performance symbolic-numeric computations. It allows users to automatically enhance high-level descriptions of their models with symbolic preprocessing and automatic sparsification and parallelization of computations. This enables performant solution of differential equations, efficient parameter estimation and methodologies for automated model discovery with neural differential equations and sparse identification of nonlinear dynamics. To give the systems biology community easy access to SciML, we developed SBMLToolkit.jl. SBMLToolkit.jl imports dynamic SBML models into the SciML ecosystem to accelerate model simulation and fitting of kinetic parameters. By providing computational systems biologists with easy access to the open-source Julia ecosystevnm, we hope to catalyze the development of further Julia tools in this domain and the growth of the Julia bioscience community. SBMLToolkit.jl is freely available under the MIT license. The source code is available at https://github.com/SciML/SBMLToolkit.jl.},
	language = {en},
	number = {1},
	urldate = {2025-02-18},
	journal = {Journal of Integrative Bioinformatics},
	author = {Lang, Paul F. and Jain, Anand and Rackauckas, Christopher},
	month = mar,
	year = {2024},
	note = {Publisher: De Gruyter},
	keywords = {Julia, SBML, scientific machine learning, systems biology markup language},
}

@article{nieves_uncertainty_2024,
	title = {Uncertainty quantified discovery of chemical reaction systems via {Bayesian} scientific machine learning},
	volume = {4},
	issn = {2674-0702},
	url = {https://www.frontiersin.org/journals/systems-biology/articles/10.3389/fsysb.2024.1338518/full},
	doi = {10.3389/fsysb.2024.1338518},
	abstract = {{\textless}p{\textgreater}The recently proposed Chemical Reaction Neural Network (CRNN) discovers chemical reaction pathways from time resolved species concentration data in a deterministic manner. Since the weights and biases of a CRNN are physically interpretable, the CRNN acts as a digital twin of a classical chemical reaction network. In this study, we employ a Bayesian inference analysis coupled with neural ordinary differential equations (ODEs) on this digital twin to discover chemical reaction pathways in a probabilistic manner. This allows for estimation of the uncertainty surrounding the learned reaction network. To achieve this, we propose an algorithm which combines neural ODEs with a preconditioned stochastic gradient langevin descent (pSGLD) Bayesian framework, and ultimately performs posterior sampling on the neural network weights. We demonstrate the successful implementation of this algorithm on several reaction systems by not only recovering the chemical reaction pathways but also estimating the uncertainty in our predictions. We compare the results of the pSGLD with that of the standard SGLD and show that this optimizer more efficiently and accurately estimates the posterior of the reaction network parameters. Additionally, we demonstrate how the embedding of scientific knowledge improves extrapolation accuracy by comparing results to purely data-driven machine learning methods. Together, this provides a new framework for robust, autonomous Bayesian inference on unknown or complex chemical and biological reaction systems.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2025-02-18},
	journal = {Frontiers in Systems Biology},
	author = {Nieves, Emily and Dandekar, Raj and Rackauckas, Chris},
	month = mar,
	year = {2024},
	note = {Publisher: Frontiers},
	keywords = {Bayesian machine learning, machine learning, Quantitative Systems Pharmacology, Scientific Machine Learning, Systems Biology},
}

@article{santana_efficient_2023,
	title = {Efficient hybrid modeling and sorption model discovery for non-linear advection-diffusion-sorption systems: {A} systematic scientific machine learning approach},
	volume = {282},
	issn = {0009-2509},
	shorttitle = {Efficient hybrid modeling and sorption model discovery for non-linear advection-diffusion-sorption systems},
	url = {https://www.sciencedirect.com/science/article/pii/S0009250923007790},
	doi = {10.1016/j.ces.2023.119223},
	abstract = {This study presents a systematic machine learning approach for creating efficient hybrid models and discovering sorption uptake models in non-linear advection-diffusion-sorption systems. It demonstrates an effective method to train these complex systems using gradient-based optimizers, adjoint sensitivity analysis, and JIT-compiled vector Jacobian products, combined with spatial discretization and adaptive integrators. Sparse and symbolic regression were employed to identify missing functions in the artificial neural network. The robustness of the proposed method was tested on an in-silico data set of noisy breakthrough curve observations of fixed-bed adsorption, resulting in a well-fitted hybrid model. The study successfully reconstructed sorption uptake kinetics using sparse and symbolic regression, and accurately predicted breakthrough curves using identified polynomials, highlighting the potential of the proposed framework for discovering sorption kinetic law structures.},
	urldate = {2025-02-18},
	journal = {Chemical Engineering Science},
	author = {Santana, Vinicius V. and Costa, Erbet and Rebello, Carine M. and Ribeiro, Ana Mafalda and Rackauckas, Christopher and Nogueira, Idelfonso B. R.},
	month = dec,
	year = {2023},
	keywords = {Advection-diffusion-sorption, Hybrid modeling, Partial differential equations, Scientific machine learning, Sparse regression},
	pages = {119223},
}

@article{nunez_forecasting_2023,
	title = {Forecasting virus outbreaks with social media data via neural ordinary differential equations},
	volume = {13},
	issn = {2045-2322},
	doi = {10.1038/s41598-023-37118-9},
	abstract = {During the Covid-19 pandemic, real-time social media data could in principle be used as an early predictor of a new epidemic wave. This possibility is examined here by employing a neural ordinary differential equation (neural ODE) trained to forecast viral outbreaks in a specific geographic region. It learns from multivariate time series of signals derived from a novel set of large online polls regarding COVID-19 symptoms. Once trained, the neural ODE can capture the dynamics of interconnected local signals and effectively estimate the number of new infections up to two months in advance. In addition, it may predict the future consequences of changes in the number of infected at a certain period, which might be related with the flow of individuals entering or exiting a region. This study provides persuasive evidence for the predictive ability of widely disseminated social media surveys for public health applications.},
	language = {eng},
	number = {1},
	journal = {Scientific Reports},
	author = {Núñez, Matías and Barreiro, Nadia L. and Barrio, Rafael A. and Rackauckas, Christopher},
	month = jul,
	year = {2023},
	pmid = {37407583},
	pmcid = {PMC10322995},
	keywords = {COVID-19, Disease Outbreaks, Epidemiology, Forecasting, Humans, Machine learning, Pandemics, SARS-CoV-2, Social Media},
	pages = {10870},
}

@article{pestourie_physics-enhanced_2023,
	title = {Physics-enhanced deep surrogates for partial differential equations},
	volume = {5},
	copyright = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-023-00761-y},
	doi = {10.1038/s42256-023-00761-y},
	abstract = {Many physics and engineering applications demand partial differential equations (PDE) property evaluations that are traditionally computed with resource-intensive high-fidelity numerical solvers. Data-driven surrogate models provide an efficient alternative but come with a substantial cost of training. Emerging applications would benefit from surrogates with an improved accuracy–cost tradeoff when studied at scale. Here we present a ‘physics-enhanced deep-surrogate’ (PEDS) approach towards developing fast surrogate models for complex physical systems, which is described by PDEs. Specifically, a combination of a low-fidelity, explainable physics simulator and a neural network generator is proposed, which is trained end-to-end to globally match the output of an expensive high-fidelity numerical solver. Experiments on three exemplar test cases, diffusion, reaction–diffusion and electromagnetic scattering models, show that a PEDS surrogate can be up to three times more accurate than an ensemble of feedforward neural networks with limited data (approximately 103 training points), and reduces the training data need by at least a factor of 100 to achieve a target error of 5\%. Experiments reveal that PEDS provides a general, data-driven strategy to bridge the gap between a vast array of simplified physical models with corresponding brute-force numerical solvers modelling complex systems, offering accuracy, speed and data efficiency, as well as physical insights into the process.},
	language = {en},
	number = {12},
	urldate = {2025-02-18},
	journal = {Nature Machine Intelligence},
	author = {Pestourie, Raphaël and Mroueh, Youssef and Rackauckas, Chris and Das, Payel and Johnson, Steven G.},
	month = dec,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational methods, Computational science, Computer science},
	pages = {1458--1465},
}

@misc{santana_advancing_2023,
	title = {Advancing {Odor} {Classification} {Models} {Enhanced} by {Scientific} {Machine} {Learning} and {Mechanistic} {Model}: {Probabilistic} {Weight} {Assignment} for {Odor} {Intensity} {Prediction} and {Uncertainty} {Analysis} for {Robust} {Fragrance} {Classification}},
	shorttitle = {Advancing {Odor} {Classification} {Models} {Enhanced} by {Scientific} {Machine} {Learning} and {Mechanistic} {Model}},
	url = {https://chemrxiv.org/engage/chemrxiv/article-details/656703395bc9fcb5c9a3a476},
	doi = {10.26434/chemrxiv-2023-mtfrc},
	abstract = {This study presents an innovative framework for classifying and predicting odor intensity in perfumery, combining scientific machine learning with mechanistic modeling to enhance fragrance design precision. A probabilistic weight assignment is introduced, utilizing scent classifier outputs to determine the contribution of each fragrance component, thereby recognizing the subjective nature of scent classification and variability in olfactory perception. Additionally, an uncertainty analysis framework is integrated, quantifying uncertainties within perfume diffusion and human sensory perception models, thus improving model adaptability and reliability. The methodology comprises three parts: a perfume diffusion model that simulates fragrance molecule evaporation and dispersion, an odor perception model using Odor Value for scent intensity quantification, and an uncertainty quantification that rigorously analyzes model parameters and predictions. This approach aims to scientifically advance the art of perfumery, allowing for the creation of sophisticated fragrances with enhanced predictive accuracy.},
	language = {en},
	urldate = {2025-02-18},
	publisher = {ChemRxiv},
	author = {Santana, Vinicius and Costa, Erbet and Rebello, Carine and Ribeiro, Ana Mafalda and Rackauckas, Chris and Nogueira, Idelfonso},
	month = nov,
	year = {2023},
	keywords = {Perfume Engineering, Scent Classification, Scent Science, Scientific Machine Learning},
}

@article{loman_catalyst_2023,
	title = {Catalyst: {Fast} and flexible modeling of reaction networks},
	volume = {19},
	issn = {1553-7358},
	shorttitle = {Catalyst},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1011530},
	doi = {10.1371/journal.pcbi.1011530},
	abstract = {We introduce Catalyst.jl, a flexible and feature-filled Julia library for modeling and high-performance simulation of chemical reaction networks (CRNs). Catalyst supports simulating stochastic chemical kinetics (jump process), chemical Langevin equation (stochastic differential equation), and reaction rate equation (ordinary differential equation) representations for CRNs. Through comprehensive benchmarks, we demonstrate that Catalyst simulation runtimes are often one to two orders of magnitude faster than other popular tools. More broadly, Catalyst acts as both a domain-specific language and an intermediate representation for symbolically encoding CRN models as Julia-native objects. This enables a pipeline of symbolically specifying, analyzing, and modifying CRNs; converting Catalyst models to symbolic representations of concrete mathematical models; and generating compiled code for numerical solvers. Leveraging ModelingToolkit.jl and Symbolics.jl, Catalyst models can be analyzed, simplified, and compiled into optimized representations for use in numerical solvers. Finally, we demonstrate Catalyst’s broad extensibility and composability by highlighting how it can compose with a variety of Julia libraries, and how existing open-source biological modeling projects have extended its intermediate representation.},
	language = {en},
	number = {10},
	urldate = {2025-02-18},
	journal = {PLOS Computational Biology},
	author = {Loman, Torkel E. and Ma, Yingbo and Ilin, Vasily and Gowda, Shashi and Korsbo, Niklas and Yewale, Nikhil and Rackauckas, Chris and Isaacson, Samuel A.},
	month = oct,
	year = {2023},
	note = {Publisher: Public Library of Science},
	keywords = {Biocatalysis, Biophysical simulations, Chemical reactions, Computer software, Network analysis, Polynomials, Simulation and modeling, Software tools},
	pages = {e1011530},
}

@article{shen_differentiable_2023,
	title = {Differentiable modelling to unify machine learning and physical models for geosciences},
	volume = {4},
	copyright = {2023 Springer Nature Limited},
	issn = {2662-138X},
	url = {https://www.nature.com/articles/s43017-023-00450-9},
	doi = {10.1038/s43017-023-00450-9},
	abstract = {Process-based modelling offers interpretability and physical consistency in many domains of geosciences but struggles to leverage large datasets efficiently. Machine-learning methods, especially deep networks, have strong predictive skills yet are unable to answer specific scientific questions. In this Perspective, we explore differentiable modelling as a pathway to dissolve the perceived barrier between process-based modelling and machine learning in the geosciences and demonstrate its potential with examples from hydrological modelling. ‘Differentiable’ refers to accurately and efficiently calculating gradients with respect to model variables or parameters, enabling the discovery of high-dimensional unknown relationships. Differentiable modelling involves connecting (flexible amounts of) prior physical knowledge to neural networks, pushing the boundary of physics-informed machine learning. It offers better interpretability, generalizability, and extrapolation capabilities than purely data-driven machine learning, achieving a similar level of accuracy while requiring less training data. Additionally, the performance and efficiency of differentiable models scale well with increasing data volumes. Under data-scarce scenarios, differentiable models have outperformed machine-learning models in producing short-term dynamics and decadal-scale trends owing to the imposed physical constraints. Differentiable modelling approaches are primed to enable geoscientists to ask questions, test hypotheses, and discover unrecognized physical relationships. Future work should address computational challenges, reduce uncertainty, and verify the physical significance of outputs.},
	language = {en},
	number = {8},
	urldate = {2025-02-18},
	journal = {Nature Reviews Earth \& Environment},
	author = {Shen, Chaopeng and Appling, Alison P. and Gentine, Pierre and Bandai, Toshiyuki and Gupta, Hoshin and Tartakovsky, Alexandre and Baity-Jesi, Marco and Fenicia, Fabrizio and Kifer, Daniel and Li, Li and Liu, Xiaofeng and Ren, Wei and Zheng, Yi and Harman, Ciaran J. and Clark, Martyn and Farthing, Matthew and Feng, Dapeng and Kumar, Praveen and Aboelyazeed, Doaa and Rahmani, Farshid and Song, Yalan and Beck, Hylke E. and Bindas, Tadd and Dwivedi, Dipankar and Fang, Kuai and Höge, Marvin and Rackauckas, Chris and Mohanty, Binayak and Roy, Tirthankar and Xu, Chonggang and Lawson, Kathryn},
	month = aug,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Climate sciences, Ecology, Environmental sciences, Hydrology, Natural hazards},
	pages = {552--567},
}

@article{aboelyazeed_differentiable_2023,
	title = {A differentiable, physics-informed ecosystem modeling and learning framework for large-scale inverse problems: demonstration with photosynthesis simulations},
	volume = {20},
	issn = {1726-4170},
	shorttitle = {A differentiable, physics-informed ecosystem modeling and learning framework for large-scale inverse problems},
	url = {https://bg.copernicus.org/articles/20/2671/2023/},
	doi = {10.5194/bg-20-2671-2023},
	abstract = {Photosynthesis plays an important role in carbon, nitrogen, and water cycles. Ecosystem models for photosynthesis are characterized by many parameters that are obtained from limited in situ measurements and applied to the same plant types. Previous site-by-site calibration approaches could not leverage big data and faced issues like overfitting or parameter non-uniqueness. Here we developed an end-to-end programmatically differentiable (meaning gradients of outputs to variables used in the model can be obtained efficiently and accurately) version of the photosynthesis process representation within the Functionally Assembled Terrestrial Ecosystem Simulator (FATES) model. As a genre of physics-informed machine learning (ML), differentiable models couple physics-based formulations to neural networks (NNs) that learn parameterizations (and potentially processes) from observations, here photosynthesis rates. We first demonstrated that the framework was able to correctly recover multiple assumed parameter values concurrently using synthetic training data. Then, using a real-world dataset consisting of many different plant functional types (PFTs), we learned parameters that performed substantially better and greatly reduced biases compared to literature values. Further, the framework allowed us to gain insights at a large scale. Our results showed that the carboxylation rate at 25 ∘C (Vc,max25) was more impactful than a factor representing water limitation, although tuning both was helpful in addressing biases with the default values. This framework could potentially enable substantial improvement in our capability to learn parameters and reduce biases for ecosystem modeling at large scales.},
	language = {English},
	number = {13},
	urldate = {2025-02-18},
	journal = {Biogeosciences},
	author = {Aboelyazeed, Doaa and Xu, Chonggang and Hoffman, Forrest M. and Liu, Jiangtao and Jones, Alex W. and Rackauckas, Chris and Lawson, Kathryn and Shen, Chaopeng},
	month = jul,
	year = {2023},
	note = {Publisher: Copernicus GmbH},
	pages = {2671--2692},
}

@misc{arya_differentiating_2023,
	title = {Differentiating {Metropolis}-{Hastings} to {Optimize} {Intractable} {Densities}},
	url = {http://arxiv.org/abs/2306.07961},
	doi = {10.48550/arXiv.2306.07961},
	abstract = {We develop an algorithm for automatic differentiation of Metropolis-Hastings samplers, allowing us to differentiate through probabilistic inference, even if the model has discrete components within it. Our approach fuses recent advances in stochastic automatic differentiation with traditional Markov chain coupling schemes, providing an unbiased and low-variance gradient estimator. This allows us to apply gradient-based optimization to objectives expressed as expectations over intractable target densities. We demonstrate our approach by finding an ambiguous observation in a Gaussian mixture model and by maximizing the specific heat in an Ising model.},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Arya, Gaurav and Seyer, Ruben and Schäfer, Frank and Chandra, Kartik and Lew, Alexander K. and Huot, Mathieu and Mansinghka, Vikash K. and Ragan-Kelley, Jonathan and Rackauckas, Christopher and Schauer, Moritz},
	month = jun,
	year = {2023},
	note = {arXiv:2306.07961 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
}

@misc{zagatti_extending_2023,
	title = {Extending {JumpProcess}.jl for fast point process simulation with time-varying intensities},
	url = {http://arxiv.org/abs/2306.06992},
	doi = {10.21105/jcon.00133},
	abstract = {Point processes model the occurrence of a countable number of random points over some support. They can model diverse phenomena, such as chemical reactions, stock market transactions and social interactions. We show that JumpProcesses.jl is a fast, general-purpose library for simulating point processes. JumpProcesses.jl was first developed for simulating jump processes via stochastic simulation algorithms (SSAs) (including Doob's method, Gillespie's methods, and Kinetic Monte Carlo methods). Historically, jump processes have been developed in the context of dynamical systems to describe dynamics with discrete jumps. In contrast, the development of point processes has been more focused on describing the occurrence of random events. In this paper, we bridge the gap between the treatment of point and jump process simulation. The algorithms previously included in JumpProcesses.jl can be mapped to three general methods developed in statistics for simulating evolutionary point processes. Our comparative exercise revealed that the library initially lacked an efficient algorithm for simulating processes with variable intensity rates. We, therefore, extended JumpProcesses.jl with a new simulation algorithm, Coevolve, that enables the rapid simulation of processes with locally-bounded variable intensity rates. It is now possible to efficiently simulate any point process on the real line with a non-negative, left-continuous, history-adapted and locally bounded intensity rate coupled or not with differential equations. This extension significantly improves the computational performance of JumpProcesses.jl when simulating such processes, enabling it to become one of the few readily available, fast, general-purpose libraries for simulating evolutionary point processes.},
	urldate = {2025-02-18},
	author = {Zagatti, Guilherme Augusto and Isaacson, Samuel A. and Rackauckas, Christopher and Ilin, Vasily and Ng, See-Kiong and Bressan, Stéphane},
	month = jul,
	year = {2023},
	note = {arXiv:2306.06992 [stat]},
	keywords = {Computer Science - Mathematical Software, Statistics - Computation},
}

@article{roesch_julia_2023,
	title = {Julia for biologists},
	volume = {20},
	issn = {1548-7105},
	doi = {10.1038/s41592-023-01832-z},
	abstract = {Major computational challenges exist in relation to the collection, curation, processing and analysis of large genomic and imaging datasets, as well as the simulation of larger and more realistic models in systems biology. Here we discuss how a relative newcomer among programming languages-Julia-is poised to meet the current and emerging demands in the computational biosciences and beyond. Speed, flexibility, a thriving package ecosystem and readability are major factors that make high-performance computing and data analysis available to an unprecedented degree. We highlight how Julia's design is already enabling new ways of analyzing biological data and systems, and we provide a list of resources that can facilitate the transition into Julian computing.},
	language = {eng},
	number = {5},
	journal = {Nature Methods},
	author = {Roesch, Elisabeth and Greener, Joe G. and MacLean, Adam L. and Nassar, Huda and Rackauckas, Christopher and Holy, Timothy E. and Stumpf, Michael P. H.},
	month = may,
	year = {2023},
	pmid = {37024649},
	pmcid = {PMC10216852},
	keywords = {Computer Simulation, Computing Methodologies, Ecosystem, Programming Languages, Software, Systems Biology},
	pages = {655--664},
}

@misc{tarek_practitioners_2023,
	title = {A {Practitioner}'s {Guide} to {Bayesian} {Inference} in {Pharmacometrics} using {Pumas}},
	url = {http://arxiv.org/abs/2304.04752},
	doi = {10.48550/arXiv.2304.04752},
	abstract = {This paper provides a comprehensive tutorial for Bayesian practitioners in pharmacometrics using Pumas workflows. We start by giving a brief motivation of Bayesian inference for pharmacometrics highlighting limitations in existing software that Pumas addresses. We then follow by a description of all the steps of a standard Bayesian workflow for pharmacometrics using code snippets and examples. This includes: model definition, prior selection, sampling from the posterior, prior and posterior simulations and predictions, counter-factual simulations and predictions, convergence diagnostics, visual predictive checks, and finally model comparison with cross-validation. Finally, the background and intuition behind many advanced concepts in Bayesian statistics are explained in simple language. This includes many important ideas and precautions that users need to keep in mind when performing Bayesian analysis. Many of the algorithms, codes, and ideas presented in this paper are highly applicable to clinical research and statistical learning at large but we chose to focus our discussions on pharmacometrics in this paper to have a narrower scope in mind and given the nature of Pumas as a software primarily for pharmacometricians.},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Tarek, Mohamed and Storopoli, Jose and Davis, Casey and Elrod, Chris and Krumbiegel, Julius and Rackauckas, Chris and Ivaturi, Vijay},
	month = mar,
	year = {2023},
	note = {arXiv:2304.04752 [stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Statistics - Applications, Statistics - Computation},
}

@misc{bassik_robust_2023,
	title = {Robust {Parameter} {Estimation} for {Rational} {Ordinary} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2303.02159},
	doi = {10.48550/arXiv.2303.02159},
	abstract = {We present a new approach for estimating parameters in rational ODE models from given (measured) time series data. In typical existing approaches, an initial guess for the parameter values is made from a given search interval. Then, in a loop, the corresponding outputs are computed by solving the ODE numerically, followed by computing the error from the given time series data. If the error is small, the loop terminates and the parameter values are returned. Otherwise, heuristics/theories are used to possibly improve the guess and continue the loop. These approaches tend to be non-robust in the sense that their accuracy depend on the search interval and the true parameter values; furthermore, they cannot handle the case where the parameters are locally identifiable. In this paper, we propose a new approach, which does not suffer from the above non-robustness. In particular, it does not require making good initial guesses for the parameter values or specifying search intervals. Instead, it uses differential algebra, interpolation of the data using rational functions, and multivariate polynomial system solving. We also compare the performance of the resulting software with several other estimation software packages.},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Bassik, Oren and Berman, Yosef and Go, Soo and Hong, Hoon and Ilmer, Ilia and Ovchinnikov, Alexey and Rackauckas, Chris and Soto, Pedro and Yap, Chee},
	month = dec,
	year = {2023},
	note = {arXiv:2303.02159 [cs]},
	keywords = {Computer Science - Mathematical Software, Computer Science - Symbolic Computation, Mathematics - Dynamical Systems, Quantitative Biology - Quantitative Methods},
}

@inproceedings{holtorf_sum--squares_2023,
	title = {Sum-of-{Squares} {Bounds} for {Quantum} {Optimal} {Control}},
	volume = {02},
	url = {https://ieeexplore.ieee.org/abstract/document/10313626},
	doi = {10.1109/QCE57702.2023.10284},
	abstract = {The precise control of devices on the quantum scale is crucial for all quantum computing applications. The limitations of quantum control are therefore of great interest to those developing quantum hardware or researching quantum information protocols. We show how moment-sum-of-squares techniques can be combined with quantum dynamics and filtering theory to construct a hierarchy of convex optimization problems that furnish hard, computable bounds on the best achievable performance in quantum control tasks. These bounds can serve as witnesses of fundamental limitations, certificates of optimality, or performance targets and thus complement existing controller design strategies. In particular, our work allows us to compute bounds for common quantum control tasks, such as maximizing the fidelity of the final quantum state relative to a target state given a finite time budget or finding the minimum time required to reach the target state with near-unit fidelity. Our work also naturally applies to feedback-controlled quantum systems subjected to continuous observations, e.g., homodyne detection or photon counting, which are of emerging interest for the next generation of quantum devices. We provide an open-source package, MarkovBounds.jl, that is built upon the optimization ecosystem in the Julia programming language and exposes a high-level interface to the proposed bounding framework.},
	urldate = {2025-02-18},
	booktitle = {2023 {IEEE} {International} {Conference} on {Quantum} {Computing} and {Engineering} ({QCE})},
	author = {Holtorf, Flemming and Schäfer, Frank and Arnold, Julian and Rackauckas, Christopher and Edelman, Alan},
	month = sep,
	year = {2023},
	keywords = {convex optimization, dynamic programming, Filtering theory, Optimal control, Protocols, Quantum computing, quantum control software, quantum filtering, quantum optimal control, quantum speed limits, Quantum state, Quantum system, Software},
	pages = {365--366},
}

@inproceedings{pal_locally_2023,
	title = {Locally {Regularized} {Neural} {Differential} {Equations}: {Some} {Black} {Boxes} were meant to remain closed!},
	shorttitle = {Locally {Regularized} {Neural} {Differential} {Equations}},
	url = {https://proceedings.mlr.press/v202/pal23a.html},
	abstract = {Neural Differential Equations have become an important modeling framework due to their ability to adapt to new problems automatically. Training a neural differential equation is effectively a search over a space of plausible dynamical systems. Controlling the computational cost for these models is difficult since it relies on the number of steps the adaptive solver takes. Most prior works have used higher-order methods to reduce prediction timings while greatly increasing training time or reducing both training and prediction timings by relying on specific training algorithms, which are harder to use as a drop-in replacement. In this manuscript, we use internal cost heuristics of adaptive differential equation solvers at stochastic time-points to guide the training towards learning a dynamical system that is easier to integrate. We “close the blackbox” and allow the use of our method with any sensitivity method. We perform experimental studies to compare our method to global regularization to show that we attain similar performance numbers without compromising on the flexibility of implementation. We develop two sampling strategies to trade-off between performance and training time. Our method reduces the number of function evaluations to 0.556x - 0.733x and accelerates predictions by 1.3x - 2x.},
	language = {en},
	urldate = {2025-02-18},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Pal, Avik and Edelman, Alan and Rackauckas, Christopher Vincent},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {26809--26819},
}

@article{edelman_fifty_2023,
	title = {Fifty {Three} {Matrix} {Factorizations}: {A} {Systematic} {Approach}},
	volume = {44},
	issn = {0895-4798},
	shorttitle = {Fifty {Three} {Matrix} {Factorizations}},
	url = {https://epubs.siam.org/doi/abs/10.1137/21M1416035},
	doi = {10.1137/21M1416035},
	abstract = {Let A belong to an automorphism group, Lie algebra, or Jordan algebra of a scalar product. When A is factored, to what extent do the factors inherit structure from A? We answer this question for the principal matrix square root, the matrix sign decomposition, and the polar decomposition. For general A, we give a simple derivation and characterization of a particular generalized polar decomposition, and we relate it to other such decompositions in the literature. Finally, we study eigendecompositions and structured singular value decompositions, considering in particular the structure in eigenvalues, eigenvectors, and singular values that persists across a wide range of scalar products.A key feature of our analysis is the identification of two particular classes of scalar products, termed unitary and orthosymmetric, which serve to unify assumptions for the existence of structured factorizations. A variety of different characterizations of these scalar product classes are given.},
	number = {2},
	urldate = {2025-02-18},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Edelman, Alan and Jeong, Sungwoo},
	month = jun,
	year = {2023},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {415--480},
}

@misc{edelman_conditional_2023,
	title = {The conditional {DPP} approach to random matrix distributions},
	url = {http://arxiv.org/abs/2304.09319},
	doi = {10.48550/arXiv.2304.09319},
	abstract = {We present the conditional determinantal point process (DPP) approach to obtain new (mostly Fredholm determinantal) expressions for various eigenvalue statistics in random matrix theory. It is well-known that many (especially \${\textbackslash}beta=2\$) eigenvalue \$n\$-point correlation functions are given in terms of \$n{\textbackslash}times n\$ determinants, i.e., they are continuous DPPs. We exploit a derived kernel of the conditional DPP which gives the \$n\$-point correlation function conditioned on the event of some eigenvalues already existing at fixed locations. Using such kernels we obtain new determinantal expressions for the joint densities of the \$k\$ largest eigenvalues, probability density functions of the \$k{\textasciicircum}{\textbackslash}text\{th\}\$ largest eigenvalue, density of the first eigenvalue spacing, and more. Our formulae are highly amenable to numerical computations and we provide various numerical experiments. Several numerical values that required hours of computing time could now be computed in seconds with our expressions, which proves the effectiveness of our approach. We also demonstrate that our technique can be applied to an efficient sampling of DR paths of the Aztec diamond domino tiling. Further extending the conditional DPP sampling technique, we sample Airy processes from the extended Airy kernel. Additionally we propose a sampling method for non-Hermitian projection DPPs.},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Edelman, Alan and Jeong, Sungwoo},
	month = oct,
	year = {2023},
	note = {arXiv:2304.09319 [math-ph]},
	keywords = {Computer Science - Numerical Analysis, Mathematical Physics, Mathematics - Mathematical Physics, Mathematics - Numerical Analysis, Mathematics - Probability},
}

@article{edelman_structure_2023,
	title = {On the structure of the solutions to the matrix equation \textit{{G}}⁎\textit{{JG}} = \textit{{J}}},
	volume = {657},
	issn = {0024-3795},
	url = {https://www.sciencedirect.com/science/article/pii/S0024379522003664},
	doi = {10.1016/j.laa.2022.10.007},
	abstract = {We study the mathematical structure of the solution set (and its tangent space) to the matrix equation G⁎JG=J for a given square matrix J. In the language of pure mathematics, this is a Lie group which is the isometry group for a bilinear (or a sesquilinear) form. Generally these groups are described as intersections of a few special groups. The tangent space to \{G:G⁎JG=J\} consists of solutions to the linear matrix equation X⁎J+JX=0. For the complex case, the solution set of this linear equation was computed by De Terán and Dopico. We found that on its own, the equation X⁎J+JX=0 is hard to solve. By throwing into the mix the complementary linear equation X⁎J−JX=0, we find that the direct sum of the two solution sets is an easier to compute linear space. Thus, we obtain the two solution sets from projection maps. Not only is it possible to now solve the original problem, but we can approach the broader algebraic and geometric structure. One implication is that the two equations form an h and m pair familiar in the study of pseudo-Riemannian symmetric spaces. We explicitly demonstrate the computation of the solutions to the equation X⁎J±XJ=0 for real and complex matrices. However, real, complex or quaternionic case with an arbitrary involution (e.g., transpose, conjugate transpose, and the various quaternion transposes) can be effectively solved with the same strategy. We provide numerical examples and visualizations.},
	urldate = {2025-02-18},
	journal = {Linear Algebra and its Applications},
	author = {Edelman, Alan and Jeong, Sungwoo},
	month = jan,
	year = {2023},
	keywords = {Automorphism group, Lie group, Matrix congruence},
	pages = {241--273},
}

@misc{churavy_bridging_2022,
	title = {Bridging {HPC} {Communities} through the {Julia} {Programming} {Language}},
	url = {http://arxiv.org/abs/2211.02740},
	doi = {10.48550/arXiv.2211.02740},
	abstract = {The Julia programming language has evolved into a modern alternative to fill existing gaps in scientific computing and data science applications. Julia leverages a unified and coordinated single-language and ecosystem paradigm and has a proven track record of achieving high performance without sacrificing user productivity. These aspects make Julia a viable alternative to high-performance computing's (HPC's) existing and increasingly costly many-body workflow composition strategy in which traditional HPC languages (e.g., Fortran, C, C++) are used for simulations, and higher-level languages (e.g., Python, R, MATLAB) are used for data analysis and interactive computing. Julia's rapid growth in language capabilities, package ecosystem, and community make it a promising universal language for HPC. This paper presents the views of a multidisciplinary group of researchers from academia, government, and industry that advocate for an HPC software development paradigm that emphasizes developer productivity, workflow portability, and low barriers for entry. We believe that the Julia programming language, its ecosystem, and its community provide modern and powerful capabilities that enable this group's objectives. Crucially, we believe that Julia can provide a feasible and less costly approach to programming scientific applications and workflows that target HPC facilities. In this work, we examine the current practice and role of Julia as a common, end-to-end programming model to address major challenges in scientific reproducibility, data-driven AI/machine learning, co-design and workflows, scalability and performance portability in heterogeneous computing, network communication, data management, and community education. As a result, the diversification of current investments to fulfill the needs of the upcoming decade is crucial as more supercomputing centers prepare for the exascale era.},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Churavy, Valentin and Godoy, William F. and Bauer, Carsten and Ranocha, Hendrik and Schlottke-Lakemper, Michael and Räss, Ludovic and Blaschke, Johannes and Giordano, Mosè and Schnetter, Erik and Omlin, Samuel and Vetter, Jeffrey S. and Edelman, Alan},
	month = nov,
	year = {2022},
	note = {arXiv:2211.02740 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
}

@article{annevelink_automat_2022,
	title = {{AutoMat}: {Automated} materials discovery for electrochemical systems},
	volume = {47},
	issn = {1938-1425},
	shorttitle = {{AutoMat}},
	url = {https://doi.org/10.1557/s43577-022-00424-0},
	doi = {10.1557/s43577-022-00424-0},
	abstract = {Large-scale electrification is vital to addressing the climate crisis, but several scientific and technological challenges remain to fully electrify both the chemical industry and transportation. In both of these areas, new electrochemical materials will be critical, but their development currently relies heavily on human-time-intensive experimental trial and error and computationally expensive first-principles, mesoscale, and continuum simulations. We present an automated workflow, AutoMat, which accelerates these computational steps by introducing both automated input generation and management of simulations across scales from first principles to continuum device modeling. Furthermore, we show how to seamlessly integrate multi-fidelity predictions, such as machine learning surrogates or automated robotic experiments “in-the-loop.” The automated framework is implemented with design space search techniques to dramatically accelerate the overall materials discovery pipeline by implicitly learning design features that optimize device performance across several metrics. We discuss the benefits of AutoMat using examples in electrocatalysis and energy storage and highlight lessons learned.},
	language = {en},
	number = {10},
	urldate = {2025-02-18},
	journal = {MRS Bulletin},
	author = {Annevelink, Emil and Kurchin, Rachel and Muckley, Eric and Kavalsky, Lance and Hegde, Vinay I. and Sulzer, Valentin and Zhu, Shang and Pu, Jiankun and Farina, David and Johnson, Matthew and Gandhi, Dhairya and Dave, Adarsh and Lin, Hongyi and Edelman, Alan and Ramsundar, Bharath and Saal, James and Rackauckas, Christopher and Shah, Viral and Meredig, Bryce and Viswanathan, Venkatasubramanian},
	month = oct,
	year = {2022},
	keywords = {Artificial Intelligence, Autonomous research, Electrochemical synthesis, Energy storage, Machine learning, Robotics, Simulation},
	pages = {1036--1044},
}

@article{edelman_cartan_2022,
	title = {On the {Cartan} decomposition for classical random matrix ensembles},
	volume = {63},
	issn = {0022-2488},
	url = {https://doi.org/10.1063/5.0087010},
	doi = {10.1063/5.0087010},
	abstract = {We complete Dyson’s dream by cementing the links between symmetric spaces and classical random matrix ensembles. Previous work has focused on a one-to-one correspondence between symmetric spaces and many but not all of the classical random matrix ensembles. This work shows that we can completely capture all of the classical random matrix ensembles from Cartan’s symmetric spaces through the use of alternative coordinate systems. In the end, we have to let go of the notion of a one-to-one correspondence. We emphasize that the KAK decomposition traditionally favored by mathematicians is merely one coordinate system on the symmetric space, albeit a beautiful one. However, other matrix factorizations, especially the generalized singular value decomposition from numerical linear algebra, reveal themselves to be perfectly valid coordinate systems that one symmetric space can lead to many classical random matrix theories. We establish the connection between this numerical linear algebra viewpoint and the theory of generalized Cartan decompositions. This, in turn, allows us to produce yet more random matrix theories from a single symmetric space. Yet, again, these random matrix theories arise from matrix factorizations, though ones that we are not aware have appeared in the literature.},
	number = {6},
	urldate = {2025-02-18},
	journal = {Journal of Mathematical Physics},
	author = {Edelman, Alan and Jeong, Sungwoo},
	month = jun,
	year = {2022},
	pages = {061705},
}

@article{anantharaman_stably_2022,
	series = {9th {IFAC} {Conference} on {Foundations} of {Systems} {Biology} in {Engineering} {FOSBE} 2022},
	title = {Stably {Accelerating} {Stiff} {Quantitative} {Systems} {Pharmacology} {Models}: {Continuous}-{Time} {Echo} {State} {Networks} as {Implicit} {Machine} {Learning}},
	volume = {55},
	issn = {2405-8963},
	shorttitle = {Stably {Accelerating} {Stiff} {Quantitative} {Systems} {Pharmacology} {Models}},
	url = {https://www.sciencedirect.com/science/article/pii/S2405896323000046},
	doi = {10.1016/j.ifacol.2023.01.004},
	abstract = {In this paper, we propose the use of neural network surrogates of stiff QsP models, which reduces and accelerates QsP models by training ML approximations on simulations. We describe how common neural network methodologies, such as residual neural networks, recurrent neural networks, and physics/biologically-informed neural networks, are fundamentally related to explicit solvers of ordinary differential equations (ODEs), and thus are ill-equipped to deal with stiffness. To address this issue, we showcase methods from scientific machine learning (SciML) which combine techniques from mechanistic modeling with traditional deep learning. We describe the continuous-time echo state network (CTESN) as the implicit analogue of ML architectures. We demonstrate the CTESN's ability to surrogatize a production QsP model, a {\textgreater}1,000 ODE chemical reaction system from the SBML Biomodels repository, and a reaction-diffusion partial differential equation. We showcase the ability to accelerate QsP simulations by up to 5.2x against the optimized Differential Equations. jl solvers while achieving {\textless}5\% relative error. This shows how incorporating the numerical properties of QsP methods into ML can improve the intersection, and thus presents a potential method for accelerating repeated calculations such as global sensitivity analysis and virtual populations.},
	number = {23},
	urldate = {2025-02-18},
	journal = {IFAC-PapersOnLine},
	author = {Anantharaman, Ranjan and Abdelrehim, Anas and Jain, Anand and Pal, Avik and Sharp, Danny and {Utkarsh} and Edelman, Alan and Rackauckas, Chris},
	month = jan,
	year = {2022},
	keywords = {Machine Learning, Ordinary Differential Equations, Partial Differential Equations, Quantitative Systems Pharmacology, Surrogate Modeling},
	pages = {1--6},
}

@book{pal_mixing_2022,
	title = {Mixing {Implicit} and {Explicit} {Deep} {Learning} with {Skip} {DEQs} and {Infinite} {Time} {Neural} {ODEs} ({Continuous} {DEQs})},
	abstract = {Implicit deep learning architectures, like Neural ODEs and Deep Equilibrium Models (DEQs), separate the definition of a layer from the description of its solution process. While implicit layers allow features such as depth to adapt to new scenarios and inputs automatically, this adaptivity makes its computational expense challenging to predict. Numerous authors have noted that implicit layer techniques can be more computationally intensive than explicit layer methods. In this manuscript, we address the question: is there a way to simultaneously achieve the robustness of implicit layers while allowing the reduced computational expense of an explicit layer? To solve this we develop Skip DEQ, an implicit-explicit (IMEX) layer that simultaneously trains an explicit prediction followed by an implicit correction. We show that training this explicit layer is free and even decreases the training time by 2.5x and prediction time by 3.4x. We then further increase the "implicitness" of the DEQ by redefining the method in terms of an infinite time neural ODE which paradoxically decreases the training cost over a standard neural ODE by not requiring backpropagation through time. We demonstrate how the resulting Continuous Skip DEQ architecture trains more robustly than the original DEQ while achieving faster training and prediction times. Together, this manuscript shows how bridging the dichotomy of implicit and explicit deep learning can combine the advantages of both techniques.},
	author = {Pal, Avik and Edelman, Alan and Rackauckas, Chris},
	month = jan,
	year = {2022},
	doi = {10.48550/arXiv.2201.12240},
}

@article{arya_automatic_2022,
	title = {Automatic {Differentiation} of {Programs} with {Discrete} {Randomness}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/43d8e5fc816c692f342493331d5e98fc-Abstract-Conference.html},
	language = {en},
	urldate = {2025-02-18},
	journal = {Advances in Neural Information Processing Systems},
	author = {Arya, Gaurav and Schauer, Moritz and Schäfer, Frank and Rackauckas, Christopher},
	month = dec,
	year = {2022},
	pages = {10435--10447},
}

@article{iravanian_symbolic-numeric_2022,
	title = {Symbolic-numeric integration of univariate expressions based on sparse regression},
	volume = {56},
	issn = {1932-2232},
	url = {https://doi.org/10.1145/3572867.3572882},
	doi = {10.1145/3572867.3572882},
	abstract = {The majority of computer algebra systems (CAS) support symbolic integration using a combination of heuristic algebraic and rule-based (integration table) methods. In this paper, we present a hybrid (symbolic-numeric) method to calculate the indefinite integrals of univariate expressions. Our method is broadly similar to the Risch-Norman algorithm. The primary motivation for this work is to add symbolic integration functionality to a modern CAS (the symbolic manipulation packages of SciML, the Scientific Machine Learning ecosystem of the Julia programming language), which is designed for numerical and machine learning applications. The symbolic part of our method is based on the combination of candidate terms generation (ansatz generation using a methodology borrowed from the Homotopy operators theory) combined with rule-based expression transformations provided by the underlying CAS. The numeric part uses sparse regression, a component of the Sparse Identification of Nonlinear Dynamics (SINDy) technique, to find the coefficients of the candidate terms. We show that this system can solve a large variety of common integration problems using only a few dozen basic integration rules.},
	number = {2},
	urldate = {2025-02-18},
	journal = {ACM Commun. Comput. Algebra},
	author = {Iravanian, Shahriar and Martensen, Carl Julius and Cheli, Alessandro and Gowda, Shashi and Jain, Anand and Ma, Yingbo and Rackauckas, Chris},
	month = nov,
	year = {2022},
	pages = {84--87},
}

@article{roberts_continuous-time_2022,
	title = {Continuous-time echo state networks for predicting power system dynamics},
	volume = {212},
	issn = {0378-7796},
	url = {https://www.sciencedirect.com/science/article/pii/S0378779622006587},
	doi = {10.1016/j.epsr.2022.108562},
	abstract = {With the growing penetration of converter-interfaced generation in power systems, the dynamical behavior of these systems is rapidly evolving. One of the challenges with converter-interfaced generation is the increased number of equations, as well as the required numerical timestep, involved in simulating these systems. Within this work, we explore the use of continuous-time echo state networks as a means to cheaply, and accurately, predict the dynamic response of power systems subject to a disturbance for varying system parameters. We show an application for predicting frequency dynamics following a loss of generation for varying penetrations of grid-following and grid-forming converters. We demonstrate that, after training on 20 solutions of the full-order system, we achieve a median nadir prediction error of 0.17 mHz with 95\% of all nadir prediction errors within ±4 mHz. We conclude with some discussion on how this approach can be used for parameter sensitivity analysis and within optimization algorithms to rapidly predict the dynamical behavior of the system.},
	urldate = {2025-02-18},
	journal = {Electric Power Systems Research},
	author = {Roberts, Ciaran and Lara, José Daniel and Henriquez-Auba, Rodrigo and Bossart, Matthew and Anantharaman, Ranjan and Rackauckas, Chris and Hodge, Bri-Mathias and Callaway, Duncan S.},
	month = nov,
	year = {2022},
	keywords = {Data-driven modeling techniques, Electro-magnetic transients, Machine learning, Power system dynamics},
	pages = {108562},
}

@article{shankar_validation_2022,
	title = {Validation and parameterization of a novel physics-constrained neural dynamics model applied to turbulent fluid flow},
	volume = {34},
	issn = {1070-6631},
	url = {https://doi.org/10.1063/5.0122115},
	doi = {10.1063/5.0122115},
	abstract = {In fluid physics, data-driven models to enhance or accelerate time to solution are becoming increasingly popular for many application domains, such as alternatives to turbulence closures, system surrogates, or for new physics discovery. In the context of reduced order models of high-dimensional time-dependent fluid systems, machine learning methods grant the benefit of automated learning from data, but the burden of a model lies on its reduced-order representation of both the fluid state and physical dynamics. In this work, we build a physics-constrained, data-driven reduced order model for Navier–Stokes equations to approximate spatiotemporal fluid dynamics in the canonical case of isotropic turbulence in a triply periodic box. The model design choices mimic numerical and physical constraints by, for example, implicitly enforcing the incompressibility constraint and utilizing continuous neural ordinary differential equations for tracking the evolution of the governing differential equation. We demonstrate this technique on a three-dimensional, moderate Reynolds number turbulent fluid flow. In assessing the statistical quality and characteristics of the machine-learned model through rigorous diagnostic tests, we find that our model is capable of reconstructing the dynamics of the flow over large integral timescales, favoring accuracy at the larger length scales. More significantly, comprehensive diagnostics suggest that physically interpretable model parameters, corresponding to the representations of the fluid state and dynamics, have attributable and quantifiable impact on the quality of the model predictions and computational complexity.},
	number = {11},
	urldate = {2025-02-18},
	journal = {Physics of Fluids},
	author = {Shankar, Varun and Portwood, Gavin D. and Mohan, Arvind T. and Mitra, Peetak P. and Krishnamurthy, Dilip and Rackauckas, Christopher and Wilson, Lucas A. and Schmidt, David P. and Viswanathan, Venkatasubramanian},
	month = nov,
	year = {2022},
	pages = {115110},
}

@misc{childers_differentiable_2022,
	type = {Working {Paper}},
	series = {Working {Paper} {Series}},
	title = {Differentiable {State}-{Space} {Models} and {Hamiltonian} {Monte} {Carlo} {Estimation}},
	url = {https://www.nber.org/papers/w30573},
	doi = {10.3386/w30573},
	abstract = {We propose a methodology to take dynamic stochastic general equilibrium (DSGE) models to the data based on the combination of differentiable state-space models and the Hamiltonian Monte Carlo (HMC) sampler. First, we introduce a method for implicit automatic differentiation of perturbation solutions of DSGE models with respect to the model's parameters. We can use the resulting output for various tasks requiring gradients, such as building an HMC sampler, to estimate first- and second-order approximations of DSGE models. The availability of derivatives also enables a general filter-free method to estimate nonlinear, non-Gaussian DSGE models by sampling the joint likelihood of parameters and latent states. We show that the gradient-based joint likelihood sampling approach is superior in efficiency and robustness to standard Metropolis-Hastings samplers by estimating a canonical real business cycle model, a real small open economy model, and a medium-scale New Keynesian DSGE model.},
	urldate = {2025-02-18},
	publisher = {National Bureau of Economic Research},
	author = {Childers, David and Fernández-Villaverde, Jesús and Perla, Jesse and Rackauckas, Christopher and Wu, Peifan},
	month = oct,
	year = {2022},
	doi = {10.3386/w30573},
}

@inproceedings{utkarsh_parallelizing_2022,
	title = {Parallelizing {Explicit} and {Implicit} {Extrapolation} {Methods} for {Ordinary} {Differential} {Equations}},
	url = {https://ieeexplore.ieee.org/abstract/document/9926357},
	doi = {10.1109/HPEC55821.2022.9926357},
	abstract = {Numerically solving ordinary differential equations (ODEs) is a naturally serial process and as a result the vast majority of ODE solver software are serial. In this manuscript we developed a set of parallelized ODE solvers using extrapolation methods which exploit “parallelism within the method” so that arbitrary user ODEs can be parallelized. We describe the specific choices made in the implementation of the explicit and implicit extrapolation methods which allow for generating low overhead static schedules to then exploit with optimized multi-threaded implementations. We demonstrate that while the multi-threading gives a noticeable acceleration on both explicit and implicit problems, the explicit parallel extrapolation methods gave no significant improvement over state-of-the-art even with a multi-threading advantage against current optimized high order Runge- Kutta tableaus. However, we demonstrate that the implicit parallel extrapolation methods are able to achieve state-of-the-art performance (2x-4x) on standard multicore x86 CPUs for systems of {\textless} 200 stiff ODEs solved at low tolerance, a typical setup for a vast majority of users of high level language equation solver suites. The resulting method is distributed as the first widely available open source software for within-method parallel acceleration targeting typical modest compute architectures.},
	urldate = {2025-02-18},
	booktitle = {2022 {IEEE} {High} {Performance} {Extreme} {Computing} {Conference} ({HPEC})},
	author = {{Utkarsh} and Elrod, Chris and Ma, Yingbo and Althaus, Konstantin and Rackauckas, Christopher},
	month = sep,
	year = {2022},
	note = {ISSN: 2643-1971},
	keywords = {Computer architecture, Extrapolation, High level languages, Multicore processing, Ordinary differential equations, Parallel processing, Schedules},
	pages = {1--9},
}

@misc{holtorf_stochastic_2022,
	title = {Stochastic {Optimal} {Control} via {Local} {Occupation} {Measures}},
	url = {http://arxiv.org/abs/2211.15652},
	doi = {10.48550/arXiv.2211.15652},
	abstract = {Viewing stochastic processes through the lens of occupation measures has proved to be a powerful angle of attack for the theoretical and computational analysis of stochastic optimal control problems. We present a simple modification of the traditional occupation measure framework derived from resolving the occupation measures locally on a partition of the control problem's space-time domain. This notion of local occupation measures provides fine-grained control over the construction of structured semidefinite programming relaxations for a rich class of stochastic optimal control problems with embedded diffusion and jump processes via the moment-sum-of-squares hierarchy. As such, it bridges the gap between discretization-based approximations to the Hamilton-Jacobi-Bellmann equations and occupation measure relaxations. We demonstrate with examples that this approach enables the computation of high quality bounds for the optimal value of a large class of stochastic optimal control problems with significant performance gains relative to the traditional occupation measure framework.},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Holtorf, Flemming and Edelman, Alan and Rackauckas, Christopher},
	month = jan,
	year = {2025},
	note = {arXiv:2211.15652 [math]},
	keywords = {Computer Science - Systems and Control, Electrical Engineering and Systems Science - Systems and Control, Mathematics - Optimization and Control},
}

@techreport{acquesta_model-form_2022,
	title = {Model-{Form} {Epistemic} {Uncertainty} {Quantification} for {Modeling} with {Differential} {Equations}: {Application} to {Epidemiology}},
	shorttitle = {Model-{Form} {Epistemic} {Uncertainty} {Quantification} for {Modeling} with {Differential} {Equations}},
	url = {https://www.osti.gov/biblio/1888443},
	abstract = {Modeling real-world phenomena to any degree of accuracy is a challenge that the scientific research community has navigated since its foundation. Lack of information and limited computational and observational resources necessitate modeling assumptions which, when invalid, lead to model-form error (MFE). The work reported herein explored a novel method to represent model-form uncertainty (MFU) that combines Bayesian statistics with the emerging field of universal differential equations (UDEs). The fundamental principle behind UDEs is simple: use known equational forms that govern a dynamical system when you have them; then incorporate data-driven approaches – in this case neural networks (NNs) – embedded within the governing equations to learn the interacting terms that were underrepresented. Utilizing epidemiology as our motivating exemplar, this report will highlight the challenges of modeling novel infectious diseases while introducing ways to incorporate NN approximations to MFE. Prior to embarking on a Bayesian calibration, we first explored methods to augment the standard (non-Bayesian) UDE training procedure to account for uncertainty and increase robustness of training. In addition, it is often the case that uncertainty in observations is significant; this may be due to randomness or lack of precision in the measurement process. This uncertainty typically manifests as “noisy” observations which deviate from a true underlying signal. To account for such variability, the NN approximation to MFE is endowed with a probabilistic representation and is updated using available observational data in a Bayesian framework. By representing the MFU explicitly and deploying an embedded, data-driven model, this approach enables an agile, expressive, and interpretable method for representing MFU. In this report we will provide evidence that Bayesian UDEs show promise as a novel framework for any science-based, data-driven MFU representation; while emphasizing that significant advances must be made in the calibration of Bayesian NNs to ensure a robust calibration procedure.},
	language = {English},
	number = {SAND2022-12823},
	urldate = {2025-02-18},
	institution = {Sandia National Lab. (SNL-NM), Albuquerque, NM (United States)},
	author = {Acquesta, Erin C. S. and Portone, Teresa and Dandekar, Raj and Rackauckas, Chris and Bandy, Rileigh and Huerta, Gabriel and Dytzel, India},
	month = sep,
	year = {2022},
	doi = {10.2172/1888443},
}

@misc{widmann_delaydiffeq_2022,
	title = {{DelayDiffEq}: {Generating} {Delay} {Differential} {Equation} {Solvers} via {Recursive} {Embedding} of {Ordinary} {Differential} {Equation} {Solvers}},
	shorttitle = {{DelayDiffEq}},
	url = {http://arxiv.org/abs/2208.12879},
	doi = {10.48550/arXiv.2208.12879},
	abstract = {Traditional solvers for delay differential equations (DDEs) are designed around only a single method and do not effectively use the infrastructure of their more-developed ordinary differential equation (ODE) counterparts. In this work we present DelayDiffEq, a Julia package for numerically solving delay differential equations (DDEs) which leverages the multitude of numerical algorithms in OrdinaryDiffEq for solving both stiff and non-stiff ODEs, and manages to solve challenging stiff DDEs. We describe how compiling the ODE integrator within itself, and accounting for discontinuity propagation, leads to a design that is effective for DDEs while using all of the ODE internals. We highlight some difficulties that a numerical DDE solver has to address, and explain how DelayDiffEq deals with these problems. We show how DelayDiffEq is able to solve difficult equations, how its stiff DDE solvers give efficiency on problems with time-scale separation, and how the design allows for generality and flexibility in usage such as being repurposed for generating solvers for stochastic delay differential equations.},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Widmann, David and Rackauckas, Chris},
	month = aug,
	year = {2022},
	note = {arXiv:2208.12879 [math]},
	keywords = {Computer Science - Mathematical Software, Computer Science - Numerical Analysis, Mathematics - Dynamical Systems, Mathematics - Numerical Analysis},
}

@article{dixit_globalsensitivityjl_2022,
	title = {{GlobalSensitivity}.jl: {Performant} and {Parallel} {Global} {Sensitivity} {Analysis} with {Julia}},
	volume = {7},
	issn = {2475-9066},
	shorttitle = {{GlobalSensitivity}.jl},
	url = {https://joss.theoj.org/papers/10.21105/joss.04561},
	doi = {10.21105/joss.04561},
	abstract = {Dixit et al., (2022). GlobalSensitivity.jl: Performant and Parallel Global Sensitivity Analysis with Julia. Journal of Open Source Software, 7(76), 4561, https://doi.org/10.21105/joss.04561},
	language = {en},
	number = {76},
	urldate = {2025-02-18},
	journal = {Journal of Open Source Software},
	author = {Dixit, Vaibhav Kumar and Rackauckas, Christopher},
	month = aug,
	year = {2022},
	pages = {4561},
}

@article{zhang_two_2022,
	title = {Two heads are better than one: current landscape of integrating {QSP} and machine learning},
	volume = {49},
	issn = {1573-8744},
	shorttitle = {Two heads are better than one},
	url = {https://doi.org/10.1007/s10928-022-09805-z},
	doi = {10.1007/s10928-022-09805-z},
	abstract = {Quantitative systems pharmacology (QSP) modeling is applied to address essential questions in drug development, such as the mechanism of action of a therapeutic agent and the progression of disease. Meanwhile, machine learning (ML) approaches also contribute to answering these questions via the analysis of multi-layer ‘omics’ data such as gene expression, proteomics, metabolomics, and high-throughput imaging. Furthermore, ML approaches can also be applied to aspects of QSP modeling. Both approaches are powerful tools and there is considerable interest in integrating QSP modeling and ML. So far, a few successful implementations have been carried out from which we have learned about how each approach can overcome unique limitations of the other. The QSP + ML working group of the International Society of Pharmacometrics QSP Special Interest Group was convened in September, 2019 to identify and begin realizing new opportunities in QSP and ML integration. The working group, which comprises 21 members representing 18 academic and industry organizations, has identified four categories of current research activity which will be described herein together with case studies of applications to drug development decision making. The working group also concluded that the integration of QSP and ML is still in its early stages of moving from evaluating available technical tools to building case studies. This paper reports on this fast-moving field and serves as a foundation for future codification of best practices.},
	language = {en},
	number = {1},
	urldate = {2025-02-18},
	journal = {Journal of Pharmacokinetics and Pharmacodynamics},
	author = {Zhang, Tongli and Androulakis, Ioannis P. and Bonate, Peter and Cheng, Limei and Helikar, Tomáš and Parikh, Jaimit and Rackauckas, Christopher and Subramanian, Kalyanasundaram and Cho, Carolyn R. and Androulakis, Ioannis P. and Bonate, Peter and Borisov, Ivan and Broderick, Gordon and Cheng, Limei and Damian, Valeriu and Dariolli, Rafael and Demin, Oleg and Ellinwood, Nicholas and Fey, Dirk and Gulati, Abhishek and Helikar, Tomas and Jordie, Eric and Musante, Cynthia and Parikh, Jaimit and Rackauckas, Christopher and Saez-Rodriguez, Julio and Sobie, Eric and Subramanian, Kalyanasundaram and Cho, Carolyn R. and {on behalf of the Working Group}},
	month = feb,
	year = {2022},
	keywords = {Commentary, Machine learning, QSP, Review},
	pages = {5--18},
}

@article{martinuzzi_reservoircomputingjl_2022,
	title = {{ReservoirComputing}.jl: {An} {Efficient} and {Modular} {Library} for {Reservoir} {Computing} {Models}},
	volume = {23},
	issn = {1533-7928},
	shorttitle = {{ReservoirComputing}.jl},
	url = {http://jmlr.org/papers/v23/22-0611.html},
	abstract = {We introduce ReservoirComputing.jl, an open source Julia library for reservoir computing models. It is designed for temporal or sequential tasks such as time series prediction and modeling complex dynamical systems. As such it is suited to process a range of complex spatio-temporal data sets, from mathematical models to climate data. The key ideas of reservoir computing are the model architecture, i.e. the reservoir, which embeds the input into a higher dimensional space, and the learning paradigm, where only the readout layer is trained. As a result the computational resources can be kept low, and only linear optimization is required for the training. Although reservoir computing has proven itself as a successful machine learning algorithm, the software implementations have lagged behind, hindering wide recognition, reproducibility, and uptake by general scientists. ReservoirComputing.jl enhances this field by being intuitive, highly modular, and faster compared to alternative tools. A variety of modular components from the literature are implemented, e.g. two reservoir types - echo state networks and cellular automata models, and multiple training methods including Gaussian and support vector regression. A comprehensive documentation, which includes reproduced experiments from the literature is provided. The code and documentation are hosted on Github under an MIT license https://github.com/SciML/ReservoirComputing.jl.},
	number = {288},
	urldate = {2025-02-18},
	journal = {Journal of Machine Learning Research},
	author = {Martinuzzi, Francesco and Rackauckas, Chris and Abdelrehim, Anas and Mahecha, Miguel D. and Mora, Karin},
	year = {2022},
	pages = {1--8},
}

@misc{christ_plotsjl_2022,
	title = {Plots.jl -- a user extendable plotting {API} for the julia programming language},
	url = {http://arxiv.org/abs/2204.08775},
	doi = {10.48550/arXiv.2204.08775},
	abstract = {There are plenty of excellent plotting libraries. Each excels at a different use case: one is good for printed 2D publication figures, the other at interactive 3D graphics, a third has excellent L A TEX integration or is good for creating dashboards on the web. The aim of Plots.jl is to enable the user to use the same syntax to interact with many different plotting libraries, such that it is possible to change the library "backend" without needing to touch the code that creates the content -- and without having to learn yet another application programming interface (API). This is achieved by the separation of the plot specification from the implementation of the actual graphical backend. These plot specifications may be extended by a "recipe" system, which allows package authors and users to define how to plot any new type (be it a statistical model, a map, a phylogenetic tree or the solution to a system of differential equations) and create new types of plots -- without depending on the Plots.jl package. This supports a modular ecosystem structure for plotting and yields a high reuse potential across the entire julia package ecosystem. Plots.jl is publicly available at https://github.com/JuliaPlots/Plots.jl.},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Christ, Simon and Schwabeneder, Daniel and Rackauckas, Christopher and Borregaard, Michael Krabbe and Breloff, Thomas},
	month = jun,
	year = {2022},
	note = {arXiv:2204.08775 [cs]},
	keywords = {Computer Science - Graphics},
}

@article{mester_differential_2022,
	title = {Differential methods for assessing sensitivity in biological models},
	volume = {18},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009598},
	doi = {10.1371/journal.pcbi.1009598},
	abstract = {Differential sensitivity analysis is indispensable in fitting parameters, understanding uncertainty, and forecasting the results of both thought and lab experiments. Although there are many methods currently available for performing differential sensitivity analysis of biological models, it can be difficult to determine which method is best suited for a particular model. In this paper, we explain a variety of differential sensitivity methods and assess their value in some typical biological models. First, we explain the mathematical basis for three numerical methods: adjoint sensitivity analysis, complex perturbation sensitivity analysis, and forward mode sensitivity analysis. We then carry out four instructive case studies. (a) The CARRGO model for tumor-immune interaction highlights the additional information that differential sensitivity analysis provides beyond traditional naive sensitivity methods, (b) the deterministic SIR model demonstrates the value of using second-order sensitivity in refining model predictions, (c) the stochastic SIR model shows how differential sensitivity can be attacked in stochastic modeling, and (d) a discrete birth-death-migration model illustrates how the complex perturbation method of differential sensitivity can be generalized to a broader range of biological models. Finally, we compare the speed, accuracy, and ease of use of these methods. We find that forward mode automatic differentiation has the quickest computational time, while the complex perturbation method is the simplest to implement and the most generalizable.},
	language = {en},
	number = {6},
	urldate = {2025-02-18},
	journal = {PLOS Computational Biology},
	author = {Mester, Rachel and Landeros, Alfonso and Rackauckas, Chris and Lange, Kenneth},
	month = jun,
	year = {2022},
	note = {Publisher: Public Library of Science},
	keywords = {Algorithms, Biologists, Cancers and neoplasms, Computer software, Computing methods, Differential equations, Immune cells, Software tools},
	pages = {e1009598},
}

@inproceedings{deshpande_constrained_2022,
	title = {Constrained {Smoothers} for {State} {Estimation} of {Vapor} {Compression} {Cycles}},
	url = {https://ieeexplore.ieee.org/abstract/document/9867269},
	doi = {10.23919/ACC53348.2022.9867269},
	abstract = {State estimators can be a powerful tool in the development of advanced controls and performance monitoring capabilities for vapor compression cycles, but the nonlinear and numerically stiff aspects of these systems pose challenges for the practical implementation of estimators on large physics-based models. We develop smoothing methods in the extended and ensemble Kalman estimation frameworks that satisfy physical constraints and address practical limitations with standard implementations of these estimators. These methods are tested on a model built in the Julia language, and are demonstrated to successfully estimate unmeasured variables with high accuracy.},
	urldate = {2025-02-18},
	booktitle = {2022 {American} {Control} {Conference} ({ACC})},
	author = {Deshpande, Vedang M. and Laughman, Christopher R. and Ma, Yingbo and Rackauckas, Chris},
	month = jun,
	year = {2022},
	note = {ISSN: 2378-5861},
	keywords = {constrained smoothing, ensemble Kalman, extended Kalman, Kalman filters, Monitoring, Numerical models, Smoothing methods, Standards, state estimation, State estimation, Vapor compression cycle},
	pages = {2333--2340},
}

@misc{schafer_abstractdifferentiationjl_2022,
	title = {{AbstractDifferentiation}.jl: {Backend}-{Agnostic} {Differentiable} {Programming} in {Julia}},
	shorttitle = {{AbstractDifferentiation}.jl},
	url = {http://arxiv.org/abs/2109.12449},
	doi = {10.48550/arXiv.2109.12449},
	abstract = {No single Automatic Differentiation (AD) system is the optimal choice for all problems. This means informed selection of an AD system and combinations can be a problem-specific variable that can greatly impact performance. In the Julia programming language, the major AD systems target the same input and thus in theory can compose. Hitherto, switching between AD packages in the Julia Language required end-users to familiarize themselves with the user-facing API of the respective packages. Furthermore, implementing a new, usable AD package required AD package developers to write boilerplate code to define convenience API functions for end-users. As a response to these issues, we present AbstractDifferentiation.jl for the automatized generation of an extensive, unified, user-facing API for any AD package. By splitting the complexity between AD users and AD developers, AD package developers only need to implement one or two primitive definitions to support various utilities for AD users like Jacobians, Hessians and lazy product operators from native primitives such as pullbacks or pushforwards, thus removing tedious -- but so far inevitable -- boilerplate code, and enabling the easy switching and composing between AD implementations for end-users.},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Schäfer, Frank and Tarek, Mohamed and White, Lyndon and Rackauckas, Chris},
	month = feb,
	year = {2022},
	note = {arXiv:2109.12449 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Computer Science - Software Engineering},
}

@inproceedings{rackauckas_composing_2022,
	title = {Composing {Modeling} {And} {Simulation} {With} {Machine} {Learning} {In} {Julia}},
	doi = {10.23919/ANNSIM55834.2022.9859453},
	abstract = {In this paper we introduce JuliaSim, a high-performance programming environment designed to blend traditional modeling and simulation with machine learning. JuliaSim can build surrogates from component-based models, including Functional Mockup Units, using continuous-time echo state networks (CTESN). The foundation of this environment, Modeling-Toolkit.jl, is an acausal-modeling language which can compose the trained surrogates as components. We present the JuliaSim model library, consisting of differential-algebraic equations and pre-trained surrogates, which can be composed using the modeling system. We demonstrate a surrogate-accelerated approach on HVAC dynamics by showing that the CTESN surrogates capture dynamics at less than 4\% error with an acceleration of 340x, and speed up design optimization by two orders of magnitude. We showcase the surrogate deployed in a co-simulation loop allowing engineers to explore the design space of a coupled system. Together this demonstrates a workflow for automating the integration of machine learning into traditional modeling and simulation.},
	booktitle = {2022 {Annual} {Modeling} and {Simulation} {Conference} ({ANNSIM})},
	author = {Rackauckas, Chris and Gwozdz, Maja and Jain, Anand and Ma, Yingbo and Martinuzzi, Francesco and Rajput, Utkarsh and Saba, Elliot and Shah, Viral B. and Anantharaman, Ranjan and Edelman, Alan and Gowda, Shashi and Pal, Avik and Laughman, Chris},
	month = jul,
	year = {2022},
	keywords = {acceleration, co-simulation, Functional Mock-up Interface, Heuristic algorithms, Julia, Libraries, Machine learning, Machine learning algorithms, Mathematical models, scientific machine learning, sciml, Software, Software algorithms, surrogate modeling},
	pages = {1--17},
}

@inproceedings{pestourie_data-efficient_2021,
	title = {Data-{Efficient} {Training} with {Physics}-{Enhanced} {Deep} {Surrogates}},
	url = {https://openreview.net/forum?id=vQmS8ueWIFm},
	abstract = {We present a “physics-enhanced deep-surrogate” (“PEDS”) approach to fast surrogate models for complex physical systems described by partial differential equations (PDEs) and similar models: we embed a low-fidelity “coarse” solver layer in a neural network that generates “coarsified” inputs, trained end-to-end to globally match the output of an expensive high-fidelity numerical solver. In this way, by incorporating complex physical knowledge in the form of the low-fidelity model, we find that a PEDS surrogate can be trained with at least 10× less data than a “black-box” neural network for the same accuracy. Asymptotically, PEDS appears to learn with a steeper power law than black-box surrogates, and benefits even further in combination with active learning. We demonstrate this using an example problem in electromagnetic scattering that appears in the large-scale optimization of optical metamaterials using scientific computing.},
	language = {en},
	urldate = {2025-02-18},
	author = {Pestourie, Raphaël and Mroueh, Youssef and Rackauckas, Christopher Vincent and Das, Payel and Johnson, Steven Glenn},
	month = dec,
	year = {2021},
}

@article{gowda_high-performance_2021,
	title = {High-performance symbolic-numerics via multiple dispatch},
	volume = {55},
	issn = {1932-2240},
	url = {https://dl.acm.org/doi/10.1145/3511528.3511535},
	doi = {10.1145/3511528.3511535},
	abstract = {As mathematical computing becomes more democratized in high-level languages, high-performance symbolic-numeric systems are necessary for domain scientists and engineers to get the best performance out of their machine without deep knowledge of code optimization. Naturally, users need diﬀerent term types either to have diﬀerent algebraic properties for them, or to use eﬃcient data structures. To this end, we developed Symbolics.jl, an extendable symbolic system which uses dynamic multiple dispatch to change behavior depending on the domain needs. In this work we detail an underlying abstract term interface which allows for speed without sacriﬁcing generality. We show that by formalizing a generic API on actions independent of implementation, we can retroactively add optimized data structures to our system without changing the pre-existing term rewriters. We showcase how this can be used to optimize term construction and give a 113x acceleration on general symbolic transformations. Further, we show that such a generic API allows for complementary term-rewriting implementations. Exploiting this feature, we demonstrate the ability to swap between classical term-rewriting simpliﬁers and e-graphbased term-rewriting simpliﬁers. We illustrate how this symbolic system improves numerical computing tasks by showcasing an e-graph ruleset which minimizes the number of CPU cycles during expression evaluation, and demonstrate how it simpliﬁes a real-world reaction-network simulation to halve the runtime. Additionally, we show a reaction-diﬀusion partial diﬀerential equation solver which is able to be automatically converted into symbolic expressions via multiple dispatch tracing, which is subsequently accelerated and parallelized to give a 157x simulation speedup. Together, this presents Symbolics.jl as a next-generation symbolic-numeric computing environment geared towards modeling and simulation.},
	language = {en},
	number = {3},
	urldate = {2022-12-22},
	journal = {ACM Communications in Computer Algebra},
	author = {Gowda, Shashi and Ma, Yingbo and Cheli, Alessandro and Gwóźzdź, Maja and Shah, Viral B. and Edelman, Alan and Rackauckas, Christopher},
	month = sep,
	year = {2021},
	pages = {92--96},
}

@inproceedings{ma_comparison_2021,
	title = {A {Comparison} of {Automatic} {Differentiation} and {Continuous} {Sensitivity} {Analysis} for {Derivatives} of {Differential} {Equation} {Solutions}},
	url = {https://ieeexplore.ieee.org/abstract/document/9622796},
	doi = {10.1109/HPEC49654.2021.9622796},
	abstract = {Derivatives of differential equation solutions are commonly for parameter estimation, fitting neural differential equations, and as model diagnostics. However, with a litany of choices and a Cartesian product of potential methods, it can be difficult for practitioners to understand which method is likely to be the most effective on their particular application. In this manuscript we investigate the performance characteristics of Discrete Local Sensitivity Analysis implemented via Automatic Differentiation (DSAAD) against continuous adjoint sensitivity analysis. Non-stiff and stiff biological and pharmacometric models, including a PDE discretization, are used to quantify the performance of sensitivity analysis methods. Our benchmarks show that on small stiff and non-stiff systems of ODEs (approximately {\textless} 100 parameters+ODEs), forward-mode DSAAD is more efficient than both reverse-mode and continuous forward/adjoint sensitivity analysis. The scalability of continuous adjoint methods is shown to be more efficient than discrete adjoints and forward methods after crossing this size range. These comparative studies demonstrate a trade-off between memory usage and performance in the continuous adjoint methods that should be considered when choosing the technique, while numerically unstable backsolve techniques from the machine learning literature are demonstrated as unsuitable for most scientific models. The performance of adjoint methods is shown to be heavily tied to the reverse-mode AD method used for the vector-Jacobian product calculations, with tape-based AD methods shown to be 2 orders of magnitude slower on nonlinear partial differential equations than static AD techniques. In addition, these results demonstrate the out-of-the-box applicability of DSAAD to differential-algebraic equations, delay differential equations, and hybrid differential equation systems where the event timing and effects are dependent on model parameters, showcasing an ease of implementation advantage for DSAAD approaches. Together, these benchmarks provide a guide to help practitioners to quickly identify the best mixture of continuous sensitivities and automatic differentiation for their applications.},
	urldate = {2025-02-18},
	booktitle = {2021 {IEEE} {High} {Performance} {Extreme} {Computing} {Conference} ({HPEC})},
	author = {Ma, Yingbo and Dixit, Vaibhav and Innes, Michael J and Guo, Xingjian and Rackauckas, Chris},
	month = sep,
	year = {2021},
	note = {ISSN: 2643-1971},
	keywords = {Benchmark testing, Fitting, Machine learning, Parameter estimation, Partial differential equations, Scalability, Sensitivity analysis},
	pages = {1--9},
}

@article{kim_stiff_2021,
	title = {Stiff neural ordinary differential equations},
	volume = {31},
	issn = {1054-1500},
	url = {https://doi.org/10.1063/5.0060697},
	doi = {10.1063/5.0060697},
	abstract = {Neural Ordinary Differential Equations (ODEs) are a promising approach to learn dynamical models from time-series data in science and engineering applications. This work aims at learning neural ODEs for stiff systems, which are usually raised from chemical kinetic modeling in chemical and biological systems. We first show the challenges of learning neural ODEs in the classical stiff ODE systems of Robertson’s problem and propose techniques to mitigate the challenges associated with scale separations in stiff systems. We then present successful demonstrations in stiff systems of Robertson’s problem and an air pollution problem. The demonstrations show that the usage of deep networks with rectified activations, proper scaling of the network outputs as well as loss functions, and stabilized gradient calculations are the key techniques enabling the learning of stiff neural ODEs. The success of learning stiff neural ODEs opens up possibilities of using neural ODEs in applications with widely varying time-scales, such as chemical dynamics in energy conversion, environmental engineering, and life sciences.},
	number = {9},
	urldate = {2025-02-18},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Kim, Suyong and Ji, Weiqi and Deng, Sili and Ma, Yingbo and Rackauckas, Christopher},
	month = sep,
	year = {2021},
	pages = {093122},
}

@misc{zubov_neuralpde_2021,
	title = {{NeuralPDE}: {Automating} {Physics}-{Informed} {Neural} {Networks} ({PINNs}) with {Error} {Approximations}},
	shorttitle = {{NeuralPDE}},
	url = {http://arxiv.org/abs/2107.09443},
	doi = {10.48550/arXiv.2107.09443},
	abstract = {Physics-informed neural networks (PINNs) are an increasingly powerful way to solve partial differential equations, generate digital twins, and create neural surrogates of physical models. In this manuscript we detail the inner workings of NeuralPDE.jl and show how a formulation structured around numerical quadrature gives rise to new loss functions which allow for adaptivity towards bounded error tolerances. We describe the various ways one can use the tool, detailing mathematical techniques like using extended loss functions for parameter estimation and operator discovery, to help potential users adopt these PINN-based techniques into their workflow. We showcase how NeuralPDE uses a purely symbolic formulation so that all of the underlying training code is generated from an abstract formulation, and show how to make use of GPUs and solve systems of PDEs. Afterwards we give a detailed performance analysis which showcases the trade-off between training techniques on a large set of PDEs. We end by focusing on a complex multiphysics example, the Doyle-Fuller-Newman (DFN) Model, and showcase how this PDE can be formulated and solved with NeuralPDE. Together this manuscript is meant to be a detailed and approachable technical report to help potential users of the technique quickly get a sense of the real-world performance trade-offs and use cases of the PINN techniques.},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Zubov, Kirill and McCarthy, Zoe and Ma, Yingbo and Calisto, Francesco and Pagliarino, Valerio and Azeglio, Simone and Bottero, Luca and Luján, Emmanuel and Sulzer, Valentin and Bharambe, Ashutosh and Vinchhi, Nand and Balakrishnan, Kaushik and Upadhyay, Devesh and Rackauckas, Chris},
	month = jul,
	year = {2021},
	note = {arXiv:2107.09443 [cs]},
	keywords = {Computer Science - Mathematical Software, Computer Science - Symbolic Computation},
}

@inproceedings{pal_opening_2021,
	title = {Opening the {Blackbox}: {Accelerating} {Neural} {Differential} {Equations} by {Regularizing} {Internal} {Solver} {Heuristics}},
	shorttitle = {Opening the {Blackbox}},
	url = {https://proceedings.mlr.press/v139/pal21a.html},
	abstract = {Democratization of machine learning requires architectures that automatically adapt to new problems. Neural Differential Equations (NDEs) have emerged as a popular modeling framework by removing the need for ML practitioners to choose the number of layers in a recurrent model. While we can control the computational cost by choosing the number of layers in standard architectures, in NDEs the number of neural network evaluations for a forward pass can depend on the number of steps of the adaptive ODE solver. But, can we force the NDE to learn the version with the least steps while not increasing the training cost? Current strategies to overcome slow prediction require high order automatic differentiation, leading to significantly higher training time. We describe a novel regularization method that uses the internal cost heuristics of adaptive differential equation solvers combined with discrete adjoint sensitivities to guide the training process towards learning NDEs that are easier to solve. This approach opens up the blackbox numerical analysis behind the differential equation solver’s algorithm and directly uses its local error estimates and stiffness heuristics as cheap and accurate cost estimates. We incorporate our method without any change in the underlying NDE framework and show that our method extends beyond Ordinary Differential Equations to accommodate Neural Stochastic Differential Equations. We demonstrate how our approach can halve the prediction time and, unlike other methods which can increase the training time by an order of magnitude, we demonstrate similar reduction in training times. Together this showcases how the knowledge embedded within state-of-the-art equation solvers can be used to enhance machine learning.},
	language = {en},
	urldate = {2025-02-18},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Pal, Avik and Ma, Yingbo and Shah, Viral and Rackauckas, Christopher V.},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {8325--8335},
}

@article{roesch_collocation_2021,
	title = {Collocation based training of neural ordinary differential equations},
	volume = {20},
	copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
	issn = {1544-6115},
	url = {https://www.degruyter.com/document/doi/10.1515/sagmb-2020-0025/html},
	doi = {10.1515/sagmb-2020-0025},
	abstract = {The predictive power of machine learning models often exceeds that of mechanistic modeling approaches. However, the interpretability of purely data-driven models, without any mechanistic basis is often complicated, and predictive power by itself can be a poor metric by which we might want to judge different methods. In this work, we focus on the relatively new modeling techniques of neural ordinary differential equations. We discuss how they relate to machine learning and mechanistic models, with the potential to narrow the gulf between these two frameworks: they constitute a class of hybrid model that integrates ideas from data-driven and dynamical systems approaches. Training neural ODEs as representations of dynamical systems data has its own specific demands, and we here propose a collocation scheme as a fast and efficient training strategy. This alleviates the need for costly ODE solvers. We illustrate the advantages that collocation approaches offer, as well as their robustness to qualitative features of a dynamical system, and the quantity and quality of observational data. We focus on systems that exemplify some of the hallmarks of complex dynamical systems encountered in systems biology, and we map out how these methods can be used in the analysis of mathematical models of cellular and physiological processes.},
	language = {en},
	number = {2},
	urldate = {2025-02-18},
	journal = {Statistical Applications in Genetics and Molecular Biology},
	author = {Roesch, Elisabeth and Rackauckas, Christopher and Stumpf, Michael P. H.},
	month = apr,
	year = {2021},
	note = {Publisher: De Gruyter},
	keywords = {collocation, dynamical systems, neural ODE, systems biology},
	pages = {37--49},
}

@article{dandekar_safe_2021,
	title = {Safe {Blues}: {The} case for virtual safe virus spread in the long-term fight against epidemics},
	volume = {2},
	issn = {2666-3899},
	shorttitle = {Safe {Blues}},
	url = {https://www.cell.com/patterns/abstract/S2666-3899(21)00034-9},
	doi = {10.1016/j.patter.2021.100220},
	abstract = {{\textless}h2{\textgreater}Summary{\textless}/h2{\textgreater}{\textless}p{\textgreater}Viral spread is a complicated function of biological properties, the environment, preventative measures such as sanitation and masks, and the rate at which individuals come within physical proximity. It is these last two elements that governments can control through social-distancing directives. However, infection measurements are almost always delayed, making real-time estimation nearly impossible. Safe Blues is one way of addressing the problem caused by this time lag via online measurements combined with machine learning methods that exploit the relationship between counts of multiple forms of the Safe Blues strands and the progress of the actual epidemic. The Safe Blues protocols and techniques have been developed together with an experimental minimal viable product, presented as an app on Android devices with a server backend. Following initial exploration via simulation experiments, we are now preparing for a university-wide experiment of Safe Blues.{\textless}/p{\textgreater}},
	language = {English},
	number = {3},
	urldate = {2025-02-18},
	journal = {Patterns},
	author = {Dandekar, Raj and Henderson, Shane G. and Jansen, Hermanus M. and McDonald, Joshua and Moka, Sarat and Nazarathy, Yoni and Rackauckas, Christopher and Taylor, Peter G. and Vuorinen, Aapeli},
	month = mar,
	year = {2021},
	pmid = {33748797},
	note = {Publisher: Elsevier},
}

@misc{ma_modelingtoolkit_2022,
	title = {{ModelingToolkit}: {A} {Composable} {Graph} {Transformation} {System} {For} {Equation}-{Based} {Modeling}},
	shorttitle = {{ModelingToolkit}},
	url = {http://arxiv.org/abs/2103.05244},
	doi = {10.48550/arXiv.2103.05244},
	abstract = {Getting good performance out of numerical equation solvers requires that the user has provided stable and efficient functions representing their model. However, users should not be trusted to write good code. In this manuscript we describe ModelingToolkit (MTK), a symbolic equation-based modeling system which allows for composable transformations to generate stable, efficient, and parallelized model implementations. MTK blurs the lines of traditional symbolic computing by acting directly on a user's numerical code. We show the ability to apply graph algorithms for automatically parallelizing and performing index reduction on code written for differential-algebraic equation (DAE) solvers, "fixing" the performance and stability of the model without requiring any changes to on the user's part. We demonstrate how composable model transformations can be combined with automated data-driven surrogate generation techniques, allowing machine learning methods to generate accelerated approximate models within an acausal modeling framework. These reduced models are shown to outperform the Dymola Modelica compiler on an HVAC model by 590x at 3{\textbackslash}\% error. Together, this demonstrates MTK as a system for bringing the latest research in graph transformations directly to modeling applications.},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Ma, Yingbo and Gowda, Shashi and Anantharaman, Ranjan and Laughman, Chris and Shah, Viral and Rackauckas, Chris},
	month = feb,
	year = {2022},
	note = {arXiv:2103.05244 [cs]},
	keywords = {Computer Science - Mathematical Software, Computer Science - Software Engineering, Computer Science - Symbolic Computation},
}

@article{rackauckas_hybrid_2021,
	title = {Hybrid {Mechanistic} + {Neural} {Model} of {Laboratory} {Helicopter}},
	url = {https://ep.liu.se/en/conference-article.aspx?series=ecp&issue=176&Article_No=37},
	language = {en},
	urldate = {2025-02-18},
	author = {Rackauckas, Christopher and Sharma, Roshan and Lie, Bernt},
	month = mar,
	year = {2021},
}

@misc{rackauckas_efficient_2021,
	title = {Efficient {Precision} {Dosing} {Under} {Estimated} {Uncertainties} via {Koopman} {Expectations} of {Bayesian} {Posteriors} with {Pumas}},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2021.01.25.428134v1},
	doi = {10.1101/2021.01.25.428134},
	abstract = {Personalized precision dosing is about mathematically determining effective dosing strategies that optimize the probability of containing a patient’s outcome within a therapeutic window. However, the common Monte Carlo approach for generating patient statistics is computationally expensive because thousands of simulations need to be computed. In this manuscript we describe a new method which utilizes the Koopman operator to perform a direct computation of expected patient outcomes with respect to quantified uncertainties of Bayesian posteriors in a nonlinear mixed effect model framework. We detail how quantities such as the probability of being within the therapeutic window can be calculated with a choice of loss function on the Koopman expectation. We demonstrate a high performance parallelized implementation of this methodology in Pumas® and showcase the ability to accelerate the computation of these expectations by 50x-200x over Monte Carlo. We showcase how dosing can be optimized with respect to probabilistic statements respecting variable uncertainties using the Koopman operator. We end by demonstrating an end-to-end workflow, from estimating variable uncertainties via Bayesian estimation to optimizing a dose with respect to parametric uncertainty.},
	language = {en},
	urldate = {2025-02-18},
	publisher = {bioRxiv},
	author = {Rackauckas, Chris and Dixit, Vaibhav and Gerlach, Adam R. and Ivaturi, Vijay},
	month = jan,
	year = {2021},
	note = {Pages: 2021.01.25.428134
Section: New Results},
}

@article{dandekar_implications_2021,
	title = {Implications of {Delayed} {Reopening} in {Controlling} the {COVID}-19 {Surge} in {Southern} and {West}-{Central} {USA}},
	volume = {2021},
	url = {https://spj.science.org/doi/10.34133/2021/9798302},
	doi = {10.34133/2021/9798302},
	abstract = {In the wake of the rapid surge in the COVID-19-infected cases seen in Southern and West-Central USA in the period of June-July 2020, there is an urgent need to develop robust, data-driven models to quantify the effect which early reopening had on the infected case count increase. In particular, it is imperative to address the question: How many infected cases could have been prevented, had the worst affected states not reopened early? To address this question, we have developed a novel COVID-19 model by augmenting the classical SIR epidemiological model with a neural network module. The model decomposes the contribution of quarantine strength to the infection time series, allowing us to quantify the role of quarantine control and the associated reopening policies in the US states which showed a major surge in infections. We show that the upsurge in the infected cases seen in these states is strongly corelated with a drop in the quarantine/lockdown strength diagnosed by our model. Further, our results demonstrate that in the event of a stricter lockdown without early reopening, the number of active infected cases recorded on 14 July could have been reduced by more than 
40
\%
 in all states considered, with the actual number of infections reduced being more than 
100,000
for the states of Florida and Texas. As we continue our fight against COVID-19, our proposed model can be used as a valuable asset to simulate the effect of several reopening strategies on the infected count evolution, for any region under consideration.},
	urldate = {2025-02-18},
	journal = {Health Data Science},
	author = {Dandekar, Raj and Wang, Emma and Barbastathis, George and Rackauckas, Chris},
	month = oct,
	year = {2021},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {9798302},
}

@inproceedings{martensen_neural_2021,
	title = {Neural {Network} {Surrogates} and {Symbolic} {Regression} for {System} {Estimation}},
	url = {https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_3325251},
	language = {eng},
	urldate = {2025-02-18},
	author = {Martensen, Carl Julius and Rackauckas, Christopher},
	year = {2021},
}

@article{drensky_densities_2021,
	title = {The densities and distributions of the largest eigenvalue and the trace of a {Beta}–{Wishart} matrix},
	volume = {10},
	issn = {2010-3263},
	url = {https://www.worldscientific.com/doi/10.1142/S2010326321500106},
	doi = {10.1142/S2010326321500106},
	abstract = {We present new expressions for the densities and distributions of the largest eigenvalue and the trace of a Beta–Wishart matrix. The series expansions for these expressions involve fewer terms than previously known results. For the trace, we also present a new algorithm that is linear in the size of the matrix and the degree of truncation, which is optimal.},
	number = {01},
	urldate = {2025-02-18},
	journal = {Random Matrices: Theory and Applications},
	author = {Drensky, Vesselin and Edelman, Alan and Genoar, Tierney and Kan, Raymond and Koev, Plamen},
	month = jan,
	year = {2021},
	note = {Publisher: World Scientific Publishing Co.},
	keywords = {eigenvalue, hypergeometric function of a matrix argument, trace, Wishart},
	pages = {2150010},
}

@article{rackauckas_generalized_2021,
	title = {Generalized physics-informed learning through language-wide differentiable programming},
	copyright = {Creative Commons Attribution 4.0 International license},
	url = {https://dspace.mit.edu/handle/1721.1/137320},
	abstract = {Copyright © 2020, for this paper by its authors. Scientific computing is increasingly incorporating the advancements in machine learning to allow for data-driven physics-informed modeling approaches. However, re-targeting existing scientific computing workloads to machine learning frameworks is both costly and limiting, as scientific simulations tend to use the full feature set of a general purpose programming language. In this manuscript we develop an infrastructure for incorporating deep learning into existing scientific computing code through Differentiable Programming (∂P). We describe a ∂P system that is able to take gradients of full Julia programs, making Automatic Differentiation a first class language feature and compatibility with deep learning pervasive. Our system utilizes the one-language nature of Julia package development to augment the existing package ecosystem with deep learning, supporting almost all language constructs (control flow, recursion, mutation, etc.) while generating high-performance code without requiring any user intervention or refactoring to stage computations. We showcase several examples of physics-informed learning which directly utilizes this extension to existing simulation code: neural surrogate models, machine learning on simulated quantum hardware, and data-driven stochastic dynamical model discovery with neural stochastic differential equations.},
	language = {en},
	urldate = {2025-02-18},
	journal = {MIT web domain},
	author = {Rackauckas, C. and Edelman, A. and Fischer, K. and Innes, M. and Saba, E. and Shah, V. B. and Tebbutt, W.},
	month = nov,
	year = {2021},
	note = {Accepted: 2021-11-04T11:58:19Z},
}

@article{drake_simple_2021,
	title = {A simple model for assessing climate control trade-offs and responding to unanticipated climate outcomes},
	volume = {16},
	issn = {1748-9326},
	url = {https://dx.doi.org/10.1088/1748-9326/ac243e},
	doi = {10.1088/1748-9326/ac243e},
	abstract = {Persistent greenhouse gas (GHG) emissions threaten global climate goals and have prompted consideration of climate controls supplementary to emissions mitigation. We present MARGO, an idealized model of optimally-controlled climate change, which is complementary to both simpler conceptual models and more complicated Integrated Assessment Models. The four methods of controlling climate damage—mitigation, carbon dioxide removal (CDR), adaptation, and solar radiation modification (SRM)—are not interchangeable, as they enter at different stages of the causal chain that connects GHG emissions to climate damages. Early and aggressive mitigation is necessary to stabilize GHG concentrations below a tolerable level. While the most cost-beneficial and cost-effective pathways to reducing climate suffering include deployments of all four controls, the quantitative trade-offs between the different controls are sensitive to value-driven parameters and poorly-known future costs and damages. Static policy optimization assumes perfect foresight and obscures the active role decision-makers have in shaping a climate trajectory. We propose an explicit policy response process wherein climate control policies are re-adjusted over time in response to unanticipated outcomes. We illustrate this process in two ‘storyline’ scenarios: (a) near-term increases in mitigation and CDR are deficient, such that climate goals are expected to slip out of reach; (b) SRM is abruptly terminated after 40 years of successful deployment, causing an extremely rapid warming which is amplified by an excess of GHGs due to deterred mitigation. In both cases, an optimized policy response yields substantial benefits relative to continuing the original policy. The MARGO model is intentionally designed to be as simple, transparent, customizable, and accessible as possible, addressing concerns about previous climate-economic modelling approaches and enabling a more diverse set of stakeholders to engage with these essential and timely topics.},
	language = {en},
	number = {10},
	urldate = {2025-02-18},
	journal = {Environmental Research Letters},
	author = {Drake, Henri F and Rivest, Ronald L and Edelman, Alan and Deutch, John},
	month = sep,
	year = {2021},
	note = {Publisher: IOP Publishing},
	pages = {104012},
}

@article{hall_circuitscape_2021,
	title = {Circuitscape in {Julia}: {Empowering} {Dynamic} {Approaches} to {Connectivity} {Assessment}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2073-445X},
	shorttitle = {Circuitscape in {Julia}},
	url = {https://www.mdpi.com/2073-445X/10/3/301},
	doi = {10.3390/land10030301},
	abstract = {The conservation field is experiencing a rapid increase in the amount, variety, and quality of spatial data that can help us understand species movement and landscape connectivity patterns. As interest grows in more dynamic representations of movement potential, modelers are often limited by the capacity of their analytic tools to handle these datasets. Technology developments in software and high-performance computing are rapidly emerging in many fields, but uptake within conservation may lag, as our tools or our choice of computing language can constrain our ability to keep pace. We recently updated Circuitscape, a widely used connectivity analysis tool developed by Brad McRae and Viral Shah, by implementing it in Julia, a high-performance computing language. In this initial re-code (Circuitscape 5.0) and later updates, we improved computational efficiency and parallelism, achieving major speed improvements, and enabling assessments across larger extents or with higher resolution data. Here, we reflect on the benefits to conservation of strengthening collaborations with computer scientists, and extract examples from a collection of 572 Circuitscape applications to illustrate how through a decade of repeated investment in the software, applications have been many, varied, and increasingly dynamic. Beyond empowering continued innovations in dynamic connectivity, we expect that faster run times will play an important role in facilitating co-production of connectivity assessments with stakeholders, increasing the likelihood that connectivity science will be incorporated in land use decisions.},
	language = {en},
	number = {3},
	urldate = {2025-02-18},
	journal = {Land},
	author = {Hall, Kimberly R. and Anantharaman, Ranjan and Landau, Vincent A. and Clark, Melissa and Dickson, Brett G. and Jones, Aaron and Platt, Jim and Edelman, Alan and Shah, Viral B.},
	month = mar,
	year = {2021},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Circuitscape, computational ecology, conservation planning, dynamic connectivity, Earth observations, Julia programming language, landscape connectivity, Omniscape},
	pages = {301},
}

@inproceedings{anantharaman_composable_2021,
	title = {Composable and {Reusable} {Neural} {Surrogates} to {Predict} {System} {Response} of {Causal} {Model} {Components}},
	url = {https://openreview.net/forum?id=CFB_X4D_gQb},
	abstract = {Surrogate models, or machine learning based emulators of simulators, have been shown to be a powerful tool for accelerating simulations. However, capturing the system response of general nonlinear systems is still an open area of investigation. In this paper we propose a new surrogate architecture which is capable of capturing the input/output response of causal models to automatically replace large aspects of block model diagrams with neural-accelerated forms. We denote this technique the Nonlinear Response Continuous-Time Echo State Network (NR-CTESN) and describe a training mechanism for it to accurately predict the simulation response to exogenous inputs. We then describe a science-guided or physics-informed surrogate architecture based on Cellular Neural Networks to enable the NR-CTESN to accurately reproduce discontinuous output signals. We demonstrate this architecture on an inverter circuit and a Sky130 Digital to Analog Converter (DAC), showcasing a 9x and 300x acceleration of the respective simulations. These results showcase that the NR-CTESN can learn emulate the behavior of components within composable modeling frameworks and thus be reused in new applications without requiring retraining. Together this showcases a machine learning technique that can be used to generate nonlinear model order reductions of model components in SPICE simulators, Functional Markup Interface (FMI) representations of causal model components, and beyond.},
	language = {en},
	urldate = {2025-02-18},
	author = {Anantharaman, Ranjan and Abdelrehim, Anas and Martinuzzi, Francesco and Yalburgi, Sharan and Saba, Elliot and Fischer, Keno and Hertz, Glen and Vos, Pepijn de and Laughman, Chris and Ma, Yingbo and Shah, Viral and Edelman, Alan and Rackauckas, Chris},
	month = dec,
	year = {2021},
}

@misc{anantharaman_accelerating_2021,
	title = {Accelerating {Simulation} of {Stiff} {Nonlinear} {Systems} using {Continuous}-{Time} {Echo} {State} {Networks}},
	url = {http://arxiv.org/abs/2010.04004},
	doi = {10.48550/arXiv.2010.04004},
	abstract = {Modern design, control, and optimization often requires simulation of highly nonlinear models, leading to prohibitive computational costs. These costs can be amortized by evaluating a cheap surrogate of the full model. Here we present a general data-driven method, the continuous-time echo state network (CTESN), for generating surrogates of nonlinear ordinary differential equations with dynamics at widely separated timescales. We empirically demonstrate near-constant time performance using our CTESNs on a physically motivated scalable model of a heating system whose full execution time increases exponentially, while maintaining relative error of within 0.2 \%. We also show that our model captures fast transients as well as slow dynamics effectively, while other techniques such as physics informed neural networks have difficulties trying to train and predict the highly nonlinear behavior of these models.},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Anantharaman, Ranjan and Ma, Yingbo and Gowda, Shashi and Laughman, Chris and Shah, Viral and Edelman, Alan and Rackauckas, Chris},
	month = mar,
	year = {2021},
	note = {arXiv:2010.04004 [cs]},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems},
}

@article{souza_uncertainty_2020,
	title = {Uncertainty {Quantification} of {Ocean} {Parameterizations}: {Application} to the {K}-{Profile}-{Parameterization} for {Penetrative} {Convection}},
	volume = {12},
	copyright = {©2020. The Authors.},
	issn = {1942-2466},
	shorttitle = {Uncertainty {Quantification} of {Ocean} {Parameterizations}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2020MS002108},
	doi = {10.1029/2020MS002108},
	abstract = {Parameterizations of unresolved turbulent processes often compromise the fidelity of large-scale ocean models. In this work, we argue for a Bayesian approach to the refinement and evaluation of turbulence parameterizations. Using an ensemble of large eddy simulations of turbulent penetrative convection in the surface boundary layer, we demonstrate the method by estimating the uncertainty of parameters in the convective limit of the popular “K-Profile Parameterization.” We uncover structural deficiencies and propose an alternative scaling that overcomes them.},
	language = {en},
	number = {12},
	urldate = {2025-02-18},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Souza, A. N. and Wagner, G. L. and Ramadhan, A. and Allen, B. and Churavy, V. and Schloss, J. and Campin, J. and Hill, C. and Edelman, A. and Marshall, J. and Flierl, G. and Ferrari, R.},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2020MS002108},
	pages = {e2020MS002108},
}

@article{ramadhan_oceananigansjl_2020,
	title = {Oceananigans.jl: {Fast} and friendly geophysical fluid dynamics on {GPUs}},
	volume = {5},
	issn = {2475-9066},
	shorttitle = {Oceananigans.jl},
	url = {https://joss.theoj.org/papers/10.21105/joss.02018},
	doi = {10.21105/joss.02018},
	abstract = {Ramadhan et al., (2020). Oceananigans.jl: Fast and friendly geophysical fluid dynamics on GPUs. Journal of Open Source Software, 5(53), 2018, https://doi.org/10.21105/joss.02018},
	language = {en},
	number = {53},
	urldate = {2025-02-18},
	journal = {Journal of Open Source Software},
	author = {Ramadhan, Ali and Wagner, Gregory LeClaire and Hill, Chris and Campin, Jean-Michel and Churavy, Valentin and Besard, Tim and Souza, Andre and Edelman, Alan and Ferrari, Raffaele and Marshall, John},
	month = sep,
	year = {2020},
	pages = {2018},
}

% Alan Edelman %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibtex exported by Scopus
% Date: 11/08/2021

@ARTICLE{Drake2021,
author={Drake, H.F. and Rivest, R.L. and Edelman, A. and Deutch, J.},
title={A simple model for assessing climate control trade-offs and responding to unanticipated climate outcomes},
journal={Environmental Research Letters},
year={2021},
volume={16},
number={10},
doi={10.1088/1748-9326/ac243e},
art_number={104012},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116487654&doi=10.1088%2f1748-9326%2fac243e&partnerID=40&md5=ed05c25a841ed588bb76ed2472291650},
document_type={Article},
source={Scopus},
}

@ARTICLE{Hall2021,
author={Hall, K.R. and Anantharaman, R. and Landau, V.A. and Clark, M. and Dickson, B.G. and Jones, A. and Platt, J. and Edelman, A. and Shah, V.B.},
title={Circuitscape in julia: Empowering dynamic approaches to connectivity assessment},
journal={Land},
year={2021},
volume={10},
number={3},
doi={10.3390/land10030301},
art_number={301},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103251558&doi=10.3390%2fland10030301&partnerID=40&md5=80b177bdb629f095f0d37b63c8fb68a5},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Anantharaman2021,
author={Anantharaman, R. and Ma, Y. and Gowda, S. and Laughman, C. and Shah, V.B. and Edelman, A. and Rackauckas, C.},
title={Accelerating simulation of stiff nonlinear systems using continuous-time echo state networks},
journal={CEUR Workshop Proceedings},
year={2021},
volume={2964},
art_number={194},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116691271&partnerID=40&md5=2293b2e34c7e040ec97c72254c09662e},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{EDELMAN20211826,
author={Edelman, A. and WANG, Y.},
title={The gsvd: where are the ellipses?, matrix trigonometry, and more},
journal={SIAM Journal on Matrix Analysis and Applications},
year={2021},
volume={41},
number={4},
pages={1826-1856},
doi={10.1137/18M1234412},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099234307&doi=10.1137%2f18M1234412&partnerID=40&md5=31183fd1805e33c85b75539001742622},
document_type={Article},
source={Scopus},
}

@ARTICLE{Drensky2021,
author={Drensky, V. and Edelman, A. and Genoar, T. and Kan, R. and Koev, P.},
title={The densities and distributions of the largest eigenvalue and the trace of a Beta-Wishart matrix},
journal={Random Matrices: Theory and Application},
year={2021},
volume={10},
number={1},
doi={10.1142/S2010326321500106},
art_number={2150010},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075924404&doi=10.1142%2fS2010326321500106&partnerID=40&md5=62dd4313bd24d0503ae9911f53402d17},
document_type={Article},
source={Scopus},
}

@ARTICLE{Souza2020,
author={Souza, A.N. and Wagner, G.L. and Ramadhan, A. and Allen, B. and Churavy, V. and Schloss, J. and Campin, J. and Hill, C. and Edelman, A. and Marshall, J. and Flierl, G. and Ferrari, R.},
title={Uncertainty Quantification of Ocean Parameterizations: Application to the K-Profile-Parameterization for Penetrative Convection},
journal={Journal of Advances in Modeling Earth Systems},
year={2020},
volume={12},
number={12},
doi={10.1029/2020MS002108},
art_number={e2020MS002108},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098170696&doi=10.1029%2f2020MS002108&partnerID=40&md5=c4f7e66490b18c26105aa735b0f288c2},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Rackauckas2020,
author={Rackauckas, C. and Edelman, A. and Fischer, K. and Innes, M. and Saba, E. and Shah, V.B. and Tebbutt, W.},
title={Generalized physics-informed learning through language-wide differentiable programming},
journal={CEUR Workshop Proceedings},
year={2020},
volume={2587},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083333258&partnerID=40&md5=8feb4bff3abb4761ea8856861cc205a0},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chan2019127,
author={Chan, C. and Drensky, V. and Edelman, A. and Kan, R. and Koev, P.},
title={On computing Schur functions and series thereof},
journal={Journal of Algebraic Combinatorics},
year={2019},
volume={50},
number={2},
pages={127-141},
doi={10.1007/s10801-018-0846-y},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055563133&doi=10.1007%2fs10801-018-0846-y&partnerID=40&md5=9fb9cb22f19793bc36fea2817d6c265f},
document_type={Article},
source={Scopus},
}

@ARTICLE{Besard201929,
author={Besard, T. and Churavy, V. and Edelman, A. and Sutter, B.D.},
title={Rapid software prototyping for heterogeneous and distributed platforms},
journal={Advances in Engineering Software},
year={2019},
volume={132},
pages={29-46},
doi={10.1016/j.advengsoft.2019.02.002},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063985940&doi=10.1016%2fj.advengsoft.2019.02.002&partnerID=40&md5=18d47b8c20e17151a2a8d276dd1641d8},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kepner2018,
author={Kepner, J. and Brightwell, R. and Edelman, A. and Gadepally, V. and Jananthan, H. and Jones, M. and Madden, S. and Michaleas, P. and Okhravi, H. and Pedretti, K. and Reuther, A. and Sterling, T. and Stonebraker, M.},
title={TabulaROSA: Tabular Operating System Architecture for Massively Parallel Heterogeneous Compute Engines},
journal={2018 IEEE High Performance Extreme Computing Conference, HPEC 2018},
year={2018},
doi={10.1109/HPEC.2018.8547577},
art_number={8547577},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060082164&doi=10.1109%2fHPEC.2018.8547577&partnerID=40&md5=f6528720e5cfb09042f6abbe697c48e5},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Edelman2018643,
author={Edelman, A. and Wang, Y.},
title={Random Hyperplanes, Generalized Singular Values 'What's My β?'},
journal={2018 IEEE Statistical Signal Processing Workshop, SSP 2018},
year={2018},
pages={643-647},
doi={10.1109/SSP.2018.8450833},
art_number={8450833},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053847476&doi=10.1109%2fSSP.2018.8450833&partnerID=40&md5=46a6217092237cb81d85faabac1a2a62},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Bezanson201765,
author={Bezanson, J. and Edelman, A. and Karpinski, S. and Shah, V.B.},
title={Julia: A fresh approach to numerical computing},
journal={SIAM Review},
year={2017},
volume={59},
number={1},
pages={65-98},
doi={10.1137/141000671},
note={cited By 1205},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014461527&doi=10.1137%2f141000671&partnerID=40&md5=8856df4b328ca90250218790a37ca380},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Chen2016,
author={Chen, A. and Edelman, A. and Kepner, J. and Gadepally, V. and Hutchison, D.},
title={Julia implementation of the Dynamic Distributed Dimensional Data Model},
journal={2016 IEEE High Performance Extreme Computing Conference, HPEC 2016},
year={2016},
doi={10.1109/HPEC.2016.7761626},
art_number={7761626},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007085720&doi=10.1109%2fHPEC.2016.7761626&partnerID=40&md5=97845aee4d6848347ae20f5b32045cd8},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Edelman20161659,
author={Edelman, A. and Guionnet, A. and Péché, S.},
title={Beyond universality in random matrix theory},
journal={Annals of Applied Probability},
year={2016},
volume={26},
number={3},
pages={1659-1697},
doi={10.1214/15-AAP1129},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979021777&doi=10.1214%2f15-AAP1129&partnerID=40&md5=7a751efd00257a4fe65ed45abaeceb54},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman2015,
author={Edelman, A. and La Croix, M.},
title={The singular values of the GUE (less is more)},
journal={Random Matrices: Theory and Application},
year={2015},
volume={4},
number={4},
doi={10.1142/S2010326315500215},
art_number={1550021},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073584161&doi=10.1142%2fS2010326315500215&partnerID=40&md5=0081755e652edf089e202ad444786333},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Edelman20151271,
author={Edelman, A.},
title={Julia Introduction},
journal={Proceedings - 2015 IEEE 29th International Parallel and Distributed Processing Symposium Workshops, IPDPSW 2015},
year={2015},
pages={1271},
doi={10.1109/IPDPSW.2015.181},
art_number={7284457},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962210985&doi=10.1109%2fIPDPSW.2015.181&partnerID=40&md5=5586eac04db26d6abc55e9b3ab6fdb12},
document_type={Editorial},
source={Scopus},
}

@ARTICLE{Movassagh2015342,
author={Movassagh, R. and Edelman, A.},
title={Condition numbers of indefinite rank 2 ghost Wishart matrices},
journal={Linear Algebra and Its Applications},
year={2015},
volume={483},
pages={342-351},
doi={10.1016/j.laa.2015.05.027},
art_number={13229},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934759089&doi=10.1016%2fj.laa.2015.05.027&partnerID=40&md5=f2451feb0fbc7718955f6b09cc06f0f4},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman2015681,
author={Edelman, A. and Strang, G.},
title={Random Triangle Theory with Geometry and Applications},
journal={Foundations of Computational Mathematics},
year={2015},
volume={15},
number={3},
pages={681-713},
doi={10.1007/s10208-015-9250-3},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929944133&doi=10.1007%2fs10208-015-9250-3&partnerID=40&md5=de6674bdac7b0158989a7c5e45afea92},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dubbs2015188,
author={Dubbs, A. and Edelman, A.},
title={Infinite random matrix theory, tridiagonal bordered Toeplitz matrices, and the moment problem},
journal={Linear Algebra and Its Applications},
year={2015},
volume={467},
pages={188-201},
doi={10.1016/j.laa.2014.11.006},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912137508&doi=10.1016%2fj.laa.2014.11.006&partnerID=40&md5=7a71debaa84864fb26f757d5c0dc1961},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Bezanson201456,
author={Bezanson, J. and Chen, J. and Karpinski, S. and Shah, V. and Edelman, A.},
title={Array operators using multiple dispatch a design methodology for array implementations in dynamic languages},
journal={Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
year={2014},
pages={56-61},
doi={10.1145/2627373.2627383},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908233928&doi=10.1145%2f2627373.2627383&partnerID=40&md5=a74da3b6ab1bba542bf6de004bdc2c67},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Edelman2014,
author={Edelman, A. and Persson, P.-O. and Sutton, B.D.},
title={Low-temperature random matrix theory at the soft edge},
journal={Journal of Mathematical Physics},
year={2014},
volume={55},
number={6},
doi={10.1063/1.4874109},
art_number={063302},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929207289&doi=10.1063%2f1.4874109&partnerID=40&md5=85b306765b1262a4a39d43d511433c8e},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman2014,
author={Edelman, A. and Koev, P.},
title={Eigenvalue distributions of beta-Wishart matrices},
journal={Random Matrices: Theory and Application},
year={2014},
volume={3},
number={2},
doi={10.1142/S2010326314500099},
art_number={1450009},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042900714&doi=10.1142%2fS2010326314500099&partnerID=40&md5=77e98d3c2dc3c1996412d690e20f7253},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Chen201447,
author={Chen, J. and Edelman, A.},
title={Parallel prefix polymorphism permits parallelization, presentation &amp; proof},
journal={Proceedings of HPTCDL 2014: 1st Workshop for High Performance Technical Computing in Dynamic Languages - Held in Conjunction with SC 2014: The International Conference for High Performance Computing, Networking, Storage and Analysis},
year={2014},
pages={47-56},
doi={10.1109/HPTCDL.2014.9},
art_number={7069904},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946692972&doi=10.1109%2fHPTCDL.2014.9&partnerID=40&md5=3eefa32f943a01ed76acda3427679033},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dubbs2014,
author={Dubbs, A. and Edelman, A.},
title={The beta-manova ensemble with general covariance},
journal={Random Matrices: Theory and Application},
year={2014},
volume={3},
number={1},
doi={10.1142/S2010326314500026},
art_number={1450002},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053856284&doi=10.1142%2fS2010326314500026&partnerID=40&md5=0d5068f106d10a9c4c2861d5d638cc2b},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dubbs2013,
author={Dubbs, A. and Edelman, A. and Koev, P. and Venkataramana, P.},
title={The beta-Wishart ensemble},
journal={Journal of Mathematical Physics},
year={2013},
volume={54},
number={8},
doi={10.1063/1.4818304},
art_number={083507},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883359975&doi=10.1063%2f1.4818304&partnerID=40&md5=f8f678e5a3cba7635437e83b0a06d4a6},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman201391,
author={Edelman, A. and Wang, Y.},
title={Random Matrix theory and its innovative applications},
journal={Fields Institute Communications},
year={2013},
volume={66},
pages={91-116},
doi={10.1007/978-1-4614-5389-5_5},
note={cited By 26},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874325290&doi=10.1007%2f978-1-4614-5389-5_5&partnerID=40&md5=d6a6e49e3a756421f96fbbb1bed158d5},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Shah2013,
author={Shah, V.B. and Edelman, A. and Karpinski, S. and Bezanson, J. and Kepner, J.},
title={Novel algebras for advanced analytics in Julia},
journal={2013 IEEE High Performance Extreme Computing Conference, HPEC 2013},
year={2013},
doi={10.1109/HPEC.2013.6670347},
art_number={6670347},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893466701&doi=10.1109%2fHPEC.2013.6670347&partnerID=40&md5=cc02067d209cd3d8e827e3547e4e555f},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chen2012,
author={Chen, J. and Hontz, E. and Moix, J. and Welborn, M. and Van Voorhis, T. and Suárez, A. and Movassagh, R. and Edelman, A.},
title={Error analysis of free probability approximations to the density of states of disordered systems},
journal={Physical Review Letters},
year={2012},
volume={109},
number={3},
doi={10.1103/PhysRevLett.109.036403},
art_number={036403},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863889288&doi=10.1103%2fPhysRevLett.109.036403&partnerID=40&md5=5e1f1b895699b6a9e7f4868f7fc600dd},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Richmond20112036,
author={Richmond, C.D. and Geddes, R.L. and Movassagh, R. and Edelman, A.},
title={Performance of sample covariance based capon bearing only tracker},
journal={Conference Record - Asilomar Conference on Signals, Systems and Computers},
year={2011},
pages={2036-2039},
doi={10.1109/ACSSC.2011.6190384},
art_number={6190384},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861306924&doi=10.1109%2fACSSC.2011.6190384&partnerID=40&md5=448ce678df608260ac2edf5b71ea42b1},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Edelman2011530,
author={Edelman, A. and Hassidim, A. and Nguyen, H.N. and Onak, K.},
title={An efficient partitioning oracle for bounded-treewidth graphs},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2011},
volume={6845 LNCS},
pages={530-541},
doi={10.1007/978-3-642-22935-0_45},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052343116&doi=10.1007%2f978-3-642-22935-0_45&partnerID=40&md5=7913782ba39f8d30456cd72a96d1e340},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Movassagh2011,
author={Movassagh, R. and Edelman, A.},
title={Density of states of quantum spin systems from isotropic entanglement},
journal={Physical Review Letters},
year={2011},
volume={107},
number={9},
doi={10.1103/PhysRevLett.107.097205},
art_number={097205},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052251177&doi=10.1103%2fPhysRevLett.107.097205&partnerID=40&md5=756e88599526e43eb777c740db5e4eb2},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ansel201185,
author={Ansel, J. and Wong, Y.L. and Chan, C. and Olszewski, M. and Edelman, A. and Amarasinghe, S.},
title={Language and compiler support for auto-tuning variable-accuracy algorithms},
journal={Proceedings - International Symposium on Code Generation and Optimization, CGO 2011},
year={2011},
pages={85-96},
doi={10.1109/CGO.2011.5764677},
art_number={5764677},
note={cited By 84},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957514447&doi=10.1109%2fCGO.2011.5764677&partnerID=40&md5=ed0ff2f75b48f8a3f24dfd2e6b64bac1},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Richmond20101842,
author={Richmond, C.D. and Geddes, R.L. and Movassagh, R. and Edelman, A.},
title={Sample covariance based estimation of Capon algorithm error probabilities},
journal={Conference Record - Asilomar Conference on Signals, Systems and Computers},
year={2010},
pages={1842-1845},
doi={10.1109/ACSSC.2010.5757895},
art_number={5757895},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958014048&doi=10.1109%2fACSSC.2010.5757895&partnerID=40&md5=530d9fff07a8f53a3348c8e55d6cec8d},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chan2009,
author={Chan, C. and Ansel, J. and Wong, Y.L. and Amarasinghe, S. and Edelman, A.},
title={Autotuning multigrid with PetaBricks},
journal={Proceedings of the Conference on High Performance Computing Networking, Storage and Analysis, SC '09},
year={2009},
doi={10.1145/1654059.1654065},
art_number={1654065},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049162437&doi=10.1145%2f1654059.1654065&partnerID=40&md5=a0d54e6f893c70b9ae97d9a2d1451ede},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ansel200938,
author={Ansel, J. and Chan, C. and Wong, Y.L. and Olszewski, M. and Zhao, Q. and Edelman, A. and Amarasinghe, S.},
title={PetaBricks: A language and compiler for algorithmic choice},
journal={Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
year={2009},
pages={38-49},
doi={10.1145/1542476.1542481},
note={cited By 142},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70450227331&doi=10.1145%2f1542476.1542481&partnerID=40&md5=ab74f461ebda09722d5fcf29ef501a8c},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Albrecht2009461,
author={Albrecht, J.T. and Chan, C.P. and Edelman, A.},
title={Sturm sequences and random eigenvalue distributions},
journal={Foundations of Computational Mathematics},
year={2009},
volume={9},
number={4},
pages={461-483},
doi={10.1007/s10208-008-9037-x},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349561091&doi=10.1007%2fs10208-008-9037-x&partnerID=40&md5=0cd8dc084e552c713c07278bf2d3bb1b},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ansel200938,
author={Ansel, J. and Chan, C. and Wong, Y.L. and Olszewski, M. and Zhao, Q. and Edelman, A. and Amarasinghe, S.},
title={PetaBricks: A language and compiler for algorithmic choice},
journal={ACM SIGPLAN Notices},
year={2009},
volume={44},
number={6},
pages={38-49},
note={cited By 100},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650786281&partnerID=40&md5=813717223103e7ab180d20251d8ba927},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Rao2008649,
author={Rao, N.R. and Edelman, A.},
title={The polynomial method for random matrices},
journal={Foundations of Computational Mathematics},
year={2008},
volume={8},
number={6},
pages={649-702},
doi={10.1007/s10208-007-9013-x},
note={cited By 35},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-57149146598&doi=10.1007%2fs10208-007-9013-x&partnerID=40&md5=b5b3c471fc689ddb44e54a8df9b4cc0f},
document_type={Article},
source={Scopus},
}

@ARTICLE{Rao20082850,
author={Rao, N.R. and Mingo, J.A. and Speicher, R. and Edelman, A.},
title={Statistical Eigen-Inference from large wishart matrices},
journal={Annals of Statistics},
year={2008},
volume={36},
number={6},
pages={2850-2885},
doi={10.1214/07-AOS583},
note={cited By 41},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-46749119703&doi=10.1214%2f07-AOS583&partnerID=40&md5=4a935647eefd47877bae102b9f898e16},
document_type={Article},
source={Scopus},
}

@ARTICLE{Nadakuditi20082625,
author={Nadakuditi, R.R. and Edelman, A.},
title={Sample eigenvalue based detection of high-dimensional signals in white noise using relatively few samples},
journal={IEEE Transactions on Signal Processing},
year={2008},
volume={56},
number={7 I},
pages={2625-2638},
doi={10.1109/TSP.2008.917356},
note={cited By 191},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-46749116893&doi=10.1109%2fTSP.2008.917356&partnerID=40&md5=cc0cbcd1135711654c6c80b6dfb72f9a},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman2008259,
author={Edelman, A. and Sutton, B.D.},
title={The beta-Jacobi matrix model, the CS decomposition, and generalized singular value problems},
journal={Foundations of Computational Mathematics},
year={2008},
volume={8},
number={2},
pages={259-285},
doi={10.1007/s10208-006-0215-9},
note={cited By 36},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-44649173035&doi=10.1007%2fs10208-006-0215-9&partnerID=40&md5=33036d8dc9d094ca9d60ac95a208d1c5},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Edelman2007IV1197,
author={Edelman, A.},
title={The Star-P high performance computing platform},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2007},
volume={4},
pages={IV1197-IV1200},
doi={10.1109/ICASSP.2007.367290},
art_number={4218321},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547534987&doi=10.1109%2fICASSP.2007.367290&partnerID=40&md5=80119fb5539bb2dfd0df3e1c1465216c},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Edelman20071121,
author={Edelman, A. and Sutton, B.D.},
title={From random matrices to stochastic operators},
journal={Journal of Statistical Physics},
year={2007},
volume={127},
number={6},
pages={1121-1165},
doi={10.1007/s10955-006-9226-4},
note={cited By 59},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249901791&doi=10.1007%2fs10955-006-9226-4&partnerID=40&md5=20be505b8af012a2847c5eb28c4d1b8d},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dumitriu2007587,
author={Dumitriu, I. and Edelman, A. and Shuman, G.},
title={MOPS: Multivariate orthogonal polynomials (symbolically)},
journal={Journal of Symbolic Computation},
year={2007},
volume={42},
number={6},
pages={587-620},
doi={10.1016/j.jsc.2007.01.005},
note={cited By 49},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247596045&doi=10.1016%2fj.jsc.2007.01.005&partnerID=40&md5=a6cd4bb73b6cd44e04d132acae212f1f},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Nadakuditi2007,
author={Nadakuditi, R.R. and Edelman, A.},
title={Sample size cognizant detection of signals in white noise},
journal={IEEE Workshop on Signal Processing Advances in Wireless Communications, SPAWC},
year={2007},
doi={10.1109/spawc.2007.4401273},
art_number={4401273},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-48049085895&doi=10.1109%2fspawc.2007.4401273&partnerID=40&md5=14519f6e1905851738258c204a8ef368},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Rao2006V1001,
author={Rao, N.R. and Edelman, A.},
title={Free probability, sample covariance matrices, and signal processing},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2006},
volume={5},
pages={V1001-V1004},
art_number={1661447},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947702628&partnerID=40&md5=904026c1634412655f164b27cd62efe0},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Mitsouras20061209,
author={Mitsouras, D. and Zientara, G.P. and Edelman, A. and Rybicki, F.J.},
title={Enhancing the acquisition efficiency of fast magnetic resonance imaging via broadband encoding of signal content},
journal={Magnetic Resonance Imaging},
year={2006},
volume={24},
number={9},
pages={1209-1227},
doi={10.1016/j.mri.2006.07.003},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750374665&doi=10.1016%2fj.mri.2006.07.003&partnerID=40&md5=118dedae4170e73e2d881d1a47537b5f},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dumitriu2006,
author={Dumitriu, I. and Edelman, A.},
title={Global spectrum fluctuations for the β-Hermite and β-Laguerre ensembles via matrix models},
journal={Journal of Mathematical Physics},
year={2006},
volume={47},
number={6},
doi={10.1063/1.2200144},
art_number={063302},
note={cited By 62},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745683206&doi=10.1063%2f1.2200144&partnerID=40&md5=80fe1deb1ca668f2bf72677e206b54b0},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman2006547,
author={Edelman, A. and Sutton, B.D.},
title={Tails of condition number distributions},
journal={SIAM Journal on Matrix Analysis and Applications},
year={2006},
volume={27},
number={2},
pages={547-560},
doi={10.1137/040614256},
note={cited By 35},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646242497&doi=10.1137%2f040614256&partnerID=40&md5=40691eb1ca3f654e99d0483569ceb124},
document_type={Article},
source={Scopus},
}

@ARTICLE{Koev2006833,
author={Koev, P. and Edelman, A.},
title={The efficient evaluation of the hypergeometric function of a matrix argument},
journal={Mathematics of Computation},
year={2006},
volume={75},
number={254},
pages={833-846},
doi={10.1090/S0025-5718-06-01824-2},
note={cited By 127},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646424751&doi=10.1090%2fS0025-5718-06-01824-2&partnerID=40&md5=95878453d573793157a9d49eb4125265},
document_type={Article},
source={Scopus},
}

@ARTICLE{Absil2006288,
author={Absil, P.-A. and Edelman, A. and Koev, P.},
title={On the largest principal angle between random subspaces},
journal={Linear Algebra and Its Applications},
year={2006},
volume={414},
number={1},
pages={288-294},
doi={10.1016/j.laa.2005.10.004},
note={cited By 41},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33244482593&doi=10.1016%2fj.laa.2005.10.004&partnerID=40&md5=d9fce7a9edebee1ed04f771be7e2a47e},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Richmond20051711,
author={Richmond, C.D. and Rao Nadakuditi, R. and Edelman, A.},
title={Asymptotic mean squared error performance of diagonally loaded Capon-MVDR processor},
journal={Conference Record - Asilomar Conference on Signals, Systems and Computers},
year={2005},
volume={2005},
pages={1711-1716},
art_number={1600062},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847622079&partnerID=40&md5=e252d6e9c3ebacf7eb99c3f13565d061},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{RaoNadakuditi20051717,
author={Rao Nadakuditi, R. and Edelman, A.},
title={On the probability distribution of the outputs of the diagonally loading Capon-MVDR processor},
journal={Conference Record - Asilomar Conference on Signals, Systems and Computers},
year={2005},
volume={2005},
pages={1717-1723},
art_number={1600063},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847610228&partnerID=40&md5=26056324406acbb6312e4adf45b7ef94},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Edelman2005,
author={Edelman, A.},
title={Advances in random matrix theory},
journal={IEEE Workshop on Signal Processing Advances in Wireless Communications, SPAWC},
year={2005},
volume={2005},
doi={10.1109/SPAWC.2005.1505854},
art_number={1505854},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745496818&doi=10.1109%2fSPAWC.2005.1505854&partnerID=40&md5=6b755b9c453c0625a246474984b3e5fa},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dumitriu20051083,
author={Dumitriu, I. and Edelman, A.},
title={Eigenvalues of Hermite and Laguerre ensembles: Large beta asymptotics},
journal={Annales de l'institut Henri Poincare (B) Probability and Statistics},
year={2005},
volume={41},
number={6},
pages={1083-1099},
doi={10.1016/j.anihpb.2004.11.002},
note={cited By 39},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-26844510449&doi=10.1016%2fj.anihpb.2004.11.002&partnerID=40&md5=a186c4e5faff6dcad508e19e790875f3},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman2005233,
author={Edelman, A. and Rao, N.R.},
title={Random matrix theory},
journal={Acta Numerica},
year={2005},
volume={14},
pages={233-297},
doi={10.1017/S0962492904000236},
note={cited By 240},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-19844372999&doi=10.1017%2fS0962492904000236&partnerID=40&md5=c1c9b77a24643399e45375b69aad118b},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Nadakuditi2005IV793,
author={Nadakuditi, R.R. and Edelman, A.},
title={The bias of the MVDR beamformer outputs under diagonal loading},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2005},
volume={IV},
pages={IV793-IV796},
doi={10.1109/ICASSP.2005.1416128},
art_number={1416128},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646760665&doi=10.1109%2fICASSP.2005.1416128&partnerID=40&md5=6e1e32d56d141d36f41f5cc0580198d9},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Choy2005331,
author={Choy, R. and Edelman, A.},
title={Parallel MATLAB: Doing it right},
journal={Proceedings of the IEEE},
year={2005},
volume={93},
number={2},
pages={331-341},
doi={10.1109/JPROC.2004.840490},
note={cited By 87},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-20744451713&doi=10.1109%2fJPROC.2004.840490&partnerID=40&md5=b679c70a13ec468289763aa6403532aa},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Mitsouras2004321,
author={Mitsouras, D. and Hoge, W.S. and Rybicki, F.J. and Kyriakos, W.E. and Edelman, A. and Zientara, G.P.},
title={Non-Fourier-encoded parallel MRI using multiple receiver coils},
journal={Magnetic Resonance in Medicine},
year={2004},
volume={52},
number={2},
pages={321-328},
doi={10.1002/mrm.20172},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-3543081104&doi=10.1002%2fmrm.20172&partnerID=40&md5=19758986934f789abf090a5be79ce2fd},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman2004189,
author={Edelman, A. and Strang, G.},
title={Pascal Matrices},
journal={American Mathematical Monthly},
year={2004},
volume={111},
number={3},
pages={189-197},
doi={10.2307/4145127},
note={cited By 52},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-1642603002&doi=10.2307%2f4145127&partnerID=40&md5=7f0c285b2e31269d28b3842ac076347b},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dumitriu20025830,
author={Dumitriu, I. and Edelman, A.},
title={Matrix models for beta ensembles},
journal={Journal of Mathematical Physics},
year={2002},
volume={43},
number={11},
pages={5830-5847},
doi={10.1063/1.1507823},
note={cited By 314},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036850051&doi=10.1063%2f1.1507823&partnerID=40&md5=13328cd2c8315ff59ae2daad1a0b9ff2},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman20001004,
author={Edelman, A. and Ma, Y.},
title={Staircase failures explained by orthogonal versal forms},
journal={SIAM Journal on Matrix Analysis and Applications},
year={2000},
volume={21},
number={3},
pages={1004-1025},
doi={10.1137/S089547989833574X},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034342260&doi=10.1137%2fS089547989833574X&partnerID=40&md5=fd05104c22d7a93fa4a38d36618ae579},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman1999667,
author={Edelman, A. and Elmroth, E. and Kågström, B.},
title={A geometric approach to perturbation theory of matrices and matrix pencils. Part II: A stratification-enhanced staircase algorithm},
journal={SIAM Journal on Matrix Analysis and Applications},
year={1999},
volume={20},
number={3},
pages={667-699},
doi={10.1137/S0895479896310184},
note={cited By 57},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033239806&doi=10.1137%2fS0895479896310184&partnerID=40&md5=387942855ad0c3b3d751fa40bcb60a71},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ma199845,
author={Ma, Y. and Edelman, A.},
title={Nongeneric eigenvalue perturbations of Jordan blocks},
journal={Linear Algebra and Its Applications},
year={1998},
volume={273},
number={1-3},
pages={45-63},
doi={10.1016/S0024-3795(97)00342-X},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0005441166&doi=10.1016%2fS0024-3795%2897%2900342-X&partnerID=40&md5=0e642046d33dd1e12de77ffd5c07061e},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lippert1998278,
author={Lippert, R.A. and Arias, T.A. and Edelman, A.},
title={Multiscale Computation with Interpolating Wavelets},
journal={Journal of Computational Physics},
year={1998},
volume={140},
number={2},
pages={278-310},
doi={10.1006/jcph.1998.5885},
note={cited By 35},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000104702&doi=10.1006%2fjcph.1998.5885&partnerID=40&md5=8c2a9d9cb79b2c181e28276cad16f43b},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman199853,
author={Edelman, A. and Friedman, D.},
title={A counterexample to a hadamard matrix pivot conjecture},
journal={Linear and Multilinear Algebra},
year={1998},
volume={44},
number={1},
pages={53-56},
doi={10.1080/03081089808818547},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-22044440111&doi=10.1080%2f03081089808818547&partnerID=40&md5=07d3b9e02b9843d347a1b2d88a34ca0a},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman19981094,
author={Edelman, A. and McCorquodale, P. and Toledo, S.},
title={The future fast fourier transform?},
journal={SIAM Journal of Scientific Computing},
year={1998},
volume={20},
number={3},
pages={1094-1114},
doi={10.1137/S1064827597316266},
note={cited By 37},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032226441&doi=10.1137%2fS1064827597316266&partnerID=40&md5=0ac8636100132f09f5b06d584d82266d},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman1998303,
author={Edelman, A. and Arias, T.A. and Smith, S.T.},
title={The geometry of algorithms with orthogonality constraints},
journal={SIAM Journal on Matrix Analysis and Applications},
year={1998},
volume={20},
number={2},
pages={303-353},
doi={10.1137/S0895479895290954},
note={cited By 1661},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032216898&doi=10.1137%2fS0895479895290954&partnerID=40&md5=b957a0fd8644285cfac9f271717376bd},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman1997653,
author={Edelman, A. and Elmroth, E. and Kågström, B.},
title={A geometric approach to perturbation theory of matrices and matrix pencils. Part I: Versal deformations},
journal={SIAM Journal on Matrix Analysis and Applications},
year={1997},
volume={18},
number={3},
pages={653-692},
doi={10.1137/S0895479895284634},
note={cited By 70},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031524936&doi=10.1137%2fS0895479895284634&partnerID=40&md5=9694d3f5192d19ef5c6a1dd77012d73b},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman199754,
author={Edelman, A.},
title={The mathematics of the pentium division bug},
journal={SIAM Review},
year={1997},
volume={39},
number={1},
pages={54-67},
doi={10.1137/S0036144595293959},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031097396&doi=10.1137%2fS0036144595293959&partnerID=40&md5=8c23f6e222766c1a6e17a2ea73a871f6},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman1997203,
author={Edelman, A.},
title={The probability that a random real Gaussian matrix has k real eigenvalues, related distributions, and the circular law},
journal={Journal of Multivariate Analysis},
year={1997},
volume={60},
number={2},
pages={203-232},
doi={10.1006/jmva.1996.1653},
note={cited By 140},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031068715&doi=10.1006%2fjmva.1996.1653&partnerID=40&md5=5756d28644d841b97c347498b0d61000},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman1996494,
author={Edelman, A. and Smith, S.T.},
title={On conjugate gradient-like methods for eigen-like problems},
journal={BIT Numerical Mathematics},
year={1996},
volume={36},
number={3},
pages={494-508},
doi={10.1007/BF01731929},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-1542603298&doi=10.1007%2fBF01731929&partnerID=40&md5=b7c3d700d2314bf2e3166559b4854266},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman1995181,
author={Edelman, A. and Mascarenhas, W.},
title={On the Complete Pivoting Conjecture for a Hadamard Matrix of Order 12},
journal={Linear and Multilinear Algebra},
year={1995},
volume={38},
number={3},
pages={181-187},
doi={10.1080/03081089508818353},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012031357&doi=10.1080%2f03081089508818353&partnerID=40&md5=2d32e6d4a295a181cb527993cf3056c8},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman1995243,
author={Edelman, A. and Mascarenhas, W.F.},
title={On Parlett's matrix norm inequality for the Cholesky decomposition},
journal={Numerical Linear Algebra with Applications},
year={1995},
volume={2},
number={3},
pages={243-250},
doi={10.1002/nla.1680020306},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985396164&doi=10.1002%2fnla.1680020306&partnerID=40&md5=0ba4c6aba7d0af2ac9c3d2416fd704b1},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman1995763,
author={Edelman, A. and Murakami, H.},
title={Polynomial roots from companion matrix eigenvalues},
journal={Mathematics of Computation},
year={1995},
volume={64},
number={210},
pages={763-776},
doi={10.1090/S0025-5718-1995-1262279-2},
note={cited By 161},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968482180&doi=10.1090%2fS0025-5718-1995-1262279-2&partnerID=40&md5=119d35410fff1a0cbbf997844ca32069},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman19951,
author={Edelman, A. and Kostlan, E.},
title={How many zeros of a random polynomial are real?},
journal={Bulletin of the American Mathematical Society},
year={1995},
volume={32},
number={1},
pages={1-37},
doi={10.1090/S0273-0979-1995-00571-9},
note={cited By 240},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967770230&doi=10.1090%2fS0273-0979-1995-00571-9&partnerID=40&md5=c0500dbbec2a25b9dfb6ef5d4ae8a778},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman1995352,
author={Edelman, A.},
title={On the determinant of a uniformly distributed complex matrix},
journal={Journal of Complexity},
year={1995},
volume={11},
number={3},
pages={352-357},
doi={10.1006/jcom.1995.1017},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-58149320501&doi=10.1006%2fjcom.1995.1017&partnerID=40&md5=88b9cef598345d23d9a21cbc29ab56bb},
document_type={Article},
source={Scopus},
}

@ARTICLE{Demmel199561,
author={Demmel, J.W. and Edelman, A.},
title={The dimension of matrices (matrix pencils) with given Jordan (Kronecker) canonical forms},
journal={Linear Algebra and Its Applications},
year={1995},
volume={230},
pages={61-87},
doi={10.1016/0024-3795(93)00362-4},
note={cited By 50},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346549005&doi=10.1016%2f0024-3795%2893%2900362-4&partnerID=40&md5=dff838cf451ae8f253e9e68f4c86121f},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Edelman1994781,
author={Edelman, Alan},
title={Large numerical linear algebra in 1994: The continuing influence of parallel computing},
journal={Proceedings of the Scalable High-Performance Computing Conference},
year={1994},
pages={781-787},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028564201&partnerID=40&md5=d35c872370eda2262fa40537da7e35cc},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Edelman1994247,
author={Edelman, A. and Kostlan, E. and Shub, M.},
title={How many eigenvalues of a random matrix are real?},
journal={Journal of the American Mathematical Society},
year={1994},
volume={7},
number={1},
pages={247-267},
doi={10.1090/S0894-0347-1994-1231689-0},
note={cited By 102},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968466626&doi=10.1090%2fS0894-0347-1994-1231689-0&partnerID=40&md5=1f53b8ed5963937188869d6debe79e37},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman19941302,
author={Edelman, A. and Heller, S. and Johnsson, S.L.},
title={Index Transformation Algorithms in a Linear Algebra Framework},
journal={IEEE Transactions on Parallel and Distributed Systems},
year={1994},
volume={5},
number={12},
pages={1302-1309},
doi={10.1109/71.334903},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028750115&doi=10.1109%2f71.334903&partnerID=40&md5=f4ec92ce01cd4557c749c2397f2d9818},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman1993113,
author={Edelman, A.},
title={Large dense numerical linear algebra in 1993: The parallel computing influence},
journal={International Journal of High Performance Computing Applications},
year={1993},
volume={7},
number={2},
pages={113-128},
doi={10.1177/109434209300700203},
note={cited By 35},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973744454&doi=10.1177%2f109434209300700203&partnerID=40&md5=704a8400e08431ea4c2654375bd7b6cc},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman19931676,
author={Edelman, A. and Stewart, G.W.},
title={Scaling for Orthogonality},
journal={IEEE Transactions on Signal Processing},
year={1993},
volume={41},
number={4},
pages={1676-1677},
doi={10.1109/78.212741},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027577793&doi=10.1109%2f78.212741&partnerID=40&md5=01942fa256563f90a765627622eab0ef},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman1992185,
author={Edelman, A.},
title={On the distribution of a scaled condition number},
journal={Mathematics of Computation},
year={1992},
volume={58},
number={197},
pages={185-190},
doi={10.1090/S0025-5718-1992-1106966-2},
note={cited By 55},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968515219&doi=10.1090%2fS0025-5718-1992-1106966-2&partnerID=40&md5=cf5765d7889a69a19ce04ede18f5d5e4},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ho1991447,
author={Ho, C.-T. and Johnsson, S.L. and Edelman, A.},
title={Matrix multiplication on hypercubes using full bandwidth and constant storage},
journal={6th Distributed Memory Computing Conference, DMCC 1991 - Proceedings},
year={1991},
pages={447-451},
doi={10.1109/DMCC.1991.633211},
art_number={633211},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973758198&doi=10.1109%2fDMCC.1991.633211&partnerID=40&md5=467770114737643058227a37d4026c25},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Edelman1991328,
author={Edelman, A.},
title={Optimal matrix transposition and bit reversal on hypercubes: All-to-all personalized communication},
journal={Journal of Parallel and Distributed Computing},
year={1991},
volume={11},
number={4},
pages={328-331},
doi={10.1016/0743-7315(91)90039-C},
note={cited By 25},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026140090&doi=10.1016%2f0743-7315%2891%2990039-C&partnerID=40&md5=99b95aeb6012c65d55fb2f3ba1d95716},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman199155,
author={Edelman, A.},
title={The distribution and moments of the smallest eigenvalue of a random matrix of wishart type},
journal={Linear Algebra and Its Applications},
year={1991},
volume={159},
number={C},
pages={55-80},
doi={10.1016/0024-3795(91)90076-9},
note={cited By 76},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001990372&doi=10.1016%2f0024-3795%2891%2990076-9&partnerID=40&md5=c063d405cb7902e30a80bca003efd556},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Brunet1990748,
author={Brunet, Jean-Philippe and Mesirov, Jill P. and Edelman, Alan},
title={An optimal hypercube direct N-body solver on the Connection Machine},
year={1990},
pages={748-752},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025539984&partnerID=40&md5=297f907dcaa938c89fdccf9261fca8d6},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dougherty1989471,
author={Dougherty, R.L. and Edelman, A. and Hyman, J.M.},
title={Nonnegativity-, monotonicity-, or convexity-preserving cubic and quintic hermite interpolation},
journal={Mathematics of Computation},
year={1989},
volume={52},
number={186},
pages={471-494},
doi={10.1090/S0025-5718-1989-0962209-1},
note={cited By 100},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966199150&doi=10.1090%2fS0025-5718-1989-0962209-1&partnerID=40&md5=95a229f79254122aef750a3265c0c406},
document_type={Article},
source={Scopus},
}

@ARTICLE{Edelman1987441,
author={Edelman, A. and Micchelli, C.A.},
title={Admissible slopes for monotone and convex interpolation},
journal={Numerische Mathematik},
year={1987},
volume={51},
number={4},
pages={441-458},
doi={10.1007/BF01397546},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000887774&doi=10.1007%2fBF01397546&partnerID=40&md5=6debe499195a28d2469f9639598783f4},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dyn1987313,
author={Dyn, N. and Edelman, A. and Micchelli, C.A.},
title={On locally supported basis functions for the representation of geometrically continuous curves},
journal={Analysis},
year={1987},
volume={7},
number={3-4},
pages={313-342},
doi={10.1524/anly.1987.7.34.313},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0009445982&doi=10.1524%2fanly.1987.7.34.313&partnerID=40&md5=9133646a8e1b0e0669d55d8bcf3ecbab},
document_type={Article},
source={Scopus},
}


% Christopher Rackauckas %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibtex exported by Scopus
% Date: 11/08/2021

@ARTICLE{Kim2021,
author={Kim, S. and Ji, W. and Deng, S. and Ma, Y. and Rackauckas, C.},
title={Stiff neural ordinary differential equations},
journal={Chaos},
year={2021},
volume={31},
number={9},
doi={10.1063/5.0060697},
art_number={093122},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115311950&doi=10.1063%2f5.0060697&partnerID=40&md5=8c62ee9b7a3d191c36089ca0aaa5a89b},
document_type={Article},
source={Scopus},
}

@ARTICLE{Roesch202137,
author={Roesch, E. and Rackauckas, C. and Stumpf, M.P.H.},
title={Collocation based training of neural ordinary differential equations},
journal={Statistical Applications in Genetics and Molecular Biology},
year={2021},
volume={20},
number={2},
pages={37-49},
doi={10.1515/sagmb-2020-0025},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110085461&doi=10.1515%2fsagmb-2020-0025&partnerID=40&md5=a5fb414383a6b79bbfa28fb9c05ddc7b},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dandekar2021,
author={Dandekar, R. and Henderson, S.G. and Jansen, H.M. and McDonald, J. and Moka, S. and Nazarathy, Y. and Rackauckas, C. and Taylor, P.G. and Vuorinen, A.},
title={Safe Blues: The case for virtual safe virus spread in the long-term fight against epidemics},
journal={Patterns},
year={2021},
volume={2},
number={3},
doi={10.1016/j.patter.2021.100220},
art_number={100220},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102307395&doi=10.1016%2fj.patter.2021.100220&partnerID=40&md5=23dffe5ad43645316af8c3bc825b2aa3},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Anantharaman2021,
author={Anantharaman, R. and Ma, Y. and Gowda, S. and Laughman, C. and Shah, V.B. and Edelman, A. and Rackauckas, C.},
title={Accelerating simulation of stiff nonlinear systems using continuous-time echo state networks},
journal={CEUR Workshop Proceedings},
year={2021},
volume={2964},
art_number={194},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116691271&partnerID=40&md5=2293b2e34c7e040ec97c72254c09662e},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dandekar2020,
author={Dandekar, R. and Rackauckas, C. and Barbastathis, G.},
title={A Machine Learning-Aided Global Diagnostic and Comparative Tool to Assess Effect of Quarantine Control in COVID-19 Spread},
journal={Patterns},
year={2020},
volume={1},
number={9},
doi={10.1016/j.patter.2020.100145},
art_number={100145},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097386310&doi=10.1016%2fj.patter.2020.100145&partnerID=40&md5=ce9f5f079d0074fdff0b2ded7d6c823e},
document_type={Article},
source={Scopus},
}

@ARTICLE{Irurzun-Arana2020882,
author={Irurzun-Arana, I. and Rackauckas, C. and McDonald, T.O. and Trocóniz, I.F.},
title={Beyond Deterministic Models in Drug Discovery and Development},
journal={Trends in Pharmacological Sciences},
year={2020},
volume={41},
number={11},
pages={882-895},
doi={10.1016/j.tips.2020.09.005},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092178465&doi=10.1016%2fj.tips.2020.09.005&partnerID=40&md5=21a2fda8ab5c100931d4ab024a545100},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Rackauckas2020,
author={Rackauckas, C. and Nie, Q.},
title={Stability-Optimized High Order Methods and Stiffness Detection for Pathwise Stiff Stochastic Differential Equations},
journal={2020 IEEE High Performance Extreme Computing Conference, HPEC 2020},
year={2020},
doi={10.1109/HPEC43674.2020.9286178},
art_number={9286178},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099333485&doi=10.1109%2fHPEC43674.2020.9286178&partnerID=40&md5=ab47007900f3242f96bd73f6d2aa0638},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Rackauckas2020,
author={Rackauckas, C. and Edelman, A. and Fischer, K. and Innes, M. and Saba, E. and Shah, V.B. and Tebbutt, W.},
title={Generalized physics-informed learning through language-wide differentiable programming},
journal={CEUR Workshop Proceedings},
year={2020},
volume={2587},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083333258&partnerID=40&md5=8feb4bff3abb4761ea8856861cc205a0},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Rackauckas20191,
author={Rackauckas, C. and Nie, Q.},
title={Confederated modular differential equation APIs for accelerated algorithm development and benchmarking},
journal={Advances in Engineering Software},
year={2019},
volume={132},
pages={1-6},
doi={10.1016/j.advengsoft.2019.03.009},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063544155&doi=10.1016%2fj.advengsoft.2019.03.009&partnerID=40&md5=2305e6c3c41aa4288b5bd85f0187a165},
document_type={Article},
source={Scopus},
}

@ARTICLE{Rackauckas2018267,
author={Rackauckas, C. and Schilling, T. and Nie, Q.},
title={Interdisciplinary Case Study: How Mathematicians and Biologists Found Order in Cellular Noise},
journal={iScience},
year={2018},
volume={8},
pages={267-270},
doi={10.1016/j.isci.2018.10.002},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066168700&doi=10.1016%2fj.isci.2018.10.002&partnerID=40&md5=b5a9e42e817f60f85c3370b5e5eaff1c},
document_type={Note},
source={Scopus},
}

@ARTICLE{Rackauckas201811,
author={Rackauckas, C. and Schilling, T. and Nie, Q.},
title={Mean-Independent Noise Control of Cell Fates via Intermediate States},
journal={iScience},
year={2018},
volume={3},
pages={11-20},
doi={10.1016/j.isci.2018.04.002},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065174542&doi=10.1016%2fj.isci.2018.04.002&partnerID=40&md5=076feba72b03ce657233af848aad3c49},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sosnik2016,
author={Sosnik, J. and Zheng, L. and Rackauckas, C.V. and Digman, M. and Gratton, E. and Nie, Q. and Schilling, T.F.},
title={Noise modulation in retinoic acid signaling sharpens segmental boundaries of gene expression in the embryonic zebrafish hindbrain},
journal={eLife},
year={2016},
volume={5},
number={APRIL2016},
doi={10.7554/eLife.14034},
art_number={e14034},
note={cited By 27},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964565726&doi=10.7554%2feLife.14034&partnerID=40&md5=f6763fe7dd3c2469a7729cfed67af337},
document_type={Article},
source={Scopus},
}

@ARTICLE{Walsh20152187,
author={Walsh, J. and Rackauckas, C.},
title={On the budyko-sellers energy balance climate model with ice line coupling},
journal={Discrete and Continuous Dynamical Systems - Series B},
year={2015},
volume={20},
number={7},
pages={2187-2216},
doi={10.3934/dcdsb.2015.20.2187},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936791840&doi=10.3934%2fdcdsb.2015.20.2187&partnerID=40&md5=012f7972f8d663075ff381fd2d2a8ddb},
document_type={Article},
source={Scopus},
}


% Viral B. Shah %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@ARTICLE{Hall2021,
author={Hall, K.R. and Anantharaman, R. and Landau, V.A. and Clark, M. and Dickson, B.G. and Jones, A. and Platt, J. and Edelman, A. and Shah, V.B.},
title={Circuitscape in julia: Empowering dynamic approaches to connectivity assessment},
journal={Land},
year={2021},
volume={10},
number={3},
doi={10.3390/land10030301},
art_number={301},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103251558&doi=10.3390%2fland10030301&partnerID=40&md5=80b177bdb629f095f0d37b63c8fb68a5},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Anantharaman2021,
author={Anantharaman, R. and Ma, Y. and Gowda, S. and Laughman, C. and Shah, V.B. and Edelman, A. and Rackauckas, C.},
title={Accelerating simulation of stiff nonlinear systems using continuous-time echo state networks},
journal={CEUR Workshop Proceedings},
year={2021},
volume={2964},
art_number={194},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116691271&partnerID=40&md5=2293b2e34c7e040ec97c72254c09662e},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Rackauckas2020,
author={Rackauckas, C. and Edelman, A. and Fischer, K. and Innes, M. and Saba, E. and Shah, V.B. and Tebbutt, W.},
title={Generalized physics-informed learning through language-wide differentiable programming},
journal={CEUR Workshop Proceedings},
year={2020},
volume={2587},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083333258&partnerID=40&md5=8feb4bff3abb4761ea8856861cc205a0},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dickson2019239,
author={Dickson, B.G. and Albano, C.M. and Anantharaman, R. and Beier, P. and Fargione, J. and Graves, T.A. and Gray, M.E. and Hall, K.R. and Lawler, J.J. and Leonard, P.B. and Littlefield, C.E. and McClure, M.L. and Novembre, J. and Schloss, C.A. and Schumaker, N.H. and Shah, V.B. and Theobald, D.M.},
title={Circuit-theory applications to connectivity science and conservation},
journal={Conservation Biology},
year={2019},
volume={33},
number={2},
pages={239-249},
doi={10.1111/cobi.13230},
note={cited By 74},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057297224&doi=10.1111%2fcobi.13230&partnerID=40&md5=c0742e0f2b2019940b2779415e43d842},
document_type={Review},
source={Scopus},
}

@ARTICLE{Leonard2017519,
author={Leonard, P.B. and Duffy, E.B. and Baldwin, R.F. and McRae, B.H. and Shah, V.B. and Mohapatra, T.K.},
title={gflow: software for modelling circuit theory-based connectivity at any scale},
journal={Methods in Ecology and Evolution},
year={2017},
volume={8},
number={4},
pages={519-526},
doi={10.1111/2041-210X.12689},
note={cited By 36},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002827377&doi=10.1111%2f2041-210X.12689&partnerID=40&md5=44cbbc4ef2a9b68bccde4eaa3b666fc9},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bezanson201765,
author={Bezanson, J. and Edelman, A. and Karpinski, S. and Shah, V.B.},
title={Julia: A fresh approach to numerical computing},
journal={SIAM Review},
year={2017},
volume={59},
number={1},
pages={65-98},
doi={10.1137/141000671},
note={cited By 1209},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014461527&doi=10.1137%2f141000671&partnerID=40&md5=8856df4b328ca90250218790a37ca380},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Bezanson201456,
author={Bezanson, J. and Chen, J. and Karpinski, S. and Shah, V. and Edelman, A.},
title={Array operators using multiple dispatch a design methodology for array implementations in dynamic languages},
journal={Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
year={2014},
pages={56-61},
doi={10.1145/2627373.2627383},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908233928&doi=10.1145%2f2627373.2627383&partnerID=40&md5=a74da3b6ab1bba542bf6de004bdc2c67},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Shah2013,
author={Shah, V.B. and Edelman, A. and Karpinski, S. and Bezanson, J. and Kepner, J.},
title={Novel algebras for advanced analytics in Julia},
journal={2013 IEEE High Performance Extreme Computing Conference, HPEC 2013},
year={2013},
doi={10.1109/HPEC.2013.6670347},
art_number={6670347},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893466701&doi=10.1109%2fHPEC.2013.6670347&partnerID=40&md5=cc02067d209cd3d8e827e3547e4e555f},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Gilbert2008225,
author={Gilbert, J.R. and Reinhardt, S. and Shah, V.B.},
title={Distributed Sparse Matrices for Very High Level Languages},
journal={Advances in Computers},
year={2008},
volume={72},
pages={225-252},
doi={10.1016/S0065-2458(08)00005-3},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649253123&doi=10.1016%2fS0065-2458%2808%2900005-3&partnerID=40&md5=12de198ef9dd0a96a5dd980548728a19},
document_type={Article},
source={Scopus},
}

@ARTICLE{McRae20082712,
author={McRae, B.H. and Dickson, B.G. and Keitt, T.H. and Shah, V.B.},
title={Using circuit theory to model connectivity in ecology, evolution, and conservation},
journal={Ecology},
year={2008},
volume={89},
number={10},
pages={2712-2724},
doi={10.1890/07-1861.1},
note={cited By 877},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-58149355439&doi=10.1890%2f07-1861.1&partnerID=40&md5=25d6e25dd25067c8ef39189421c4b5b1},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gilbert200820,
author={Gilbert, J.R. and Shah, V.B. and Reinhardt, S.},
title={A unified framework for numerical and combinatorial computing},
journal={Computing in Science and Engineering},
year={2008},
volume={10},
number={2},
pages={20-25},
doi={10.1109/MCSE.2008.45},
art_number={4454427},
note={cited By 32},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-39749144465&doi=10.1109%2fMCSE.2008.45&partnerID=40&md5=a8b55c3aeaad4a4cb9c935c96d33e916},
document_type={Article},
source={Scopus},
}

@ARTICLE{Reinhardt200730,
author={Reinhardt, S. and Shah, V.},
title={Tackling the challenges of exploratory data analysis on large-scale data},
journal={Scientific Computing},
year={2007},
volume={24},
number={10},
pages={30-33},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953683996&partnerID=40&md5=7ac7dc133326c09fbea9a4dcd2699817},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Gilbert2007IV1201,
author={Gilbert, J.R. and Shah, V. and Reinhardt, S.},
title={An interactive environment to manipulate large graphs},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2007},
volume={4},
pages={IV1201-IV1204},
doi={10.1109/ICASSP.2007.367291},
art_number={4218322},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547513783&doi=10.1109%2fICASSP.2007.367291&partnerID=40&md5=cdbec3dcba25fa71541bc930d41685eb},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Gilbert2007260,
author={Gilbert, J.R. and Reinhardt, S. and Shah, V.B.},
title={High-performance graph algorithms from parallel sparse matrices},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2007},
volume={4699 LNCS},
pages={260-269},
doi={10.1007/978-3-540-75755-9_32},
note={cited By 44},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-38049068400&doi=10.1007%2f978-3-540-75755-9_32&partnerID=40&md5=a1fa3a712882dd3f2c74d9b75d0515de},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Smith200565,
author={Smith, B. and Mizell, D. and Gilbert, J. and Shah, V.},
title={Towards a timed Markov process model of software development},
journal={Proceedings - International Conference on Software Engineering},
year={2005},
pages={65-67},
doi={10.1145/1145319.1145338},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954443967&doi=10.1145%2f1145319.1145338&partnerID=40&md5=03a24e5821600aa464eff5159f9f23e2},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Shah200585,
author={Shah, V.},
title={Costs and benefits of using smaller assessment models for software process assessment and improvement in small software organizations},
journal={5th International SPICE Conference on Software Process Assessment and Improvement, SPICE 2005},
year={2005},
pages={85-91},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946743624&partnerID=40&md5=43d7f686fe00a7da9e94cf3b84a49558},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Shah2005144,
author={Shah, V. and Gilbert, J.R.},
title={Sparse matrices in MATLAB*P: Design and implementation},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2005},
volume={3296 LNCS},
pages={144-155},
doi={10.1137/0613024},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-24344447939&doi=10.1137%2f0613024&partnerID=40&md5=b476ee37264d967fd5a4470d729af031},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Shah2004144,
author={Shah, V. and Gilbert, J.R.},
title={Sparse matrices in MATLAB*P:Design and implementation},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2004},
volume={3296},
pages={144-155},
doi={10.1007/978-3-540-30474-6_20},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088761620&doi=10.1007%2f978-3-540-30474-6_20&partnerID=40&md5=ac2b791814898451319c32b8e4f3673a},
document_type={Article},
source={Scopus},
}

% New content from Chris %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Google Scholar, arXiv, etc.

@CONFERENCE{Kurchin2020Aced,
title={ACED: Accelerated Computational Electrochemical systems Discovery},
author={Kurchin, R.C. and Muckley, E. and Kavalsky, L. and Hegde, V. and Gandhi, D. and Sun, X. and Johnson, M. and Edelman, A. and Saal, J. and Rackauckas, C.V. and Meredig, B. and Shah, V. and Viswanathan, V.},
journal={NeurIPS 2020 Workshop on Tackling Climate Change with Machine Learning},
year={2020},
url={https://www.climatechange.ai/papers/neurips2020/91},
}

@CONFERENCE{Rackauckas2020Stability,
title={Stability-Optimized High Order Methods and Stiffness Detection for Pathwise Stiff Stochastic Differential Equations}, 
author={Rackauckas, C. and Nie, Q.},
journal={2020 IEEE High Performance Extreme Computing Conference (HPEC)}, 
year={2020},
pages={1-8},
doi={10.1109/HPEC43674.2020.9286178},
url={https://ieeexplore.ieee.org/abstract/document/9286178},
}

@CONFERENCE{Dandekar2021Bayesian,
title={Bayesian Neural Ordinary Differential Equations}, 
author={Dandekar, R. and Chung, K. and Dixit, V. and Tarek, M. and Garcia-Valadez, A. and Vishal Vemula, K. and Rackauckas, C.},
journal={LAFI},
year={2021},
eprint={2012.07244},
archivePrefix={arXiv},
primaryClass={cs.LG},
url={https://popl21.sigplan.org/details/lafi-2021-papers/7/Bayesian-Neural-Ordinary-Differential-Equations},
}

@CONFERENCE{Pal2021Opening,
title={Opening the Blackbox: Accelerating Neural Differential Equations by Regularizing Internal Solver Heuristics},
author={Pal, A. and Ma, Y. and Shah, V. and Rackauckas, C.V.},
journal={Proceedings of the 38th International Conference on Machine Learning},
pages={8325--8335},
year={2021},
editor={Meila, Marina and Zhang, Tong},
volume={139},
series={Proceedings of Machine Learning Research},
month={18--24 Jul},
publisher={PMLR},
pdf={http://proceedings.mlr.press/v139/pal21a/pal21a.pdf},
url={https://proceedings.mlr.press/v139/pal21a.html},
}

@CONFERENCE{Rackauckas2021Hybrid,
title={Hybrid Mechanistic + Neural Model of Laboratory Helicopter},
author={Rackauckas, C. and Sharma, R. and Lie, B.},
journal={Proceedings of The 61st SIMS Conference on Simulation and Modelling SIMS 2020},
year={2021},
doi={10.3384/ecp20176264},
url={https://ep.liu.se/en/conference-article.aspx?series=ecp&issue=176&Article_No=37},
month=mar,
publisher={Link\"{o}ping University Electronic Press},
}

@CONFERENCE{Gowda2021Highperformance,
title={High-performance symbolic-numerics via multiple dispatch}, 
author={Gowda, S. and Ma, Y. and Cheli, A. and Gwozdz, M. and Shah, V.B. and Edelman, A. and Rackauckas, C.},
journal={arXiv},
year={2021},
eprint={2105.03949},
archivePrefix={arXiv},
primaryClass={cs.CL},
url={https://arxiv.org/abs/2105.03949},
}

@CONFERENCE{Sykora2020StochasticDelayDiffEq,
title={StochasticDelayDiffEq.jl - An Integrator Interface for Stochastic Delay Differential Equations in Julia},
author={Sykora, H.T. and Rackauckas, C.V. and Widmann, D. and Bachrathy, D.},
journal={ENOC 2020},
year={2020},
pages={1--2},
url={http://real.mtak.hu/106039/},
}

@CONFERENCE{Shankar2020Learning,
title={Learning non-linear spatio-temporal dynamics with convolutional Neural ODEs},
author={Shankar, V. and Portwood, G. and Mohan, A. and Mitra, P. and Rackauckas, C. and Wilson, L. and Schmidt, D. and Viswanathan, V.},
journal={Third Workshop on Machine Learning and the Physical Sciences (NeurIPS)},
year={2020},
month={12},
pages={},
url={https://ml4physicalsciences.github.io/2020/files/NeurIPS_ML4PS_2020_125.pdf},
}

@CONFERENCE{Rackauckas2021Composing,
title={Composing Modeling and Simulation with Machine Learning in Julia}, 
author={Rackauckas, C. and Anantharaman, R and Edelman, A. and Gowda, S. and Gwozdz, M. and Jain, A. and Laughman, C. and Ma, Y. and Martinuzzi, F. and Pal, A. and Rajput, U. and Saba, E. and Shah, V.B.},
journal={arXiv},
year={2021},
eprint={2105.05946},
archivePrefix={arXiv},
primaryClass={cs.CE},
url={https://arxiv.org/abs/2105.05946},
}

@CONFERENCE{Schafer2021AbstractDifferentiation,
author={Schäfer, F. and Tarek, M and White, L. and Rackauckas, C.},
title={AbstractDifferentiation.jl: Backend-Agnostic Differentiable Programming in Julia},
journal={arXiv e-prints},
keywords={Computer Science - Mathematical Software, Computer Science - Machine Learning, Computer Science - Software Engineering},
year=2021,
month=sep,
eid={arXiv:2109.12449},
pages={arXiv:2109.12449},
archivePrefix={arXiv},
eprint={2109.12449},
primaryClass={cs.MS},
url={https://ui.adsabs.harvard.edu/abs/2021arXiv210912449S},
adsnote={Provided by the SAO/NASA Astrophysics Data System},
}

@ARTICLE{Dandekar2021Implications,
title={Implications of Delayed Reopening in Controlling the COVID-19 Surge in Southern and West-Central USA},
author={Dandekar, R. and Wang, E. and Barbastathis, G. and Rackauckas, C.},
journal={Health Data Science},
year=2021,
doi={https://doi.org/10.34133/2021/9798302},
url={https://spj.sciencemag.org/journals/hds/2021/9798302/},
}

@CONFERENCE{Gowda2019Sparsity,
title={Sparsity Programming: Automated Sparsity-Aware Optimizations in Differentiable Programming},
author={Gowda, S. and Ma, Y. and Churavy, V. and Edelman, A. and Rackauckas, C.},
journal={NeurIPS 2019 Program Transformations for Machine Learning Workshop},
url={https://openreview.net/pdf?id=rJlPdcY38B},
year={2019},
}


% Valentin Churavy %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibtex exported by Scopus
% Date: 11/08/2021

@ARTICLE{Souza2020,
author={Souza, A.N. and Wagner, G.L. and Ramadhan, A. and Allen, B. and Churavy, V. and Schloss, J. and Campin, J. and Hill, C. and Edelman, A. and Marshall, J. and Flierl, G. and Ferrari, R.},
title={Uncertainty Quantification of Ocean Parameterizations: Application to the K-Profile-Parameterization for Penetrative Convection},
journal={Journal of Advances in Modeling Earth Systems},
year={2020},
volume={12},
number={12},
doi={10.1029/2020MS002108},
art_number={e2020MS002108},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098170696&doi=10.1029%2f2020MS002108&partnerID=40&md5=c4f7e66490b18c26105aa735b0f288c2},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Moses2020,
author={Moses, W.S. and Churavy, V.},
title={Instead of rewriting foreign code for machine learning, automatically synthesize fast gradients},
journal={Advances in Neural Information Processing Systems},
year={2020},
volume={2020-December},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108066870&partnerID=40&md5=10a10145e0035f420452e8d15cd72253},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yang20191,
author={Yang, L. and Treichler, S. and Kurth, T. and Fischer, K. and Barajas-Solano, D. and Romero, J. and Churavy, V. and Tartakovsky, A. and Houston, M. and Prabhat and Karniadakis, G.},
title={Highly-Ccalable, physics-informed GANs for learning solutions of stochastic PDEs},
journal={Proceedings of DLS 2019: Deep Learning on Supercomputers - Held in conjunction with SC 2019: The International Conference for High Performance Computing, Networking, Storage and Analysis},
year={2019},
pages={1-11},
doi={10.1109/DLS49591.2019.00006},
art_number={8945120},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078152879&doi=10.1109%2fDLS49591.2019.00006&partnerID=40&md5=877ceb5f95261c2b5dd244f20e8a1b90},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Besard201929,
author={Besard, T. and Churavy, V. and Edelman, A. and Sutter, B.D.},
title={Rapid software prototyping for heterogeneous and distributed platforms},
journal={Advances in Engineering Software},
year={2019},
volume={132},
pages={29-46},
doi={10.1016/j.advengsoft.2019.02.002},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063985940&doi=10.1016%2fj.advengsoft.2019.02.002&partnerID=40&md5=18d47b8c20e17151a2a8d276dd1641d8},
document_type={Article},
source={Scopus},
}

% Albert Gnadt %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@ARTICLE{Gnadt2020Signal,
title={Signal Enhancement for Magnetic Navigation Challenge Problem},
author={Gnadt, A.R. and Belarge, J. and Canciani, A. and Carl, G. and Conger, L. and Curro, J. and Edelman, A. and Morales, P. and Nielsen, A.P. and O'Keeffe, M.F. and Rackauckas, C.V. and Taylor, J. and Wollaber, A.B.},
journal={arXiv},
year={2020},
doi={10.48550/arXiv.2007.12158},
url={https://arxiv.org/abs/2007.12158},
document_type={Pre-print},
eprint={2007.12158},
archivePrefix={arXiv},
primaryClass={cs.LG},
}

@CONFERENCE{Gnadt2022,
title={Machine Learning-Enhanced Magnetic Calibration for Airborne Magnetic Anomaly Navigation},
author={Gnadt, A.R.},
journal={AIAA SCITECH 2022 Forum},
year={2022},
url={https://doi.org/10.2514/6.2022-1760},
doi={10.2514/6.2022-1760},
pages={1--16},
publisher={AIAA},
}

@ARTICLE{Gnadt2022TL,
title={Derivation and Extensions of the Tolles-Lawson Model for Aeromagnetic Compensation},
author={Gnadt, A.R. and Wollaber, A.B. and Nielsen, A.P.},
journal={arXiv},
year={2022},
doi={10.48550/arXiv.2212.09899},
url={https://arxiv.org/abs/2212.09899},
document_type={Pre-print},
eprint={2212.09899},
archivePrefix={arXiv},
primaryClass={physics.app-ph},
}
